digraph {
	graph [size="472.04999999999995,472.04999999999995"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1606703954384 [label="
 (1, 5, 256, 256)" fillcolor=darkolivegreen1]
	1606591229152 [label=ConvolutionBackward0]
	1608447977904 -> 1606591229152
	1608447977904 [label=ReluBackward0]
	1606704253680 -> 1608447977904
	1606704253680 [label=CudnnBatchNormBackward0]
	1606552266704 -> 1606704253680
	1606552266704 [label=ConvolutionBackward0]
	1606596975232 -> 1606552266704
	1606596975232 [label=ReluBackward0]
	1606597309344 -> 1606596975232
	1606597309344 [label=CudnnBatchNormBackward0]
	1606597306512 -> 1606597309344
	1606597306512 [label=ConvolutionBackward0]
	1606597304112 -> 1606597306512
	1606597304112 [label=UpsampleNearest2DBackward0]
	1606597306656 -> 1606597304112
	1606597306656 [label=ReluBackward0]
	1606886334016 -> 1606597306656
	1606886334016 [label=CudnnBatchNormBackward0]
	1606886328880 -> 1606886334016
	1606886328880 [label=ConvolutionBackward0]
	1606886342032 -> 1606886328880
	1606886342032 [label=ReluBackward0]
	1606886336272 -> 1606886342032
	1606886336272 [label=CudnnBatchNormBackward0]
	1606886342368 -> 1606886336272
	1606886342368 [label=ConvolutionBackward0]
	1606886332240 -> 1606886342368
	1606886332240 [label=CatBackward0]
	1606886328448 -> 1606886332240
	1606886328448 [label=UpsampleNearest2DBackward0]
	1606886330992 -> 1606886328448
	1606886330992 [label=ReluBackward0]
	1606886340736 -> 1606886330992
	1606886340736 [label=CudnnBatchNormBackward0]
	1606886326576 -> 1606886340736
	1606886326576 [label=ConvolutionBackward0]
	1606886333776 -> 1606886326576
	1606886333776 [label=ReluBackward0]
	1606886332816 -> 1606886333776
	1606886332816 [label=CudnnBatchNormBackward0]
	1606886329264 -> 1606886332816
	1606886329264 [label=ConvolutionBackward0]
	1606886332384 -> 1606886329264
	1606886332384 [label=CatBackward0]
	1606886337136 -> 1606886332384
	1606886337136 [label=UpsampleNearest2DBackward0]
	1606886336176 -> 1606886337136
	1606886336176 [label=ReluBackward0]
	1606886340976 -> 1606886336176
	1606886340976 [label=CudnnBatchNormBackward0]
	1606886329600 -> 1606886340976
	1606886329600 [label=ConvolutionBackward0]
	1606886334400 -> 1606886329600
	1606886334400 [label=ReluBackward0]
	1606886339056 -> 1606886334400
	1606886339056 [label=CudnnBatchNormBackward0]
	1606886328832 -> 1606886339056
	1606886328832 [label=ConvolutionBackward0]
	1606886333728 -> 1606886328832
	1606886333728 [label=CatBackward0]
	1606886334784 -> 1606886333728
	1606886334784 [label=UpsampleNearest2DBackward0]
	1606886337184 -> 1606886334784
	1606886337184 [label=ReluBackward0]
	1606886329696 -> 1606886337184
	1606886329696 [label=CudnnBatchNormBackward0]
	1606886329408 -> 1606886329696
	1606886329408 [label=ConvolutionBackward0]
	1606886333632 -> 1606886329408
	1606886333632 [label=ReluBackward0]
	1606886332336 -> 1606886333632
	1606886332336 [label=CudnnBatchNormBackward0]
	1606886335600 -> 1606886332336
	1606886335600 [label=ConvolutionBackward0]
	1606886329984 -> 1606886335600
	1606886329984 [label=CatBackward0]
	1606886332960 -> 1606886329984
	1606886332960 [label=UpsampleNearest2DBackward0]
	1606886336896 -> 1606886332960
	1606886336896 [label=ReluBackward0]
	1606886335216 -> 1606886336896
	1606886335216 [label=AddBackward0]
	1606886326720 -> 1606886335216
	1606886326720 [label=CudnnBatchNormBackward0]
	1606886332672 -> 1606886326720
	1606886332672 [label=ConvolutionBackward0]
	1606886342560 -> 1606886332672
	1606886342560 [label=ReluBackward0]
	1606886327056 -> 1606886342560
	1606886327056 [label=CudnnBatchNormBackward0]
	1606886330944 -> 1606886327056
	1606886330944 [label=ConvolutionBackward0]
	1606886328160 -> 1606886330944
	1606886328160 [label=ReluBackward0]
	1606886328688 -> 1606886328160
	1606886328688 [label=CudnnBatchNormBackward0]
	1606886341408 -> 1606886328688
	1606886341408 [label=ConvolutionBackward0]
	1606886328256 -> 1606886341408
	1606886328256 [label=ReluBackward0]
	1606886326384 -> 1606886328256
	1606886326384 [label=AddBackward0]
	1606886333872 -> 1606886326384
	1606886333872 [label=CudnnBatchNormBackward0]
	1606886334592 -> 1606886333872
	1606886334592 [label=ConvolutionBackward0]
	1606886342416 -> 1606886334592
	1606886342416 [label=ReluBackward0]
	1606886333440 -> 1606886342416
	1606886333440 [label=CudnnBatchNormBackward0]
	1606886340592 -> 1606886333440
	1606886340592 [label=ConvolutionBackward0]
	1606886330608 -> 1606886340592
	1606886330608 [label=ReluBackward0]
	1606886327632 -> 1606886330608
	1606886327632 [label=CudnnBatchNormBackward0]
	1606886338288 -> 1606886327632
	1606886338288 [label=ConvolutionBackward0]
	1606886337904 -> 1606886338288
	1606886337904 [label=ReluBackward0]
	1606886333104 -> 1606886337904
	1606886333104 [label=AddBackward0]
	1606886335648 -> 1606886333104
	1606886335648 [label=CudnnBatchNormBackward0]
	1606886332576 -> 1606886335648
	1606886332576 [label=ConvolutionBackward0]
	1606886340016 -> 1606886332576
	1606886340016 [label=ReluBackward0]
	1606886329936 -> 1606886340016
	1606886329936 [label=CudnnBatchNormBackward0]
	1606886334688 -> 1606886329936
	1606886334688 [label=ConvolutionBackward0]
	1606886341840 -> 1606886334688
	1606886341840 [label=ReluBackward0]
	1606886338144 -> 1606886341840
	1606886338144 [label=CudnnBatchNormBackward0]
	1606886338912 -> 1606886338144
	1606886338912 [label=ConvolutionBackward0]
	1606886334304 -> 1606886338912
	1606886334304 [label=ReluBackward0]
	1606886333296 -> 1606886334304
	1606886333296 [label=AddBackward0]
	1606886337520 -> 1606886333296
	1606886337520 [label=CudnnBatchNormBackward0]
	1606886335072 -> 1606886337520
	1606886335072 [label=ConvolutionBackward0]
	1606886329888 -> 1606886335072
	1606886329888 [label=ReluBackward0]
	1606886328400 -> 1606886329888
	1606886328400 [label=CudnnBatchNormBackward0]
	1606886340544 -> 1606886328400
	1606886340544 [label=ConvolutionBackward0]
	1606886328784 -> 1606886340544
	1606886328784 [label=ReluBackward0]
	1606886336848 -> 1606886328784
	1606886336848 [label=CudnnBatchNormBackward0]
	1606886337856 -> 1606886336848
	1606886337856 [label=ConvolutionBackward0]
	1606886334208 -> 1606886337856
	1606886334208 [label=ReluBackward0]
	1606886332864 -> 1606886334208
	1606886332864 [label=AddBackward0]
	1606886326480 -> 1606886332864
	1606886326480 [label=CudnnBatchNormBackward0]
	1606886339008 -> 1606886326480
	1606886339008 [label=ConvolutionBackward0]
	1606886327584 -> 1606886339008
	1606886327584 [label=ReluBackward0]
	1606886330656 -> 1606886327584
	1606886330656 [label=CudnnBatchNormBackward0]
	1606886337040 -> 1606886330656
	1606886337040 [label=ConvolutionBackward0]
	1606886334256 -> 1606886337040
	1606886334256 [label=ReluBackward0]
	1606886337328 -> 1606886334256
	1606886337328 [label=CudnnBatchNormBackward0]
	1606886327344 -> 1606886337328
	1606886327344 [label=ConvolutionBackward0]
	1606886330800 -> 1606886327344
	1606886330800 [label=ReluBackward0]
	1606886554880 -> 1606886330800
	1606886554880 [label=AddBackward0]
	1606886543504 -> 1606886554880
	1606886543504 [label=CudnnBatchNormBackward0]
	1606886543552 -> 1606886543504
	1606886543552 [label=ConvolutionBackward0]
	1606886548400 -> 1606886543552
	1606886548400 [label=ReluBackward0]
	1606886551856 -> 1606886548400
	1606886551856 [label=CudnnBatchNormBackward0]
	1606886545472 -> 1606886551856
	1606886545472 [label=ConvolutionBackward0]
	1606886549792 -> 1606886545472
	1606886549792 [label=ReluBackward0]
	1606886542640 -> 1606886549792
	1606886542640 [label=CudnnBatchNormBackward0]
	1606886542832 -> 1606886542640
	1606886542832 [label=ConvolutionBackward0]
	1606886553104 -> 1606886542832
	1606886553104 [label=ReluBackward0]
	1606886554976 -> 1606886553104
	1606886554976 [label=AddBackward0]
	1606886547248 -> 1606886554976
	1606886547248 [label=CudnnBatchNormBackward0]
	1606886550320 -> 1606886547248
	1606886550320 [label=ConvolutionBackward0]
	1606886540192 -> 1606886550320
	1606886540192 [label=ReluBackward0]
	1606886541344 -> 1606886540192
	1606886541344 [label=CudnnBatchNormBackward0]
	1606886543312 -> 1606886541344
	1606886543312 [label=ConvolutionBackward0]
	1606886551472 -> 1606886543312
	1606886551472 [label=ReluBackward0]
	1606886550128 -> 1606886551472
	1606886550128 [label=CudnnBatchNormBackward0]
	1606886551328 -> 1606886550128
	1606886551328 [label=ConvolutionBackward0]
	1606886555072 -> 1606886551328
	1606886555072 [label=ReluBackward0]
	1606886542880 -> 1606886555072
	1606886542880 [label=AddBackward0]
	1606886548064 -> 1606886542880
	1606886548064 [label=CudnnBatchNormBackward0]
	1606886545664 -> 1606886548064
	1606886545664 [label=ConvolutionBackward0]
	1606886545376 -> 1606886545664
	1606886545376 [label=ReluBackward0]
	1606886545952 -> 1606886545376
	1606886545952 [label=CudnnBatchNormBackward0]
	1606886549072 -> 1606886545952
	1606886549072 [label=ConvolutionBackward0]
	1606886547968 -> 1606886549072
	1606886547968 [label=ReluBackward0]
	1606886547632 -> 1606886547968
	1606886547632 [label=CudnnBatchNormBackward0]
	1606886547776 -> 1606886547632
	1606886547776 [label=ConvolutionBackward0]
	1606886550656 -> 1606886547776
	1606886550656 [label=ReluBackward0]
	1606886545328 -> 1606886550656
	1606886545328 [label=AddBackward0]
	1606886549024 -> 1606886545328
	1606886549024 [label=CudnnBatchNormBackward0]
	1606886552528 -> 1606886549024
	1606886552528 [label=ConvolutionBackward0]
	1606886551520 -> 1606886552528
	1606886551520 [label=ReluBackward0]
	1606886548880 -> 1606886551520
	1606886548880 [label=CudnnBatchNormBackward0]
	1606886553824 -> 1606886548880
	1606886553824 [label=ConvolutionBackward0]
	1606886555552 -> 1606886553824
	1606886555552 [label=ReluBackward0]
	1606886550512 -> 1606886555552
	1606886550512 [label=CudnnBatchNormBackward0]
	1606886540000 -> 1606886550512
	1606886540000 [label=ConvolutionBackward0]
	1606886550896 -> 1606886540000
	1606886550896 [label=ReluBackward0]
	1606886550704 -> 1606886550896
	1606886550704 [label=AddBackward0]
	1606886541776 -> 1606886550704
	1606886541776 [label=CudnnBatchNormBackward0]
	1606886543936 -> 1606886541776
	1606886543936 [label=ConvolutionBackward0]
	1606886539664 -> 1606886543936
	1606886539664 [label=ReluBackward0]
	1606886546000 -> 1606886539664
	1606886546000 [label=CudnnBatchNormBackward0]
	1606886544272 -> 1606886546000
	1606886544272 [label=ConvolutionBackward0]
	1606886546912 -> 1606886544272
	1606886546912 [label=ReluBackward0]
	1606886550992 -> 1606886546912
	1606886550992 [label=CudnnBatchNormBackward0]
	1606886549696 -> 1606886550992
	1606886549696 [label=ConvolutionBackward0]
	1606886539904 -> 1606886549696
	1606886539904 [label=ReluBackward0]
	1606886549600 -> 1606886539904
	1606886549600 [label=AddBackward0]
	1606886546528 -> 1606886549600
	1606886546528 [label=CudnnBatchNormBackward0]
	1606886551232 -> 1606886546528
	1606886551232 [label=ConvolutionBackward0]
	1606886543456 -> 1606886551232
	1606886543456 [label=ReluBackward0]
	1606886541680 -> 1606886543456
	1606886541680 [label=CudnnBatchNormBackward0]
	1606886553344 -> 1606886541680
	1606886553344 [label=ConvolutionBackward0]
	1606886555312 -> 1606886553344
	1606886555312 [label=ReluBackward0]
	1606886552336 -> 1606886555312
	1606886552336 [label=CudnnBatchNormBackward0]
	1606886541200 -> 1606886552336
	1606886541200 [label=ConvolutionBackward0]
	1606886542256 -> 1606886541200
	1606886542256 [label=ReluBackward0]
	1606886553920 -> 1606886542256
	1606886553920 [label=AddBackward0]
	1606886541632 -> 1606886553920
	1606886541632 [label=CudnnBatchNormBackward0]
	1606886553488 -> 1606886541632
	1606886553488 [label=ConvolutionBackward0]
	1606886539952 -> 1606886553488
	1606886539952 [label=ReluBackward0]
	1606886548112 -> 1606886539952
	1606886548112 [label=CudnnBatchNormBackward0]
	1606886553632 -> 1606886548112
	1606886553632 [label=ConvolutionBackward0]
	1606886545232 -> 1606886553632
	1606886545232 [label=ReluBackward0]
	1606886549456 -> 1606886545232
	1606886549456 [label=CudnnBatchNormBackward0]
	1606886542304 -> 1606886549456
	1606886542304 [label=ConvolutionBackward0]
	1606886551280 -> 1606886542304
	1606886551280 [label=ReluBackward0]
	1606886542928 -> 1606886551280
	1606886542928 [label=AddBackward0]
	1606886552096 -> 1606886542928
	1606886552096 [label=CudnnBatchNormBackward0]
	1606886547056 -> 1606886552096
	1606886547056 [label=ConvolutionBackward0]
	1606886539808 -> 1606886547056
	1606886539808 [label=ReluBackward0]
	1606886555360 -> 1606886539808
	1606886555360 [label=CudnnBatchNormBackward0]
	1606886548832 -> 1606886555360
	1606886548832 [label=ConvolutionBackward0]
	1606886540048 -> 1606886548832
	1606886540048 [label=ReluBackward0]
	1606886542448 -> 1606886540048
	1606886542448 [label=CudnnBatchNormBackward0]
	1606886542352 -> 1606886542448
	1606886542352 [label=ConvolutionBackward0]
	1606886539568 -> 1606886542352
	1606886539568 [label=ReluBackward0]
	1606886544464 -> 1606886539568
	1606886544464 [label=AddBackward0]
	1606886541152 -> 1606886544464
	1606886541152 [label=CudnnBatchNormBackward0]
	1606886547392 -> 1606886541152
	1606886547392 [label=ConvolutionBackward0]
	1606886551136 -> 1606886547392
	1606886551136 [label=ReluBackward0]
	1606886543600 -> 1606886551136
	1606886543600 [label=CudnnBatchNormBackward0]
	1606886548016 -> 1606886543600
	1606886548016 [label=ConvolutionBackward0]
	1606886547536 -> 1606886548016
	1606886547536 [label=ReluBackward0]
	1606886545280 -> 1606886547536
	1606886545280 [label=CudnnBatchNormBackward0]
	1606886553728 -> 1606886545280
	1606886553728 [label=ConvolutionBackward0]
	1606886542064 -> 1606886553728
	1606886542064 [label=ReluBackward0]
	1606886539856 -> 1606886542064
	1606886539856 [label=AddBackward0]
	1606886550848 -> 1606886539856
	1606886550848 [label=CudnnBatchNormBackward0]
	1606886543216 -> 1606886550848
	1606886543216 [label=ConvolutionBackward0]
	1606886541536 -> 1606886543216
	1606886541536 [label=ReluBackward0]
	1606886539520 -> 1606886541536
	1606886539520 [label=CudnnBatchNormBackward0]
	1606886540288 -> 1606886539520
	1606886540288 [label=ConvolutionBackward0]
	1606886545184 -> 1606886540288
	1606886545184 [label=ReluBackward0]
	1606886539376 -> 1606886545184
	1606886539376 [label=CudnnBatchNormBackward0]
	1606886545856 -> 1606886539376
	1606886545856 [label=ConvolutionBackward0]
	1606886549312 -> 1606886545856
	1606886549312 [label=ReluBackward0]
	1606886542688 -> 1606886549312
	1606886542688 [label=AddBackward0]
	1606886552048 -> 1606886542688
	1606886552048 [label=CudnnBatchNormBackward0]
	1606886544320 -> 1606886552048
	1606886544320 [label=ConvolutionBackward0]
	1606886541872 -> 1606886544320
	1606886541872 [label=ReluBackward0]
	1606886550416 -> 1606886541872
	1606886550416 [label=CudnnBatchNormBackward0]
	1606886551040 -> 1606886550416
	1606886551040 [label=ConvolutionBackward0]
	1606886554016 -> 1606886551040
	1606886554016 [label=ReluBackward0]
	1606886553776 -> 1606886554016
	1606886553776 [label=CudnnBatchNormBackward0]
	1606886551376 -> 1606886553776
	1606886551376 [label=ConvolutionBackward0]
	1606886550944 -> 1606886551376
	1606886550944 [label=ReluBackward0]
	1606886553248 -> 1606886550944
	1606886553248 [label=AddBackward0]
	1606886063456 -> 1606886553248
	1606886063456 [label=CudnnBatchNormBackward0]
	1606886060432 -> 1606886063456
	1606886060432 [label=ConvolutionBackward0]
	1606886057648 -> 1606886060432
	1606886057648 [label=ReluBackward0]
	1606886057072 -> 1606886057648
	1606886057072 [label=CudnnBatchNormBackward0]
	1606886054288 -> 1606886057072
	1606886054288 [label=ConvolutionBackward0]
	1606886052512 -> 1606886054288
	1606886052512 [label=ReluBackward0]
	1606886050016 -> 1606886052512
	1606886050016 [label=CudnnBatchNormBackward0]
	1606886056976 -> 1606886050016
	1606886056976 [label=ConvolutionBackward0]
	1606886063312 -> 1606886056976
	1606886063312 [label=ReluBackward0]
	1606886060000 -> 1606886063312
	1606886060000 [label=AddBackward0]
	1606886052416 -> 1606886060000
	1606886052416 [label=CudnnBatchNormBackward0]
	1606886050832 -> 1606886052416
	1606886050832 [label=ConvolutionBackward0]
	1606886053184 -> 1606886050832
	1606886053184 [label=ReluBackward0]
	1606886049344 -> 1606886053184
	1606886049344 [label=CudnnBatchNormBackward0]
	1606886058848 -> 1606886049344
	1606886058848 [label=ConvolutionBackward0]
	1606886047904 -> 1606886058848
	1606886047904 [label=ReluBackward0]
	1606886053952 -> 1606886047904
	1606886053952 [label=CudnnBatchNormBackward0]
	1606886052080 -> 1606886053952
	1606886052080 [label=ConvolutionBackward0]
	1606886050112 -> 1606886052080
	1606886050112 [label=ReluBackward0]
	1606886054768 -> 1606886050112
	1606886054768 [label=AddBackward0]
	1606886047856 -> 1606886054768
	1606886047856 [label=CudnnBatchNormBackward0]
	1606886058320 -> 1606886047856
	1606886058320 [label=ConvolutionBackward0]
	1606886063552 -> 1606886058320
	1606886063552 [label=ReluBackward0]
	1606886048912 -> 1606886063552
	1606886048912 [label=CudnnBatchNormBackward0]
	1606886060096 -> 1606886048912
	1606886060096 [label=ConvolutionBackward0]
	1606886059712 -> 1606886060096
	1606886059712 [label=ReluBackward0]
	1606886060960 -> 1606886059712
	1606886060960 [label=CudnnBatchNormBackward0]
	1606886048240 -> 1606886060960
	1606886048240 [label=ConvolutionBackward0]
	1606886061008 -> 1606886048240
	1606886061008 [label=ReluBackward0]
	1606886061584 -> 1606886061008
	1606886061584 [label=AddBackward0]
	1606886058464 -> 1606886061584
	1606886058464 [label=CudnnBatchNormBackward0]
	1606886048576 -> 1606886058464
	1606886048576 [label=ConvolutionBackward0]
	1606886055104 -> 1606886048576
	1606886055104 [label=ReluBackward0]
	1606886051312 -> 1606886055104
	1606886051312 [label=CudnnBatchNormBackward0]
	1606886049824 -> 1606886051312
	1606886049824 [label=ConvolutionBackward0]
	1606886062880 -> 1606886049824
	1606886062880 [label=ReluBackward0]
	1606886055872 -> 1606886062880
	1606886055872 [label=CudnnBatchNormBackward0]
	1606886051744 -> 1606886055872
	1606886051744 [label=ConvolutionBackward0]
	1606886061152 -> 1606886051744
	1606886061152 [label=ReluBackward0]
	1606886058560 -> 1606886061152
	1606886058560 [label=AddBackward0]
	1606886061632 -> 1606886058560
	1606886061632 [label=CudnnBatchNormBackward0]
	1606886063840 -> 1606886061632
	1606886063840 [label=ConvolutionBackward0]
	1606886057264 -> 1606886063840
	1606886057264 [label=ReluBackward0]
	1606886055728 -> 1606886057264
	1606886055728 [label=CudnnBatchNormBackward0]
	1606886049152 -> 1606886055728
	1606886049152 [label=ConvolutionBackward0]
	1606886051072 -> 1606886049152
	1606886051072 [label=ReluBackward0]
	1606886051408 -> 1606886051072
	1606886051408 [label=CudnnBatchNormBackward0]
	1606886054192 -> 1606886051408
	1606886054192 [label=ConvolutionBackward0]
	1606886062736 -> 1606886054192
	1606886062736 [label=ReluBackward0]
	1606886049536 -> 1606886062736
	1606886049536 [label=AddBackward0]
	1606886060240 -> 1606886049536
	1606886060240 [label=CudnnBatchNormBackward0]
	1606886055296 -> 1606886060240
	1606886055296 [label=ConvolutionBackward0]
	1606886054528 -> 1606886055296
	1606886054528 [label=ReluBackward0]
	1606886061824 -> 1606886054528
	1606886061824 [label=CudnnBatchNormBackward0]
	1606886059280 -> 1606886061824
	1606886059280 [label=ConvolutionBackward0]
	1606886059088 -> 1606886059280
	1606886059088 [label=ReluBackward0]
	1606886063168 -> 1606886059088
	1606886063168 [label=CudnnBatchNormBackward0]
	1606886050256 -> 1606886063168
	1606886050256 [label=ConvolutionBackward0]
	1606886054096 -> 1606886050256
	1606886054096 [label=ReluBackward0]
	1606886059856 -> 1606886054096
	1606886059856 [label=AddBackward0]
	1606886063696 -> 1606886059856
	1606886063696 [label=CudnnBatchNormBackward0]
	1606886049872 -> 1606886063696
	1606886049872 [label=ConvolutionBackward0]
	1606886056688 -> 1606886049872
	1606886056688 [label=ReluBackward0]
	1606886052704 -> 1606886056688
	1606886052704 [label=CudnnBatchNormBackward0]
	1606886053760 -> 1606886052704
	1606886053760 [label=ConvolutionBackward0]
	1606886051984 -> 1606886053760
	1606886051984 [label=ReluBackward0]
	1606886052032 -> 1606886051984
	1606886052032 [label=CudnnBatchNormBackward0]
	1606886060816 -> 1606886052032
	1606886060816 [label=ConvolutionBackward0]
	1606886058896 -> 1606886060816
	1606886058896 [label=ReluBackward0]
	1606886062064 -> 1606886058896
	1606886062064 [label=AddBackward0]
	1606886048864 -> 1606886062064
	1606886048864 [label=CudnnBatchNormBackward0]
	1606886059232 -> 1606886048864
	1606886059232 [label=ConvolutionBackward0]
	1606886062112 -> 1606886059232
	1606886062112 [label=ReluBackward0]
	1606886049248 -> 1606886062112
	1606886049248 [label=CudnnBatchNormBackward0]
	1606886057600 -> 1606886049248
	1606886057600 [label=ConvolutionBackward0]
	1606886063072 -> 1606886057600
	1606886063072 [label=ReluBackward0]
	1606886059472 -> 1606886063072
	1606886059472 [label=CudnnBatchNormBackward0]
	1606886056400 -> 1606886059472
	1606886056400 [label=ConvolutionBackward0]
	1606886055344 -> 1606886056400
	1606886055344 [label=ReluBackward0]
	1606886060336 -> 1606886055344
	1606886060336 [label=AddBackward0]
	1606886059568 -> 1606886060336
	1606886059568 [label=CudnnBatchNormBackward0]
	1606886053904 -> 1606886059568
	1606886053904 [label=ConvolutionBackward0]
	1606886053040 -> 1606886053904
	1606886053040 [label=ReluBackward0]
	1606886061968 -> 1606886053040
	1606886061968 [label=CudnnBatchNormBackward0]
	1606886050928 -> 1606886061968
	1606886050928 [label=ConvolutionBackward0]
	1606886053664 -> 1606886050928
	1606886053664 [label=ReluBackward0]
	1606886063264 -> 1606886053664
	1606886063264 [label=CudnnBatchNormBackward0]
	1606886048960 -> 1606886063264
	1606886048960 [label=ConvolutionBackward0]
	1606886051120 -> 1606886048960
	1606886051120 [label=ReluBackward0]
	1606886054144 -> 1606886051120
	1606886054144 [label=AddBackward0]
	1606886059664 -> 1606886054144
	1606886059664 [label=CudnnBatchNormBackward0]
	1606886057744 -> 1606886059664
	1606886057744 [label=ConvolutionBackward0]
	1606886056112 -> 1606886057744
	1606886056112 [label=ReluBackward0]
	1606886061680 -> 1606886056112
	1606886061680 [label=CudnnBatchNormBackward0]
	1606886058608 -> 1606886061680
	1606886058608 [label=ConvolutionBackward0]
	1606886061728 -> 1606886058608
	1606886061728 [label=ReluBackward0]
	1606886049104 -> 1606886061728
	1606886049104 [label=CudnnBatchNormBackward0]
	1606886055536 -> 1606886049104
	1606886055536 [label=ConvolutionBackward0]
	1606886062592 -> 1606886055536
	1606886062592 [label=ReluBackward0]
	1606886054240 -> 1606886062592
	1606886054240 [label=AddBackward0]
	1606886055008 -> 1606886054240
	1606886055008 [label=CudnnBatchNormBackward0]
	1606886051648 -> 1606886055008
	1606886051648 [label=ConvolutionBackward0]
	1606886051792 -> 1606886051648
	1606886051792 [label=ReluBackward0]
	1606886056304 -> 1606886051792
	1606886056304 [label=CudnnBatchNormBackward0]
	1606886064080 -> 1606886056304
	1606886064080 [label=ConvolutionBackward0]
	1606886063888 -> 1606886064080
	1606886063888 [label=ReluBackward0]
	1606886053808 -> 1606886063888
	1606886053808 [label=CudnnBatchNormBackward0]
	1606886056736 -> 1606886053808
	1606886056736 [label=ConvolutionBackward0]
	1606886054576 -> 1606886056736
	1606886054576 [label=ReluBackward0]
	1606886062160 -> 1606886054576
	1606886062160 [label=AddBackward0]
	1606886267664 -> 1606886062160
	1606886267664 [label=CudnnBatchNormBackward0]
	1606886264304 -> 1606886267664
	1606886264304 [label=ConvolutionBackward0]
	1606886276160 -> 1606886264304
	1606886276160 [label=ReluBackward0]
	1606886275104 -> 1606886276160
	1606886275104 [label=CudnnBatchNormBackward0]
	1606886274768 -> 1606886275104
	1606886274768 [label=ConvolutionBackward0]
	1606886273088 -> 1606886274768
	1606886273088 [label=ReluBackward0]
	1606886272560 -> 1606886273088
	1606886272560 [label=CudnnBatchNormBackward0]
	1606886271792 -> 1606886272560
	1606886271792 [label=ConvolutionBackward0]
	1606886265504 -> 1606886271792
	1606886265504 [label=ReluBackward0]
	1606886270208 -> 1606886265504
	1606886270208 [label=AddBackward0]
	1606886269824 -> 1606886270208
	1606886269824 [label=CudnnBatchNormBackward0]
	1606886269344 -> 1606886269824
	1606886269344 [label=ConvolutionBackward0]
	1606886268288 -> 1606886269344
	1606886268288 [label=ReluBackward0]
	1606886267616 -> 1606886268288
	1606886267616 [label=CudnnBatchNormBackward0]
	1606886265648 -> 1606886267616
	1606886265648 [label=ConvolutionBackward0]
	1606886264640 -> 1606886265648
	1606886264640 [label=ReluBackward0]
	1606886264160 -> 1606886264640
	1606886264160 [label=CudnnBatchNormBackward0]
	1606886263728 -> 1606886264160
	1606886263728 [label=ConvolutionBackward0]
	1606886269680 -> 1606886263728
	1606886269680 [label=ReluBackward0]
	1606886261712 -> 1606886269680
	1606886261712 [label=AddBackward0]
	1606886275584 -> 1606886261712
	1606886275584 [label=CudnnBatchNormBackward0]
	1606886260800 -> 1606886275584
	1606886260800 [label=ConvolutionBackward0]
	1606886272992 -> 1606886260800
	1606886272992 [label=ReluBackward0]
	1606886264976 -> 1606886272992
	1606886264976 [label=CudnnBatchNormBackward0]
	1606886261424 -> 1606886264976
	1606886261424 [label=ConvolutionBackward0]
	1606886266752 -> 1606886261424
	1606886266752 [label=ReluBackward0]
	1606886275920 -> 1606886266752
	1606886275920 [label=CudnnBatchNormBackward0]
	1606886261664 -> 1606886275920
	1606886261664 [label=ConvolutionBackward0]
	1606886265696 -> 1606886261664
	1606886265696 [label=ReluBackward0]
	1606886275488 -> 1606886265696
	1606886275488 [label=AddBackward0]
	1606886275440 -> 1606886275488
	1606886275440 [label=CudnnBatchNormBackward0]
	1606886274576 -> 1606886275440
	1606886274576 [label=ConvolutionBackward0]
	1606886272944 -> 1606886274576
	1606886272944 [label=ReluBackward0]
	1606886275968 -> 1606886272944
	1606886275968 [label=CudnnBatchNormBackward0]
	1606886265888 -> 1606886275968
	1606886265888 [label=ConvolutionBackward0]
	1606886264016 -> 1606886265888
	1606886264016 [label=ReluBackward0]
	1606886261376 -> 1606886264016
	1606886261376 [label=CudnnBatchNormBackward0]
	1606886268768 -> 1606886261376
	1606886268768 [label=ConvolutionBackward0]
	1606886275296 -> 1606886268768
	1606886275296 [label=ReluBackward0]
	1606886274384 -> 1606886275296
	1606886274384 [label=AddBackward0]
	1606886260896 -> 1606886274384
	1606886260896 [label=CudnnBatchNormBackward0]
	1606886276400 -> 1606886260896
	1606886276400 [label=ConvolutionBackward0]
	1606886271984 -> 1606886276400
	1606886271984 [label=ReluBackward0]
	1606886269248 -> 1606886271984
	1606886269248 [label=CudnnBatchNormBackward0]
	1606886262912 -> 1606886269248
	1606886262912 [label=ConvolutionBackward0]
	1606886263536 -> 1606886262912
	1606886263536 [label=ReluBackward0]
	1606886261184 -> 1606886263536
	1606886261184 [label=CudnnBatchNormBackward0]
	1606886265072 -> 1606886261184
	1606886265072 [label=ConvolutionBackward0]
	1606886275872 -> 1606886265072
	1606886275872 [label=ReluBackward0]
	1606886276256 -> 1606886275872
	1606886276256 [label=AddBackward0]
	1606886263680 -> 1606886276256
	1606886263680 [label=CudnnBatchNormBackward0]
	1606886268144 -> 1606886263680
	1606886268144 [label=ConvolutionBackward0]
	1606886263104 -> 1606886268144
	1606886263104 [label=ReluBackward0]
	1606886272464 -> 1606886263104
	1606886272464 [label=CudnnBatchNormBackward0]
	1606886263440 -> 1606886272464
	1606886263440 [label=ConvolutionBackward0]
	1606886264352 -> 1606886263440
	1606886264352 [label=ReluBackward0]
	1606886261088 -> 1606886264352
	1606886261088 [label=CudnnBatchNormBackward0]
	1606886275536 -> 1606886261088
	1606886275536 [label=ConvolutionBackward0]
	1606886273760 -> 1606886275536
	1606886273760 [label=ReluBackward0]
	1606886271024 -> 1606886273760
	1606886271024 [label=AddBackward0]
	1606886264832 -> 1606886271024
	1606886264832 [label=CudnnBatchNormBackward0]
	1606886277072 -> 1606886264832
	1606886277072 [label=ConvolutionBackward0]
	1606886262192 -> 1606886277072
	1606886262192 [label=ReluBackward0]
	1606886269584 -> 1606886262192
	1606886269584 [label=CudnnBatchNormBackward0]
	1606886262720 -> 1606886269584
	1606886262720 [label=ConvolutionBackward0]
	1606886276736 -> 1606886262720
	1606886276736 [label=ReluBackward0]
	1606886273232 -> 1606886276736
	1606886273232 [label=CudnnBatchNormBackward0]
	1606886261904 -> 1606886273232
	1606886261904 [label=ConvolutionBackward0]
	1606886262816 -> 1606886261904
	1606886262816 [label=ReluBackward0]
	1606886261040 -> 1606886262816
	1606886261040 [label=AddBackward0]
	1606886262432 -> 1606886261040
	1606886262432 [label=CudnnBatchNormBackward0]
	1606886269440 -> 1606886262432
	1606886269440 [label=ConvolutionBackward0]
	1606886261328 -> 1606886269440
	1606886261328 [label=ReluBackward0]
	1606886262624 -> 1606886261328
	1606886262624 [label=CudnnBatchNormBackward0]
	1606886263872 -> 1606886262624
	1606886263872 [label=ConvolutionBackward0]
	1606886265984 -> 1606886263872
	1606886265984 [label=ReluBackward0]
	1606886267376 -> 1606886265984
	1606886267376 [label=CudnnBatchNormBackward0]
	1606886267904 -> 1606886267376
	1606886267904 [label=ConvolutionBackward0]
	1606886270976 -> 1606886267904
	1606886270976 [label=ReluBackward0]
	1606886269920 -> 1606886270976
	1606886269920 [label=AddBackward0]
	1606886270448 -> 1606886269920
	1606886270448 [label=CudnnBatchNormBackward0]
	1606886271264 -> 1606886270448
	1606886271264 [label=ConvolutionBackward0]
	1606886271888 -> 1606886271264
	1606886271888 [label=ReluBackward0]
	1606886273472 -> 1606886271888
	1606886273472 [label=CudnnBatchNormBackward0]
	1606886273904 -> 1606886273472
	1606886273904 [label=ConvolutionBackward0]
	1606886274720 -> 1606886273904
	1606886274720 [label=ReluBackward0]
	1606886276352 -> 1606886274720
	1606886276352 [label=CudnnBatchNormBackward0]
	1606886276832 -> 1606886276352
	1606886276832 [label=ConvolutionBackward0]
	1606886270688 -> 1606886276832
	1606886270688 [label=ReluBackward0]
	1606886276304 -> 1606886270688
	1606886276304 [label=AddBackward0]
	1606886271936 -> 1606886276304
	1606886271936 [label=CudnnBatchNormBackward0]
	1606886265360 -> 1606886271936
	1606886265360 [label=ConvolutionBackward0]
	1606886268192 -> 1606886265360
	1606886268192 [label=ReluBackward0]
	1606886267856 -> 1606886268192
	1606886267856 [label=CudnnBatchNormBackward0]
	1606886267520 -> 1606886267856
	1606886267520 [label=ConvolutionBackward0]
	1606886264688 -> 1606886267520
	1606886264688 [label=ReluBackward0]
	1606886262048 -> 1606886264688
	1606886262048 [label=CudnnBatchNormBackward0]
	1606886268096 -> 1606886262048
	1606886268096 [label=ConvolutionBackward0]
	1606886264592 -> 1606886268096
	1606886264592 [label=ReluBackward0]
	1606886264064 -> 1606886264592
	1606886264064 [label=AddBackward0]
	1606886268240 -> 1606886264064
	1606886268240 [label=CudnnBatchNormBackward0]
	1606886269776 -> 1606886268240
	1606886269776 [label=ConvolutionBackward0]
	1606886276208 -> 1606886269776
	1606886276208 [label=ReluBackward0]
	1606886272512 -> 1606886276208
	1606886272512 [label=CudnnBatchNormBackward0]
	1606886267280 -> 1606886272512
	1606886267280 [label=ConvolutionBackward0]
	1606886273040 -> 1606886267280
	1606886273040 [label=ReluBackward0]
	1606886275680 -> 1606886273040
	1606886275680 [label=CudnnBatchNormBackward0]
	1606886266272 -> 1606886275680
	1606886266272 [label=ConvolutionBackward0]
	1606886266416 -> 1606886266272
	1606886266416 [label=ReluBackward0]
	1606886421792 -> 1606886266416
	1606886421792 [label=AddBackward0]
	1606886417280 -> 1606886421792
	1606886417280 [label=CudnnBatchNormBackward0]
	1606886414880 -> 1606886417280
	1606886414880 [label=ConvolutionBackward0]
	1606886424528 -> 1606886414880
	1606886424528 [label=ReluBackward0]
	1606886424048 -> 1606886424528
	1606886424048 [label=CudnnBatchNormBackward0]
	1606886423280 -> 1606886424048
	1606886423280 [label=ConvolutionBackward0]
	1606886422560 -> 1606886423280
	1606886422560 [label=ReluBackward0]
	1606886414112 -> 1606886422560
	1606886414112 [label=CudnnBatchNormBackward0]
	1606886421312 -> 1606886414112
	1606886421312 [label=ConvolutionBackward0]
	1606886329072 -> 1606886421312
	1606886329072 [label=ReluBackward0]
	1606886420208 -> 1606886329072
	1606886420208 [label=AddBackward0]
	1606886419056 -> 1606886420208
	1606886419056 [label=CudnnBatchNormBackward0]
	1606886418816 -> 1606886419056
	1606886418816 [label=ConvolutionBackward0]
	1606886418384 -> 1606886418816
	1606886418384 [label=ReluBackward0]
	1606886416656 -> 1606886418384
	1606886416656 [label=CudnnBatchNormBackward0]
	1606886416368 -> 1606886416656
	1606886416368 [label=ConvolutionBackward0]
	1606886415120 -> 1606886416368
	1606886415120 [label=ReluBackward0]
	1606886414784 -> 1606886415120
	1606886414784 [label=CudnnBatchNormBackward0]
	1606886414448 -> 1606886414784
	1606886414448 [label=ConvolutionBackward0]
	1606886419008 -> 1606886414448
	1606886419008 [label=ReluBackward0]
	1606886413008 -> 1606886419008
	1606886413008 [label=AddBackward0]
	1606886412528 -> 1606886413008
	1606886412528 [label=CudnnBatchNormBackward0]
	1606886412144 -> 1606886412528
	1606886412144 [label=ConvolutionBackward0]
	1606886411472 -> 1606886412144
	1606886411472 [label=ReluBackward0]
	1606886410800 -> 1606886411472
	1606886410800 [label=CudnnBatchNormBackward0]
	1606886410272 -> 1606886410800
	1606886410272 [label=ConvolutionBackward0]
	1606886409312 -> 1606886410272
	1606886409312 [label=ReluBackward0]
	1606886408928 -> 1606886409312
	1606886408928 [label=CudnnBatchNormBackward0]
	1606886408784 -> 1606886408928
	1606886408784 [label=ConvolutionBackward0]
	1606886412480 -> 1606886408784
	1606886412480 [label=ReluBackward0]
	1606886410704 -> 1606886412480
	1606886410704 [label=AddBackward0]
	1606886424480 -> 1606886410704
	1606886424480 [label=CudnnBatchNormBackward0]
	1606886409168 -> 1606886424480
	1606886409168 [label=ConvolutionBackward0]
	1606886424192 -> 1606886409168
	1606886424192 [label=ReluBackward0]
	1606886416752 -> 1606886424192
	1606886416752 [label=CudnnBatchNormBackward0]
	1606886410368 -> 1606886416752
	1606886410368 [label=ConvolutionBackward0]
	1606886421456 -> 1606886410368
	1606886421456 [label=ReluBackward0]
	1606886421072 -> 1606886421456
	1606886421072 [label=CudnnBatchNormBackward0]
	1606886418144 -> 1606886421072
	1606886418144 [label=ConvolutionBackward0]
	1606886420928 -> 1606886418144
	1606886420928 [label=ReluBackward0]
	1606886421696 -> 1606886420928
	1606886421696 [label=AddBackward0]
	1606886419968 -> 1606886421696
	1606886419968 [label=CudnnBatchNormBackward0]
	1606886418432 -> 1606886419968
	1606886418432 [label=ConvolutionBackward0]
	1606886414256 -> 1606886418432
	1606886414256 [label=ReluBackward0]
	1606886412768 -> 1606886414256
	1606886412768 [label=CudnnBatchNormBackward0]
	1606886411280 -> 1606886412768
	1606886411280 [label=ConvolutionBackward0]
	1606886409504 -> 1606886411280
	1606886409504 [label=ReluBackward0]
	1606886421648 -> 1606886409504
	1606886421648 [label=CudnnBatchNormBackward0]
	1606886419872 -> 1606886421648
	1606886419872 [label=ConvolutionBackward0]
	1606886419536 -> 1606886419872
	1606886419536 [label=ReluBackward0]
	1606886410560 -> 1606886419536
	1606886410560 [label=AddBackward0]
	1606886409264 -> 1606886410560
	1606886409264 [label=CudnnBatchNormBackward0]
	1606886417568 -> 1606886409264
	1606886417568 [label=ConvolutionBackward0]
	1606886420736 -> 1606886417568
	1606886420736 [label=ReluBackward0]
	1606886410080 -> 1606886420736
	1606886410080 [label=CudnnBatchNormBackward0]
	1606886412816 -> 1606886410080
	1606886412816 [label=ConvolutionBackward0]
	1606886408880 -> 1606886412816
	1606886408880 [label=ReluBackward0]
	1606886408496 -> 1606886408880
	1606886408496 [label=CudnnBatchNormBackward0]
	1606886409120 -> 1606886408496
	1606886409120 [label=ConvolutionBackward0]
	1606886412384 -> 1606886409120
	1606886412384 [label=ReluBackward0]
	1606886410848 -> 1606886412384
	1606886410848 [label=AddBackward0]
	1606886411328 -> 1606886410848
	1606886411328 [label=CudnnBatchNormBackward0]
	1606886411520 -> 1606886411328
	1606886411520 [label=ConvolutionBackward0]
	1606886412720 -> 1606886411520
	1606886412720 [label=ReluBackward0]
	1606886413584 -> 1606886412720
	1606886413584 [label=CudnnBatchNormBackward0]
	1606886413968 -> 1606886413584
	1606886413968 [label=ConvolutionBackward0]
	1606886414832 -> 1606886413968
	1606886414832 [label=ReluBackward0]
	1606886415456 -> 1606886414832
	1606886415456 [label=CudnnBatchNormBackward0]
	1606886416272 -> 1606886415456
	1606886416272 [label=ConvolutionBackward0]
	1606886411376 -> 1606886416272
	1606886411376 [label=ReluBackward0]
	1606886417184 -> 1606886411376
	1606886417184 [label=AddBackward0]
	1606886417664 -> 1606886417184
	1606886417664 [label=CudnnBatchNormBackward0]
	1606886418480 -> 1606886417664
	1606886418480 [label=ConvolutionBackward0]
	1606886420256 -> 1606886418480
	1606886420256 [label=ReluBackward0]
	1606886420880 -> 1606886420256
	1606886420880 [label=CudnnBatchNormBackward0]
	1606886421360 -> 1606886420880
	1606886421360 [label=ConvolutionBackward0]
	1606886422032 -> 1606886421360
	1606886422032 [label=ReluBackward0]
	1606886422224 -> 1606886422032
	1606886422224 [label=CudnnBatchNormBackward0]
	1606886422656 -> 1606886422224
	1606886422656 [label=ConvolutionBackward0]
	1606886418096 -> 1606886422656
	1606886418096 [label=ReluBackward0]
	1606886424336 -> 1606886418096
	1606886424336 [label=AddBackward0]
	1606886416032 -> 1606886424336
	1606886416032 [label=CudnnBatchNormBackward0]
	1606886419392 -> 1606886416032
	1606886419392 [label=ConvolutionBackward0]
	1606886418576 -> 1606886419392
	1606886418576 [label=ReluBackward0]
	1606886423808 -> 1606886418576
	1606886423808 [label=CudnnBatchNormBackward0]
	1606886417328 -> 1606886423808
	1606886417328 [label=ConvolutionBackward0]
	1606886421264 -> 1606886417328
	1606886421264 [label=ReluBackward0]
	1606886413248 -> 1606886421264
	1606886413248 [label=CudnnBatchNormBackward0]
	1606886418000 -> 1606886413248
	1606886418000 [label=ConvolutionBackward0]
	1606886329456 -> 1606886418000
	1606886329456 [label=ReluBackward0]
	1606886417760 -> 1606886329456
	1606886417760 [label=AddBackward0]
	1606886415312 -> 1606886417760
	1606886415312 [label=CudnnBatchNormBackward0]
	1606886409744 -> 1606886415312
	1606886409744 [label=ConvolutionBackward0]
	1606886412576 -> 1606886409744
	1606886412576 [label=ReluBackward0]
	1606886414016 -> 1606886412576
	1606886414016 [label=CudnnBatchNormBackward0]
	1606886416560 -> 1606886414016
	1606886416560 [label=ConvolutionBackward0]
	1606886419440 -> 1606886416560
	1606886419440 [label=ReluBackward0]
	1606886421504 -> 1606886419440
	1606886421504 [label=CudnnBatchNormBackward0]
	1606886422896 -> 1606886421504
	1606886422896 [label=ConvolutionBackward0]
	1606886408592 -> 1606886422896
	1606886408592 [label=ReluBackward0]
	1606886423040 -> 1606886408592
	1606886423040 [label=AddBackward0]
	1606886419776 -> 1606886423040
	1606886419776 [label=CudnnBatchNormBackward0]
	1606886420064 -> 1606886419776
	1606886420064 [label=ConvolutionBackward0]
	1606886421168 -> 1606886420064
	1606886421168 [label=ReluBackward0]
	1606886410752 -> 1606886421168
	1606886410752 [label=CudnnBatchNormBackward0]
	1606886422512 -> 1606886410752
	1606886422512 [label=ConvolutionBackward0]
	1606886421024 -> 1606886422512
	1606886421024 [label=ReluBackward0]
	1606886411856 -> 1606886421024
	1606886411856 [label=CudnnBatchNormBackward0]
	1606886409072 -> 1606886411856
	1606886409072 [label=ConvolutionBackward0]
	1606886418336 -> 1606886409072
	1606886418336 [label=ReluBackward0]
	1606886614320 -> 1606886418336
	1606886614320 [label=AddBackward0]
	1606886609712 -> 1606886614320
	1606886609712 [label=CudnnBatchNormBackward0]
	1606886611920 -> 1606886609712
	1606886611920 [label=ConvolutionBackward0]
	1606886605344 -> 1606886611920
	1606886605344 [label=ReluBackward0]
	1606886611440 -> 1606886605344
	1606886611440 [label=CudnnBatchNormBackward0]
	1606886607360 -> 1606886611440
	1606886607360 [label=ConvolutionBackward0]
	1606886617584 -> 1606886607360
	1606886617584 [label=ReluBackward0]
	1606886612832 -> 1606886617584
	1606886612832 [label=CudnnBatchNormBackward0]
	1606886606784 -> 1606886612832
	1606886606784 [label=ConvolutionBackward0]
	1606886605296 -> 1606886606784
	1606886605296 [label=MaxPool2DWithIndicesBackward0]
	1606886329360 -> 1606886605296
	1606886329360 [label=ReluBackward0]
	1606886613888 -> 1606886329360
	1606886613888 [label=CudnnBatchNormBackward0]
	1606886614368 -> 1606886613888
	1606886614368 [label=ConvolutionBackward0]
	1606886612976 -> 1606886614368
	1606886157408 [label="encoder.conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	1606886157408 -> 1606886612976
	1606886612976 [label=AccumulateGrad]
	1606886617488 -> 1606886613888
	1606886157568 [label="encoder.bn1.weight
 (64)" fillcolor=lightblue]
	1606886157568 -> 1606886617488
	1606886617488 [label=AccumulateGrad]
	1606886616624 -> 1606886613888
	1606886157168 [label="encoder.bn1.bias
 (64)" fillcolor=lightblue]
	1606886157168 -> 1606886616624
	1606886616624 [label=AccumulateGrad]
	1606886606352 -> 1606886606784
	1606886153808 [label="encoder.layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	1606886153808 -> 1606886606352
	1606886606352 [label=AccumulateGrad]
	1606886616720 -> 1606886612832
	1606886153888 [label="encoder.layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1606886153888 -> 1606886616720
	1606886616720 [label=AccumulateGrad]
	1606886619264 -> 1606886612832
	1606886153648 [label="encoder.layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1606886153648 -> 1606886619264
	1606886619264 [label=AccumulateGrad]
	1606886618496 -> 1606886607360
	1606886152928 [label="encoder.layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1606886152928 -> 1606886618496
	1606886618496 [label=AccumulateGrad]
	1606886612352 -> 1606886611440
	1606886152848 [label="encoder.layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1606886152848 -> 1606886612352
	1606886612352 [label=AccumulateGrad]
	1606886611344 -> 1606886611440
	1606886153088 [label="encoder.layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1606886153088 -> 1606886611344
	1606886611344 [label=AccumulateGrad]
	1606886608368 -> 1606886611920
	1606886152768 [label="encoder.layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1606886152768 -> 1606886608368
	1606886608368 [label=AccumulateGrad]
	1606886617968 -> 1606886609712
	1606886152608 [label="encoder.layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	1606886152608 -> 1606886617968
	1606886617968 [label=AccumulateGrad]
	1606886604864 -> 1606886609712
	1606886149248 [label="encoder.layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	1606886149248 -> 1606886604864
	1606886604864 [label=AccumulateGrad]
	1606886619120 -> 1606886614320
	1606886619120 [label=CudnnBatchNormBackward0]
	1606886616336 -> 1606886619120
	1606886616336 [label=ConvolutionBackward0]
	1606886605296 -> 1606886616336
	1606886605920 -> 1606886616336
	1606886154368 [label="encoder.layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1606886154368 -> 1606886605920
	1606886605920 [label=AccumulateGrad]
	1606886615328 -> 1606886619120
	1606886154448 [label="encoder.layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1606886154448 -> 1606886615328
	1606886615328 [label=AccumulateGrad]
	1606886611488 -> 1606886619120
	1606886154528 [label="encoder.layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1606886154528 -> 1606886611488
	1606886611488 [label=AccumulateGrad]
	1606886618832 -> 1606886409072
	1606886148848 [label="encoder.layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	1606886148848 -> 1606886618832
	1606886618832 [label=AccumulateGrad]
	1606886410176 -> 1606886411856
	1606886148768 [label="encoder.layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1606886148768 -> 1606886410176
	1606886410176 [label=AccumulateGrad]
	1606886422416 -> 1606886411856
	1606886148928 [label="encoder.layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1606886148928 -> 1606886422416
	1606886422416 [label=AccumulateGrad]
	1606886424432 -> 1606886422512
	1606886148048 [label="encoder.layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1606886148048 -> 1606886424432
	1606886424432 [label=AccumulateGrad]
	1606886415504 -> 1606886410752
	1606886147968 [label="encoder.layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1606886147968 -> 1606886415504
	1606886415504 [label=AccumulateGrad]
	1606886413392 -> 1606886410752
	1606886148128 [label="encoder.layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1606886148128 -> 1606886413392
	1606886413392 [label=AccumulateGrad]
	1606886423424 -> 1606886420064
	1606886147248 [label="encoder.layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1606886147248 -> 1606886423424
	1606886423424 [label=AccumulateGrad]
	1606886411712 -> 1606886419776
	1606886147328 [label="encoder.layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	1606886147328 -> 1606886411712
	1606886411712 [label=AccumulateGrad]
	1606886417904 -> 1606886419776
	1606886147488 [label="encoder.layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	1606886147488 -> 1606886417904
	1606886417904 [label=AccumulateGrad]
	1606886418336 -> 1606886423040
	1606886413872 -> 1606886422896
	1606886147008 [label="encoder.layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	1606886147008 -> 1606886413872
	1606886413872 [label=AccumulateGrad]
	1606886424096 -> 1606886421504
	1606886146528 [label="encoder.layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	1606886146528 -> 1606886424096
	1606886424096 [label=AccumulateGrad]
	1606886420304 -> 1606886421504
	1606886146688 [label="encoder.layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	1606886146688 -> 1606886420304
	1606886420304 [label=AccumulateGrad]
	1606886419728 -> 1606886416560
	1606885920112 [label="encoder.layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1606885920112 -> 1606886419728
	1606886419728 [label=AccumulateGrad]
	1606886417232 -> 1606886414016
	1606885916832 [label="encoder.layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	1606885916832 -> 1606886417232
	1606886417232 [label=AccumulateGrad]
	1606886413680 -> 1606886414016
	1606885918672 [label="encoder.layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	1606885918672 -> 1606886413680
	1606886413680 [label=AccumulateGrad]
	1606886412960 -> 1606886409744
	1606885920512 [label="encoder.layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1606885920512 -> 1606886412960
	1606886412960 [label=AccumulateGrad]
	1606886409936 -> 1606886415312
	1606885920672 [label="encoder.layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	1606885920672 -> 1606886409936
	1606886409936 [label=AccumulateGrad]
	1606886408640 -> 1606886415312
	1606885920592 [label="encoder.layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	1606885920592 -> 1606886408640
	1606886408640 [label=AccumulateGrad]
	1606886408592 -> 1606886417760
	1606886412192 -> 1606886418000
	1607492920784 [label="encoder.layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	1607492920784 -> 1606886412192
	1606886412192 [label=AccumulateGrad]
	1606886414736 -> 1606886413248
	1607492917504 [label="encoder.layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1607492917504 -> 1606886414736
	1606886414736 [label=AccumulateGrad]
	1606886415696 -> 1606886413248
	1607492926624 [label="encoder.layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1607492926624 -> 1606886415696
	1606886415696 [label=AccumulateGrad]
	1606886418960 -> 1606886417328
	1607492924304 [label="encoder.layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1607492924304 -> 1606886418960
	1606886418960 [label=AccumulateGrad]
	1606886423184 -> 1606886423808
	1607492912464 [label="encoder.layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1607492912464 -> 1606886423184
	1606886423184 [label=AccumulateGrad]
	1606886421600 -> 1606886423808
	1607492913024 [label="encoder.layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1607492913024 -> 1606886421600
	1606886421600 [label=AccumulateGrad]
	1606886411760 -> 1606886419392
	1607492921344 [label="encoder.layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1607492921344 -> 1606886411760
	1606886411760 [label=AccumulateGrad]
	1606886408352 -> 1606886416032
	1607492916784 [label="encoder.layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	1607492916784 -> 1606886408352
	1606886408352 [label=AccumulateGrad]
	1606886417712 -> 1606886416032
	1607492920064 [label="encoder.layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	1607492920064 -> 1606886417712
	1606886417712 [label=AccumulateGrad]
	1606886417472 -> 1606886424336
	1606886417472 [label=CudnnBatchNormBackward0]
	1606886413056 -> 1606886417472
	1606886413056 [label=ConvolutionBackward0]
	1606886329456 -> 1606886413056
	1606886423520 -> 1606886413056
	1607492921504 [label="encoder.layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1607492921504 -> 1606886423520
	1606886423520 [label=AccumulateGrad]
	1606886414304 -> 1606886417472
	1607492917584 [label="encoder.layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1607492917584 -> 1606886414304
	1606886414304 [label=AccumulateGrad]
	1606886417376 -> 1606886417472
	1607492920704 [label="encoder.layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1607492920704 -> 1606886417376
	1606886417376 [label=AccumulateGrad]
	1606886423664 -> 1606886422656
	1607492916384 [label="encoder.layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1607492916384 -> 1606886423664
	1606886423664 [label=AccumulateGrad]
	1606886422704 -> 1606886422224
	1607492913984 [label="encoder.layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1607492913984 -> 1606886422704
	1606886422704 [label=AccumulateGrad]
	1606886422176 -> 1606886422224
	1607492914464 [label="encoder.layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1607492914464 -> 1606886422176
	1606886422176 [label=AccumulateGrad]
	1606886422128 -> 1606886421360
	1607492923424 [label="encoder.layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1607492923424 -> 1606886422128
	1606886422128 [label=AccumulateGrad]
	1606886421408 -> 1606886420880
	1607492912624 [label="encoder.layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1607492912624 -> 1606886421408
	1606886421408 [label=AccumulateGrad]
	1606886420832 -> 1606886420880
	1607492920464 [label="encoder.layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1607492920464 -> 1606886420832
	1606886420832 [label=AccumulateGrad]
	1606886420784 -> 1606886418480
	1607492916224 [label="encoder.layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1607492916224 -> 1606886420784
	1606886420784 [label=AccumulateGrad]
	1606886418624 -> 1606886417664
	1607492915984 [label="encoder.layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	1607492915984 -> 1606886418624
	1606886418624 [label=AccumulateGrad]
	1606886418192 -> 1606886417664
	1607492922144 [label="encoder.layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	1607492922144 -> 1606886418192
	1606886418192 [label=AccumulateGrad]
	1606886418096 -> 1606886417184
	1606886417088 -> 1606886416272
	1607492915824 [label="encoder.layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1607492915824 -> 1606886417088
	1606886417088 [label=AccumulateGrad]
	1606886416464 -> 1606886415456
	1607492925904 [label="encoder.layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	1607492925904 -> 1606886416464
	1606886416464 [label=AccumulateGrad]
	1606886415360 -> 1606886415456
	1607492918064 [label="encoder.layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	1607492918064 -> 1606886415360
	1606886415360 [label=AccumulateGrad]
	1606886415168 -> 1606886413968
	1607492917824 [label="encoder.layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1607492917824 -> 1606886415168
	1606886415168 [label=AccumulateGrad]
	1606886414160 -> 1606886413584
	1607492920304 [label="encoder.layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	1607492920304 -> 1606886414160
	1606886414160 [label=AccumulateGrad]
	1606886413296 -> 1606886413584
	1607492912544 [label="encoder.layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	1607492912544 -> 1606886413296
	1606886413296 [label=AccumulateGrad]
	1606886412864 -> 1606886411520
	1607492911984 [label="encoder.layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1607492911984 -> 1606886412864
	1606886412864 [label=AccumulateGrad]
	1606886411616 -> 1606886411328
	1607492912784 [label="encoder.layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	1607492912784 -> 1606886411616
	1606886411616 [label=AccumulateGrad]
	1606886411424 -> 1606886411328
	1607492924544 [label="encoder.layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	1607492924544 -> 1606886411424
	1606886411424 [label=AccumulateGrad]
	1606886411376 -> 1606886410848
	1606886424000 -> 1606886409120
	1607492921824 [label="encoder.layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1607492921824 -> 1606886424000
	1606886424000 [label=AccumulateGrad]
	1606886409552 -> 1606886408496
	1607492919984 [label="encoder.layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	1607492919984 -> 1606886409552
	1606886409552 [label=AccumulateGrad]
	1606886408448 -> 1606886408496
	1607492919344 [label="encoder.layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	1607492919344 -> 1606886408448
	1606886408448 [label=AccumulateGrad]
	1606886420160 -> 1606886412816
	1607492916304 [label="encoder.layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1607492916304 -> 1606886420160
	1606886420160 [label=AccumulateGrad]
	1606886415744 -> 1606886410080
	1607492923104 [label="encoder.layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	1607492923104 -> 1606886415744
	1606886415744 [label=AccumulateGrad]
	1606886414496 -> 1606886410080
	1607492926704 [label="encoder.layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	1607492926704 -> 1606886414496
	1606886414496 [label=AccumulateGrad]
	1606886416992 -> 1606886417568
	1607492925104 [label="encoder.layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1607492925104 -> 1606886416992
	1606886416992 [label=AccumulateGrad]
	1606886420448 -> 1606886409264
	1607492921424 [label="encoder.layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	1607492921424 -> 1606886420448
	1606886420448 [label=AccumulateGrad]
	1606886414640 -> 1606886409264
	1607492925184 [label="encoder.layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	1607492925184 -> 1606886414640
	1606886414640 [label=AccumulateGrad]
	1606886412384 -> 1606886410560
	1606886420112 -> 1606886419872
	1607492925024 [label="encoder.layer2.4.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1607492925024 -> 1606886420112
	1606886420112 [label=AccumulateGrad]
	1606886419632 -> 1606886421648
	1607492923984 [label="encoder.layer2.4.bn1.weight
 (128)" fillcolor=lightblue]
	1607492923984 -> 1606886419632
	1606886419632 [label=AccumulateGrad]
	1606886408256 -> 1606886421648
	1607492915504 [label="encoder.layer2.4.bn1.bias
 (128)" fillcolor=lightblue]
	1607492915504 -> 1606886408256
	1606886408256 [label=AccumulateGrad]
	1606886408688 -> 1606886411280
	1607492914384 [label="encoder.layer2.4.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1607492914384 -> 1606886408688
	1606886408688 [label=AccumulateGrad]
	1606886411136 -> 1606886412768
	1607492925264 [label="encoder.layer2.4.bn2.weight
 (128)" fillcolor=lightblue]
	1607492925264 -> 1606886411136
	1606886411136 [label=AccumulateGrad]
	1606886413152 -> 1606886412768
	1607492918304 [label="encoder.layer2.4.bn2.bias
 (128)" fillcolor=lightblue]
	1607492918304 -> 1606886413152
	1606886413152 [label=AccumulateGrad]
	1606886413824 -> 1606886418432
	1607492915104 [label="encoder.layer2.4.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1607492915104 -> 1606886413824
	1606886413824 [label=AccumulateGrad]
	1606886417424 -> 1606886419968
	1607492926224 [label="encoder.layer2.4.bn3.weight
 (512)" fillcolor=lightblue]
	1607492926224 -> 1606886417424
	1606886417424 [label=AccumulateGrad]
	1606886419152 -> 1606886419968
	1607492917184 [label="encoder.layer2.4.bn3.bias
 (512)" fillcolor=lightblue]
	1607492917184 -> 1606886419152
	1606886419152 [label=AccumulateGrad]
	1606886419536 -> 1606886421696
	1606886423088 -> 1606886418144
	1607492911664 [label="encoder.layer2.5.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1607492911664 -> 1606886423088
	1606886423088 [label=AccumulateGrad]
	1606886423568 -> 1606886421072
	1607492922384 [label="encoder.layer2.5.bn1.weight
 (128)" fillcolor=lightblue]
	1607492922384 -> 1606886423568
	1606886423568 [label=AccumulateGrad]
	1606886419824 -> 1606886421072
	1607492913504 [label="encoder.layer2.5.bn1.bias
 (128)" fillcolor=lightblue]
	1607492913504 -> 1606886419824
	1606886419824 [label=AccumulateGrad]
	1606886410896 -> 1606886410368
	1607492924144 [label="encoder.layer2.5.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1607492924144 -> 1606886410896
	1606886410896 [label=AccumulateGrad]
	1606886408304 -> 1606886416752
	1607492911504 [label="encoder.layer2.5.bn2.weight
 (128)" fillcolor=lightblue]
	1607492911504 -> 1606886408304
	1606886408304 [label=AccumulateGrad]
	1606886419488 -> 1606886416752
	1607492920224 [label="encoder.layer2.5.bn2.bias
 (128)" fillcolor=lightblue]
	1607492920224 -> 1606886419488
	1606886419488 [label=AccumulateGrad]
	1606886421552 -> 1606886409168
	1607492923904 [label="encoder.layer2.5.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1607492923904 -> 1606886421552
	1606886421552 [label=AccumulateGrad]
	1606886410032 -> 1606886424480
	1607492914704 [label="encoder.layer2.5.bn3.weight
 (512)" fillcolor=lightblue]
	1607492914704 -> 1606886410032
	1606886410032 [label=AccumulateGrad]
	1606886419344 -> 1606886424480
	1607492925744 [label="encoder.layer2.5.bn3.bias
 (512)" fillcolor=lightblue]
	1607492925744 -> 1606886419344
	1606886419344 [label=AccumulateGrad]
	1606886420928 -> 1606886410704
	1606886413440 -> 1606886408784
	1607492925824 [label="encoder.layer2.6.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1607492925824 -> 1606886413440
	1606886413440 [label=AccumulateGrad]
	1606886408544 -> 1606886408928
	1607492926304 [label="encoder.layer2.6.bn1.weight
 (128)" fillcolor=lightblue]
	1607492926304 -> 1606886408544
	1606886408544 [label=AccumulateGrad]
	1606886409024 -> 1606886408928
	1607492924784 [label="encoder.layer2.6.bn1.bias
 (128)" fillcolor=lightblue]
	1607492924784 -> 1606886409024
	1606886409024 [label=AccumulateGrad]
	1606886409216 -> 1606886410272
	1607492913824 [label="encoder.layer2.6.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1607492913824 -> 1606886409216
	1606886409216 [label=AccumulateGrad]
	1606886410128 -> 1606886410800
	1607492921984 [label="encoder.layer2.6.bn2.weight
 (128)" fillcolor=lightblue]
	1607492921984 -> 1606886410128
	1606886410128 [label=AccumulateGrad]
	1606886410992 -> 1606886410800
	1607492917024 [label="encoder.layer2.6.bn2.bias
 (128)" fillcolor=lightblue]
	1607492917024 -> 1606886410992
	1606886410992 [label=AccumulateGrad]
	1606886411088 -> 1606886412144
	1607492926144 [label="encoder.layer2.6.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1607492926144 -> 1606886411088
	1606886411088 [label=AccumulateGrad]
	1606886412048 -> 1606886412528
	1607492917424 [label="encoder.layer2.6.bn3.weight
 (512)" fillcolor=lightblue]
	1607492917424 -> 1606886412048
	1606886412048 [label=AccumulateGrad]
	1606886412336 -> 1606886412528
	1607492911264 [label="encoder.layer2.6.bn3.bias
 (512)" fillcolor=lightblue]
	1607492911264 -> 1606886412336
	1606886412336 [label=AccumulateGrad]
	1606886412480 -> 1606886413008
	1606886411808 -> 1606886414448
	1607492912304 [label="encoder.layer2.7.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1607492912304 -> 1606886411808
	1606886411808 [label=AccumulateGrad]
	1606886416944 -> 1606886414784
	1607492922304 [label="encoder.layer2.7.bn1.weight
 (128)" fillcolor=lightblue]
	1607492922304 -> 1606886416944
	1606886416944 [label=AccumulateGrad]
	1606886414928 -> 1606886414784
	1607492924624 [label="encoder.layer2.7.bn1.bias
 (128)" fillcolor=lightblue]
	1607492924624 -> 1606886414928
	1606886414928 [label=AccumulateGrad]
	1606886415024 -> 1606886416368
	1607492924464 [label="encoder.layer2.7.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1607492924464 -> 1606886415024
	1606886415024 [label=AccumulateGrad]
	1606886417856 -> 1606886416656
	1607492917264 [label="encoder.layer2.7.bn2.weight
 (128)" fillcolor=lightblue]
	1607492917264 -> 1606886417856
	1606886417856 [label=AccumulateGrad]
	1606886416800 -> 1606886416656
	1607492922704 [label="encoder.layer2.7.bn2.bias
 (128)" fillcolor=lightblue]
	1607492922704 -> 1606886416800
	1606886416800 [label=AccumulateGrad]
	1606886418048 -> 1606886418816
	1607492911424 [label="encoder.layer2.7.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1607492911424 -> 1606886418048
	1606886418048 [label=AccumulateGrad]
	1606886418768 -> 1606886419056
	1607492819600 [label="encoder.layer2.7.bn3.weight
 (512)" fillcolor=lightblue]
	1607492819600 -> 1606886418768
	1606886418768 [label=AccumulateGrad]
	1606886418864 -> 1606886419056
	1607492876352 [label="encoder.layer2.7.bn3.bias
 (512)" fillcolor=lightblue]
	1607492876352 -> 1606886418864
	1606886418864 [label=AccumulateGrad]
	1606886419008 -> 1606886420208
	1606886420400 -> 1606886421312
	1607492873552 [label="encoder.layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	1607492873552 -> 1606886420400
	1606886420400 [label=AccumulateGrad]
	1606886421216 -> 1606886414112
	1607492873232 [label="encoder.layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1607492873232 -> 1606886421216
	1606886421216 [label=AccumulateGrad]
	1606886414208 -> 1606886414112
	1607492870192 [label="encoder.layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1607492870192 -> 1606886414208
	1606886414208 [label=AccumulateGrad]
	1606886423136 -> 1606886423280
	1607492873472 [label="encoder.layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1607492873472 -> 1606886423136
	1606886423136 [label=AccumulateGrad]
	1606886422800 -> 1606886424048
	1607492873312 [label="encoder.layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1607492873312 -> 1606886422800
	1606886422800 [label=AccumulateGrad]
	1606886416128 -> 1606886424048
	1607492874112 [label="encoder.layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1607492874112 -> 1606886416128
	1606886416128 [label=AccumulateGrad]
	1606886424384 -> 1606886414880
	1607492876432 [label="encoder.layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1607492876432 -> 1606886424384
	1606886424384 [label=AccumulateGrad]
	1606886414400 -> 1606886417280
	1607492872992 [label="encoder.layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	1607492872992 -> 1606886414400
	1606886414400 [label=AccumulateGrad]
	1606886416320 -> 1606886417280
	1607492872912 [label="encoder.layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	1607492872912 -> 1606886416320
	1606886416320 [label=AccumulateGrad]
	1606886417136 -> 1606886421792
	1606886417136 [label=CudnnBatchNormBackward0]
	1606886420016 -> 1606886417136
	1606886420016 [label=ConvolutionBackward0]
	1606886329072 -> 1606886420016
	1606886422608 -> 1606886420016
	1607492864992 [label="encoder.layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	1607492864992 -> 1606886422608
	1606886422608 [label=AccumulateGrad]
	1606886413728 -> 1606886417136
	1607492874912 [label="encoder.layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	1607492874912 -> 1606886413728
	1606886413728 [label=AccumulateGrad]
	1606886413920 -> 1606886417136
	1607492869952 [label="encoder.layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	1607492869952 -> 1606886413920
	1606886413920 [label=AccumulateGrad]
	1606886274336 -> 1606886266272
	1607492873632 [label="encoder.layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1607492873632 -> 1606886274336
	1606886274336 [label=AccumulateGrad]
	1606886271120 -> 1606886275680
	1607492872752 [label="encoder.layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1607492872752 -> 1606886271120
	1606886271120 [label=AccumulateGrad]
	1606886262576 -> 1606886275680
	1607492871552 [label="encoder.layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1607492871552 -> 1606886262576
	1606886262576 [label=AccumulateGrad]
	1606886269392 -> 1606886267280
	1607492878032 [label="encoder.layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1607492878032 -> 1606886269392
	1606886269392 [label=AccumulateGrad]
	1606886266800 -> 1606886272512
	1607492871952 [label="encoder.layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1607492871952 -> 1606886266800
	1606886266800 [label=AccumulateGrad]
	1606886263296 -> 1606886272512
	1607492874832 [label="encoder.layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1607492874832 -> 1606886263296
	1606886263296 [label=AccumulateGrad]
	1606886272272 -> 1606886269776
	1607492876192 [label="encoder.layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1607492876192 -> 1606886272272
	1606886272272 [label=AccumulateGrad]
	1606886271840 -> 1606886268240
	1607492872592 [label="encoder.layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	1607492872592 -> 1606886271840
	1606886271840 [label=AccumulateGrad]
	1606886266992 -> 1606886268240
	1607492877792 [label="encoder.layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	1607492877792 -> 1606886266992
	1606886266992 [label=AccumulateGrad]
	1606886266416 -> 1606886264064
	1606886261472 -> 1606886268096
	1607492870032 [label="encoder.layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1607492870032 -> 1606886261472
	1606886261472 [label=AccumulateGrad]
	1606886267040 -> 1606886262048
	1607492875552 [label="encoder.layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	1607492875552 -> 1606886267040
	1606886267040 [label=AccumulateGrad]
	1606886263152 -> 1606886262048
	1607492877152 [label="encoder.layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	1607492877152 -> 1606886263152
	1606886263152 [label=AccumulateGrad]
	1606886274528 -> 1606886267520
	1607492878112 [label="encoder.layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1607492878112 -> 1606886274528
	1606886274528 [label=AccumulateGrad]
	1606886266608 -> 1606886267856
	1607492872512 [label="encoder.layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	1607492872512 -> 1606886266608
	1606886266608 [label=AccumulateGrad]
	1606886270880 -> 1606886267856
	1607492871312 [label="encoder.layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	1607492871312 -> 1606886270880
	1606886270880 [label=AccumulateGrad]
	1606886267088 -> 1606886265360
	1607492870272 [label="encoder.layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1607492870272 -> 1606886267088
	1606886267088 [label=AccumulateGrad]
	1606886270064 -> 1606886271936
	1607492877392 [label="encoder.layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	1607492877392 -> 1606886270064
	1606886270064 [label=AccumulateGrad]
	1606886264736 -> 1606886271936
	1607492870752 [label="encoder.layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	1607492870752 -> 1606886264736
	1606886264736 [label=AccumulateGrad]
	1606886264592 -> 1606886276304
	1606886265936 -> 1606886276832
	1607492871232 [label="encoder.layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1607492871232 -> 1606886265936
	1606886265936 [label=AccumulateGrad]
	1606886276880 -> 1606886276352
	1607492874672 [label="encoder.layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	1607492874672 -> 1606886276880
	1606886276880 [label=AccumulateGrad]
	1606886275728 -> 1606886276352
	1607492871472 [label="encoder.layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	1607492871472 -> 1606886275728
	1606886275728 [label=AccumulateGrad]
	1606886275008 -> 1606886273904
	1607492872832 [label="encoder.layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1607492872832 -> 1606886275008
	1606886275008 [label=AccumulateGrad]
	1606886274096 -> 1606886273472
	1607492877552 [label="encoder.layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	1607492877552 -> 1606886274096
	1606886274096 [label=AccumulateGrad]
	1606886273328 -> 1606886273472
	1607492870352 [label="encoder.layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	1607492870352 -> 1606886273328
	1606886273328 [label=AccumulateGrad]
	1606886272368 -> 1606886271264
	1607492872672 [label="encoder.layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1607492872672 -> 1606886272368
	1606886272368 [label=AccumulateGrad]
	1606886271312 -> 1606886270448
	1607492877952 [label="encoder.layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	1607492877952 -> 1606886271312
	1606886271312 [label=AccumulateGrad]
	1606886270784 -> 1606886270448
	1607492869792 [label="encoder.layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	1607492869792 -> 1606886270784
	1606886270784 [label=AccumulateGrad]
	1606886270688 -> 1606886269920
	1606886268864 -> 1606886267904
	1607492869312 [label="encoder.layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1607492869312 -> 1606886268864
	1606886268864 [label=AccumulateGrad]
	1606886268336 -> 1606886267376
	1607492874272 [label="encoder.layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	1607492874272 -> 1606886268336
	1606886268336 [label=AccumulateGrad]
	1606886266848 -> 1606886267376
	1607492871872 [label="encoder.layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	1607492871872 -> 1606886266848
	1606886266848 [label=AccumulateGrad]
	1606886266464 -> 1606886263872
	1606705640416 [label="encoder.layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1606705640416 -> 1606886266464
	1606886266464 [label=AccumulateGrad]
	1606886263968 -> 1606886262624
	1606705642976 [label="encoder.layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	1606705642976 -> 1606886263968
	1606886263968 [label=AccumulateGrad]
	1606886262000 -> 1606886262624
	1606705639776 [label="encoder.layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	1606705639776 -> 1606886262000
	1606886262000 [label=AccumulateGrad]
	1606886261568 -> 1606886269440
	1606885389120 [label="encoder.layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1606885389120 -> 1606886261568
	1606886261568 [label=AccumulateGrad]
	1606886275344 -> 1606886262432
	1606885378240 [label="encoder.layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	1606885378240 -> 1606886275344
	1606886275344 [label=AccumulateGrad]
	1606886265408 -> 1606886262432
	1606885376400 [label="encoder.layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	1606885376400 -> 1606886265408
	1606886265408 [label=AccumulateGrad]
	1606886270976 -> 1606886261040
	1606886275392 -> 1606886261904
	1606885041136 [label="encoder.layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1606885041136 -> 1606886275392
	1606886275392 [label=AccumulateGrad]
	1606886269536 -> 1606886273232
	1606885034176 [label="encoder.layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	1606885034176 -> 1606886269536
	1606886269536 [label=AccumulateGrad]
	1606886275776 -> 1606886273232
	1606885036016 [label="encoder.layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	1606885036016 -> 1606886275776
	1606886275776 [label=AccumulateGrad]
	1606886266512 -> 1606886262720
	1606885036656 [label="encoder.layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1606885036656 -> 1606886266512
	1606886266512 [label=AccumulateGrad]
	1606886276496 -> 1606886269584
	1606885046896 [label="encoder.layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	1606885046896 -> 1606886276496
	1606886276496 [label=AccumulateGrad]
	1606886268432 -> 1606886269584
	1606885048176 [label="encoder.layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	1606885048176 -> 1606886268432
	1606886268432 [label=AccumulateGrad]
	1606886266128 -> 1606886277072
	1606885203376 [label="encoder.layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1606885203376 -> 1606886266128
	1606886266128 [label=AccumulateGrad]
	1606886275200 -> 1606886264832
	1606885210816 [label="encoder.layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	1606885210816 -> 1606886275200
	1606886275200 [label=AccumulateGrad]
	1606886274816 -> 1606886264832
	1606885204016 [label="encoder.layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	1606885204016 -> 1606886274816
	1606886274816 [label=AccumulateGrad]
	1606886262816 -> 1606886271024
	1606886272032 -> 1606886275536
	1606885205776 [label="encoder.layer3.6.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1606885205776 -> 1606886272032
	1606886272032 [label=AccumulateGrad]
	1606886276592 -> 1606886261088
	1606885196336 [label="encoder.layer3.6.bn1.weight
 (256)" fillcolor=lightblue]
	1606885196336 -> 1606886276592
	1606886276592 [label=AccumulateGrad]
	1606886262480 -> 1606886261088
	1606885207056 [label="encoder.layer3.6.bn1.bias
 (256)" fillcolor=lightblue]
	1606885207056 -> 1606886262480
	1606886262480 [label=AccumulateGrad]
	1606886268480 -> 1606886263440
	1606885202736 [label="encoder.layer3.6.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1606885202736 -> 1606886268480
	1606886268480 [label=AccumulateGrad]
	1606886271168 -> 1606886272464
	1606885201456 [label="encoder.layer3.6.bn2.weight
 (256)" fillcolor=lightblue]
	1606885201456 -> 1606886271168
	1606886271168 [label=AccumulateGrad]
	1606886272224 -> 1606886272464
	1606885208336 [label="encoder.layer3.6.bn2.bias
 (256)" fillcolor=lightblue]
	1606885208336 -> 1606886272224
	1606886272224 [label=AccumulateGrad]
	1606886262240 -> 1606886268144
	1606705766528 [label="encoder.layer3.6.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1606705766528 -> 1606886262240
	1606886262240 [label=AccumulateGrad]
	1606886263776 -> 1606886263680
	1606705760928 [label="encoder.layer3.6.bn3.weight
 (1024)" fillcolor=lightblue]
	1606705760928 -> 1606886263776
	1606886263776 [label=AccumulateGrad]
	1606886269200 -> 1606886263680
	1606705760288 [label="encoder.layer3.6.bn3.bias
 (1024)" fillcolor=lightblue]
	1606705760288 -> 1606886269200
	1606886269200 [label=AccumulateGrad]
	1606886273760 -> 1606886276256
	1606886264448 -> 1606886265072
	1606705769088 [label="encoder.layer3.7.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1606705769088 -> 1606886264448
	1606886264448 [label=AccumulateGrad]
	1606886270496 -> 1606886261184
	1606705758448 [label="encoder.layer3.7.bn1.weight
 (256)" fillcolor=lightblue]
	1606705758448 -> 1606886270496
	1606886270496 [label=AccumulateGrad]
	1606886261520 -> 1606886261184
	1606705769648 [label="encoder.layer3.7.bn1.bias
 (256)" fillcolor=lightblue]
	1606705769648 -> 1606886261520
	1606886261520 [label=AccumulateGrad]
	1606886262096 -> 1606886262912
	1606597445824 [label="encoder.layer3.7.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1606597445824 -> 1606886262096
	1606886262096 [label=AccumulateGrad]
	1606886265744 -> 1606886269248
	1606597446784 [label="encoder.layer3.7.bn2.weight
 (256)" fillcolor=lightblue]
	1606597446784 -> 1606886265744
	1606886265744 [label=AccumulateGrad]
	1606886269872 -> 1606886269248
	1606885771056 [label="encoder.layer3.7.bn2.bias
 (256)" fillcolor=lightblue]
	1606885771056 -> 1606886269872
	1606886269872 [label=AccumulateGrad]
	1606886270832 -> 1606886276400
	1606885784416 [label="encoder.layer3.7.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1606885784416 -> 1606886270832
	1606886270832 [label=AccumulateGrad]
	1606886274432 -> 1606886260896
	1606885771936 [label="encoder.layer3.7.bn3.weight
 (1024)" fillcolor=lightblue]
	1606885771936 -> 1606886274432
	1606886274432 [label=AccumulateGrad]
	1606886276784 -> 1606886260896
	1606885783376 [label="encoder.layer3.7.bn3.bias
 (1024)" fillcolor=lightblue]
	1606885783376 -> 1606886276784
	1606886276784 [label=AccumulateGrad]
	1606886275872 -> 1606886274384
	1606886267232 -> 1606886268768
	1606885785136 [label="encoder.layer3.8.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1606885785136 -> 1606886267232
	1606886267232 [label=AccumulateGrad]
	1606886269488 -> 1606886261376
	1606885770896 [label="encoder.layer3.8.bn1.weight
 (256)" fillcolor=lightblue]
	1606885770896 -> 1606886269488
	1606886269488 [label=AccumulateGrad]
	1606886268048 -> 1606886261376
	1606885778656 [label="encoder.layer3.8.bn1.bias
 (256)" fillcolor=lightblue]
	1606885778656 -> 1606886268048
	1606886268048 [label=AccumulateGrad]
	1606886276016 -> 1606886265888
	1606885771776 [label="encoder.layer3.8.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1606885771776 -> 1606886276016
	1606886276016 [label=AccumulateGrad]
	1606886272800 -> 1606886275968
	1606885781696 [label="encoder.layer3.8.bn2.weight
 (256)" fillcolor=lightblue]
	1606885781696 -> 1606886272800
	1606886272800 [label=AccumulateGrad]
	1606886274048 -> 1606886275968
	1606885775696 [label="encoder.layer3.8.bn2.bias
 (256)" fillcolor=lightblue]
	1606885775696 -> 1606886274048
	1606886274048 [label=AccumulateGrad]
	1606886269056 -> 1606886274576
	1606885775056 [label="encoder.layer3.8.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1606885775056 -> 1606886269056
	1606886269056 [label=AccumulateGrad]
	1606886273520 -> 1606886275440
	1606885775856 [label="encoder.layer3.8.bn3.weight
 (1024)" fillcolor=lightblue]
	1606885775856 -> 1606886273520
	1606886273520 [label=AccumulateGrad]
	1606886269104 -> 1606886275440
	1606885776176 [label="encoder.layer3.8.bn3.bias
 (1024)" fillcolor=lightblue]
	1606885776176 -> 1606886269104
	1606886269104 [label=AccumulateGrad]
	1606886275296 -> 1606886275488
	1606886276976 -> 1606886261664
	1606885776256 [label="encoder.layer3.9.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1606885776256 -> 1606886276976
	1606886276976 [label=AccumulateGrad]
	1606886273184 -> 1606886275920
	1606885770336 [label="encoder.layer3.9.bn1.weight
 (256)" fillcolor=lightblue]
	1606885770336 -> 1606886273184
	1606886273184 [label=AccumulateGrad]
	1606886261616 -> 1606886275920
	1606885782256 [label="encoder.layer3.9.bn1.bias
 (256)" fillcolor=lightblue]
	1606885782256 -> 1606886261616
	1606886261616 [label=AccumulateGrad]
	1606886261952 -> 1606886261424
	1606885772336 [label="encoder.layer3.9.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1606885772336 -> 1606886261952
	1606886261952 [label=AccumulateGrad]
	1606886266944 -> 1606886264976
	1606885774896 [label="encoder.layer3.9.bn2.weight
 (256)" fillcolor=lightblue]
	1606885774896 -> 1606886266944
	1606886266944 [label=AccumulateGrad]
	1606886261136 -> 1606886264976
	1606885782416 [label="encoder.layer3.9.bn2.bias
 (256)" fillcolor=lightblue]
	1606885782416 -> 1606886261136
	1606886261136 [label=AccumulateGrad]
	1606886269296 -> 1606886260800
	1606885780096 [label="encoder.layer3.9.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1606885780096 -> 1606886269296
	1606886269296 [label=AccumulateGrad]
	1606886267760 -> 1606886275584
	1606885779216 [label="encoder.layer3.9.bn3.weight
 (1024)" fillcolor=lightblue]
	1606885779216 -> 1606886267760
	1606886267760 [label=AccumulateGrad]
	1606886260992 -> 1606886275584
	1606885782896 [label="encoder.layer3.9.bn3.bias
 (1024)" fillcolor=lightblue]
	1606885782896 -> 1606886260992
	1606886260992 [label=AccumulateGrad]
	1606886265696 -> 1606886261712
	1606886262672 -> 1606886263728
	1606885784576 [label="encoder.layer3.10.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1606885784576 -> 1606886262672
	1606886262672 [label=AccumulateGrad]
	1606886263248 -> 1606886264160
	1606885773696 [label="encoder.layer3.10.bn1.weight
 (256)" fillcolor=lightblue]
	1606885773696 -> 1606886263248
	1606886263248 [label=AccumulateGrad]
	1606886264208 -> 1606886264160
	1606885777936 [label="encoder.layer3.10.bn1.bias
 (256)" fillcolor=lightblue]
	1606885777936 -> 1606886264208
	1606886264208 [label=AccumulateGrad]
	1606886264496 -> 1606886265648
	1606885779136 [label="encoder.layer3.10.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1606885779136 -> 1606886264496
	1606886264496 [label=AccumulateGrad]
	1606886265312 -> 1606886267616
	1606885780656 [label="encoder.layer3.10.bn2.weight
 (256)" fillcolor=lightblue]
	1606885780656 -> 1606886265312
	1606886265312 [label=AccumulateGrad]
	1606886267808 -> 1606886267616
	1606885781936 [label="encoder.layer3.10.bn2.bias
 (256)" fillcolor=lightblue]
	1606885781936 -> 1606886267808
	1606886267808 [label=AccumulateGrad]
	1606886262144 -> 1606886269344
	1606885780816 [label="encoder.layer3.10.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1606885780816 -> 1606886262144
	1606886262144 [label=AccumulateGrad]
	1606886269152 -> 1606886269824
	1606885773376 [label="encoder.layer3.10.bn3.weight
 (1024)" fillcolor=lightblue]
	1606885773376 -> 1606886269152
	1606886269152 [label=AccumulateGrad]
	1606886269632 -> 1606886269824
	1606885770736 [label="encoder.layer3.10.bn3.bias
 (1024)" fillcolor=lightblue]
	1606885770736 -> 1606886269632
	1606886269632 [label=AccumulateGrad]
	1606886269680 -> 1606886270208
	1606886270736 -> 1606886271792
	1606885785456 [label="encoder.layer3.11.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1606885785456 -> 1606886270736
	1606886270736 [label=AccumulateGrad]
	1606886271408 -> 1606886272560
	1606885775376 [label="encoder.layer3.11.bn1.weight
 (256)" fillcolor=lightblue]
	1606885775376 -> 1606886271408
	1606886271408 [label=AccumulateGrad]
	1606886272656 -> 1606886272560
	1606885772416 [label="encoder.layer3.11.bn1.bias
 (256)" fillcolor=lightblue]
	1606885772416 -> 1606886272656
	1606886272656 [label=AccumulateGrad]
	1606886272848 -> 1606886274768
	1606885778976 [label="encoder.layer3.11.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1606885778976 -> 1606886272848
	1606886272848 [label=AccumulateGrad]
	1606886274192 -> 1606886275104
	1606885776016 [label="encoder.layer3.11.bn2.weight
 (256)" fillcolor=lightblue]
	1606885776016 -> 1606886274192
	1606886274192 [label=AccumulateGrad]
	1606886275632 -> 1606886275104
	1606885782176 [label="encoder.layer3.11.bn2.bias
 (256)" fillcolor=lightblue]
	1606885782176 -> 1606886275632
	1606886275632 [label=AccumulateGrad]
	1606886276112 -> 1606886264304
	1606885783056 [label="encoder.layer3.11.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1606885783056 -> 1606886276112
	1606886276112 [label=AccumulateGrad]
	1606886268816 -> 1606886267664
	1606885774096 [label="encoder.layer3.11.bn3.weight
 (1024)" fillcolor=lightblue]
	1606885774096 -> 1606886268816
	1606886268816 [label=AccumulateGrad]
	1606886265216 -> 1606886267664
	1606885778416 [label="encoder.layer3.11.bn3.bias
 (1024)" fillcolor=lightblue]
	1606885778416 -> 1606886265216
	1606886265216 [label=AccumulateGrad]
	1606886265504 -> 1606886062160
	1606886062400 -> 1606886056736
	1606885785056 [label="encoder.layer3.12.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1606885785056 -> 1606886062400
	1606886062400 [label=AccumulateGrad]
	1606886056928 -> 1606886053808
	1606885778336 [label="encoder.layer3.12.bn1.weight
 (256)" fillcolor=lightblue]
	1606885778336 -> 1606886056928
	1606886056928 [label=AccumulateGrad]
	1606886048192 -> 1606886053808
	1606885782816 [label="encoder.layer3.12.bn1.bias
 (256)" fillcolor=lightblue]
	1606885782816 -> 1606886048192
	1606886048192 [label=AccumulateGrad]
	1606886058368 -> 1606886064080
	1606885781376 [label="encoder.layer3.12.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1606885781376 -> 1606886058368
	1606886058368 [label=AccumulateGrad]
	1606886048096 -> 1606886056304
	1606885782976 [label="encoder.layer3.12.bn2.weight
 (256)" fillcolor=lightblue]
	1606885782976 -> 1606886048096
	1606886048096 [label=AccumulateGrad]
	1606886052128 -> 1606886056304
	1606885778816 [label="encoder.layer3.12.bn2.bias
 (256)" fillcolor=lightblue]
	1606885778816 -> 1606886052128
	1606886052128 [label=AccumulateGrad]
	1606886062688 -> 1606886051648
	1606885777216 [label="encoder.layer3.12.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1606885777216 -> 1606886062688
	1606886062688 [label=AccumulateGrad]
	1606886062016 -> 1606886055008
	1606885772016 [label="encoder.layer3.12.bn3.weight
 (1024)" fillcolor=lightblue]
	1606885772016 -> 1606886062016
	1606886062016 [label=AccumulateGrad]
	1606886062208 -> 1606886055008
	1606885778496 [label="encoder.layer3.12.bn3.bias
 (1024)" fillcolor=lightblue]
	1606885778496 -> 1606886062208
	1606886062208 [label=AccumulateGrad]
	1606886054576 -> 1606886054240
	1606886053280 -> 1606886055536
	1606885783936 [label="encoder.layer3.13.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1606885783936 -> 1606886053280
	1606886053280 [label=AccumulateGrad]
	1606886048288 -> 1606886049104
	1606885774816 [label="encoder.layer3.13.bn1.weight
 (256)" fillcolor=lightblue]
	1606885774816 -> 1606886048288
	1606886048288 [label=AccumulateGrad]
	1606886061920 -> 1606886049104
	1606885778176 [label="encoder.layer3.13.bn1.bias
 (256)" fillcolor=lightblue]
	1606885778176 -> 1606886061920
	1606886061920 [label=AccumulateGrad]
	1606886057936 -> 1606886058608
	1606885775936 [label="encoder.layer3.13.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1606885775936 -> 1606886057936
	1606886057936 [label=AccumulateGrad]
	1606886051552 -> 1606886061680
	1606885775776 [label="encoder.layer3.13.bn2.weight
 (256)" fillcolor=lightblue]
	1606885775776 -> 1606886051552
	1606886051552 [label=AccumulateGrad]
	1606886048672 -> 1606886061680
	1606885777856 [label="encoder.layer3.13.bn2.bias
 (256)" fillcolor=lightblue]
	1606885777856 -> 1606886048672
	1606886048672 [label=AccumulateGrad]
	1606886048528 -> 1606886057744
	1606885774576 [label="encoder.layer3.13.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1606885774576 -> 1606886048528
	1606886048528 [label=AccumulateGrad]
	1606886050976 -> 1606886059664
	1606885781776 [label="encoder.layer3.13.bn3.weight
 (1024)" fillcolor=lightblue]
	1606885781776 -> 1606886050976
	1606886050976 [label=AccumulateGrad]
	1606886052176 -> 1606886059664
	1606885783696 [label="encoder.layer3.13.bn3.bias
 (1024)" fillcolor=lightblue]
	1606885783696 -> 1606886052176
	1606886052176 [label=AccumulateGrad]
	1606886062592 -> 1606886054144
	1606886051888 -> 1606886048960
	1606885785376 [label="encoder.layer3.14.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1606885785376 -> 1606886051888
	1606886051888 [label=AccumulateGrad]
	1606886054912 -> 1606886063264
	1606885783456 [label="encoder.layer3.14.bn1.weight
 (256)" fillcolor=lightblue]
	1606885783456 -> 1606886054912
	1606886054912 [label=AccumulateGrad]
	1606886063024 -> 1606886063264
	1606885784896 [label="encoder.layer3.14.bn1.bias
 (256)" fillcolor=lightblue]
	1606885784896 -> 1606886063024
	1606886063024 [label=AccumulateGrad]
	1606886051696 -> 1606886050928
	1606705811920 [label="encoder.layer3.14.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1606705811920 -> 1606886051696
	1606886051696 [label=AccumulateGrad]
	1606886052848 -> 1606886061968
	1606705818880 [label="encoder.layer3.14.bn2.weight
 (256)" fillcolor=lightblue]
	1606705818880 -> 1606886052848
	1606886052848 [label=AccumulateGrad]
	1606886055824 -> 1606886061968
	1606705819520 [label="encoder.layer3.14.bn2.bias
 (256)" fillcolor=lightblue]
	1606705819520 -> 1606886055824
	1606886055824 [label=AccumulateGrad]
	1606886054048 -> 1606886053904
	1606705822720 [label="encoder.layer3.14.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1606705822720 -> 1606886054048
	1606886054048 [label=AccumulateGrad]
	1606886063648 -> 1606886059568
	1606705816320 [label="encoder.layer3.14.bn3.weight
 (1024)" fillcolor=lightblue]
	1606705816320 -> 1606886063648
	1606886063648 [label=AccumulateGrad]
	1606886060384 -> 1606886059568
	1606705809920 [label="encoder.layer3.14.bn3.bias
 (1024)" fillcolor=lightblue]
	1606705809920 -> 1606886060384
	1606886060384 [label=AccumulateGrad]
	1606886051120 -> 1606886060336
	1606886058704 -> 1606886056400
	1606885578768 [label="encoder.layer3.15.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1606885578768 -> 1606886058704
	1606886058704 [label=AccumulateGrad]
	1606886059808 -> 1606886059472
	1606885573888 [label="encoder.layer3.15.bn1.weight
 (256)" fillcolor=lightblue]
	1606885573888 -> 1606886059808
	1606886059808 [label=AccumulateGrad]
	1606886051168 -> 1606886059472
	1606885588448 [label="encoder.layer3.15.bn1.bias
 (256)" fillcolor=lightblue]
	1606885588448 -> 1606886051168
	1606886051168 [label=AccumulateGrad]
	1606886056544 -> 1606886057600
	1606885575008 [label="encoder.layer3.15.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1606885575008 -> 1606886056544
	1606886056544 [label=AccumulateGrad]
	1606886057792 -> 1606886049248
	1606885587808 [label="encoder.layer3.15.bn2.weight
 (256)" fillcolor=lightblue]
	1606885587808 -> 1606886057792
	1606886057792 [label=AccumulateGrad]
	1606886063936 -> 1606886049248
	1606885582528 [label="encoder.layer3.15.bn2.bias
 (256)" fillcolor=lightblue]
	1606885582528 -> 1606886063936
	1606886063936 [label=AccumulateGrad]
	1606886056592 -> 1606886059232
	1606885876160 [label="encoder.layer3.15.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1606885876160 -> 1606886056592
	1606886056592 [label=AccumulateGrad]
	1606886048048 -> 1606886048864
	1606885880880 [label="encoder.layer3.15.bn3.weight
 (1024)" fillcolor=lightblue]
	1606885880880 -> 1606886048048
	1606886048048 [label=AccumulateGrad]
	1606886063408 -> 1606886048864
	1606885869920 [label="encoder.layer3.15.bn3.bias
 (1024)" fillcolor=lightblue]
	1606885869920 -> 1606886063408
	1606886063408 [label=AccumulateGrad]
	1606886055344 -> 1606886062064
	1606886049680 -> 1606886060816
	1606885883360 [label="encoder.layer3.16.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1606885883360 -> 1606886049680
	1606886049680 [label=AccumulateGrad]
	1606886057888 -> 1606886052032
	1606885871840 [label="encoder.layer3.16.bn1.weight
 (256)" fillcolor=lightblue]
	1606885871840 -> 1606886057888
	1606886057888 [label=AccumulateGrad]
	1606886053472 -> 1606886052032
	1606885869360 [label="encoder.layer3.16.bn1.bias
 (256)" fillcolor=lightblue]
	1606885869360 -> 1606886053472
	1606886053472 [label=AccumulateGrad]
	1606886057504 -> 1606886053760
	1606705504464 [label="encoder.layer3.16.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1606705504464 -> 1606886057504
	1606886057504 [label=AccumulateGrad]
	1606886050208 -> 1606886052704
	1606705504704 [label="encoder.layer3.16.bn2.weight
 (256)" fillcolor=lightblue]
	1606705504704 -> 1606886050208
	1606886050208 [label=AccumulateGrad]
	1606886059136 -> 1606886052704
	1606705505104 [label="encoder.layer3.16.bn2.bias
 (256)" fillcolor=lightblue]
	1606705505104 -> 1606886059136
	1606886059136 [label=AccumulateGrad]
	1606886054336 -> 1606886049872
	1606705502944 [label="encoder.layer3.16.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1606705502944 -> 1606886054336
	1606886054336 [label=AccumulateGrad]
	1606886062784 -> 1606886063696
	1606705504064 [label="encoder.layer3.16.bn3.weight
 (1024)" fillcolor=lightblue]
	1606705504064 -> 1606886062784
	1606886062784 [label=AccumulateGrad]
	1606886062832 -> 1606886063696
	1606705504144 [label="encoder.layer3.16.bn3.bias
 (1024)" fillcolor=lightblue]
	1606705504144 -> 1606886062832
	1606886062832 [label=AccumulateGrad]
	1606886058896 -> 1606886059856
	1606886054480 -> 1606886050256
	1607101906640 [label="encoder.layer3.17.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1607101906640 -> 1606886054480
	1606886054480 [label=AccumulateGrad]
	1606886058032 -> 1606886063168
	1607405934928 [label="encoder.layer3.17.bn1.weight
 (256)" fillcolor=lightblue]
	1607405934928 -> 1606886058032
	1606886058032 [label=AccumulateGrad]
	1606886063984 -> 1606886063168
	1607405935008 [label="encoder.layer3.17.bn1.bias
 (256)" fillcolor=lightblue]
	1607405935008 -> 1606886063984
	1606886063984 [label=AccumulateGrad]
	1606886052944 -> 1606886059280
	1607405935488 [label="encoder.layer3.17.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1607405935488 -> 1606886052944
	1606886052944 [label=AccumulateGrad]
	1606886049008 -> 1606886061824
	1607405935408 [label="encoder.layer3.17.bn2.weight
 (256)" fillcolor=lightblue]
	1607405935408 -> 1606886049008
	1606886049008 [label=AccumulateGrad]
	1606886053712 -> 1606886061824
	1607405935568 [label="encoder.layer3.17.bn2.bias
 (256)" fillcolor=lightblue]
	1607405935568 -> 1606886053712
	1606886053712 [label=AccumulateGrad]
	1606886059616 -> 1606886055296
	1607405936048 [label="encoder.layer3.17.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1607405936048 -> 1606886059616
	1606886059616 [label=AccumulateGrad]
	1606886053136 -> 1606886060240
	1607405936128 [label="encoder.layer3.17.bn3.weight
 (1024)" fillcolor=lightblue]
	1607405936128 -> 1606886053136
	1606886053136 [label=AccumulateGrad]
	1606886054864 -> 1606886060240
	1607405936208 [label="encoder.layer3.17.bn3.bias
 (1024)" fillcolor=lightblue]
	1607405936208 -> 1606886054864
	1606886054864 [label=AccumulateGrad]
	1606886054096 -> 1606886049536
	1606886050640 -> 1606886054192
	1607405936688 [label="encoder.layer3.18.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1607405936688 -> 1606886050640
	1606886050640 [label=AccumulateGrad]
	1606886048000 -> 1606886051408
	1607405936768 [label="encoder.layer3.18.bn1.weight
 (256)" fillcolor=lightblue]
	1607405936768 -> 1606886048000
	1606886048000 [label=AccumulateGrad]
	1606886060192 -> 1606886051408
	1607405936848 [label="encoder.layer3.18.bn1.bias
 (256)" fillcolor=lightblue]
	1607405936848 -> 1606886060192
	1606886060192 [label=AccumulateGrad]
	1606886057216 -> 1606886049152
	1607405937328 [label="encoder.layer3.18.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1607405937328 -> 1606886057216
	1606886057216 [label=AccumulateGrad]
	1606886049056 -> 1606886055728
	1607405937248 [label="encoder.layer3.18.bn2.weight
 (256)" fillcolor=lightblue]
	1607405937248 -> 1606886049056
	1606886049056 [label=AccumulateGrad]
	1606886057840 -> 1606886055728
	1607405937408 [label="encoder.layer3.18.bn2.bias
 (256)" fillcolor=lightblue]
	1607405937408 -> 1606886057840
	1606886057840 [label=AccumulateGrad]
	1606886060864 -> 1606886063840
	1607405937808 [label="encoder.layer3.18.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1607405937808 -> 1606886060864
	1606886060864 [label=AccumulateGrad]
	1606886056448 -> 1606886061632
	1607405937888 [label="encoder.layer3.18.bn3.weight
 (1024)" fillcolor=lightblue]
	1607405937888 -> 1606886056448
	1606886056448 [label=AccumulateGrad]
	1606886062928 -> 1606886061632
	1607405937968 [label="encoder.layer3.18.bn3.bias
 (1024)" fillcolor=lightblue]
	1607405937968 -> 1606886062928
	1606886062928 [label=AccumulateGrad]
	1606886062736 -> 1606886058560
	1606886047952 -> 1606886051744
	1607405938448 [label="encoder.layer3.19.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1607405938448 -> 1606886047952
	1606886047952 [label=AccumulateGrad]
	1606886057456 -> 1606886055872
	1607405938528 [label="encoder.layer3.19.bn1.weight
 (256)" fillcolor=lightblue]
	1607405938528 -> 1606886057456
	1606886057456 [label=AccumulateGrad]
	1606886049296 -> 1606886055872
	1607405938608 [label="encoder.layer3.19.bn1.bias
 (256)" fillcolor=lightblue]
	1607405938608 -> 1606886049296
	1606886049296 [label=AccumulateGrad]
	1606886049200 -> 1606886049824
	1607405939168 [label="encoder.layer3.19.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1607405939168 -> 1606886049200
	1606886049200 [label=AccumulateGrad]
	1606886056352 -> 1606886051312
	1607405939088 [label="encoder.layer3.19.bn2.weight
 (256)" fillcolor=lightblue]
	1607405939088 -> 1606886056352
	1606886056352 [label=AccumulateGrad]
	1606886059328 -> 1606886051312
	1607405939248 [label="encoder.layer3.19.bn2.bias
 (256)" fillcolor=lightblue]
	1607405939248 -> 1606886059328
	1606886059328 [label=AccumulateGrad]
	1606886054432 -> 1606886048576
	1607405939728 [label="encoder.layer3.19.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1607405939728 -> 1606886054432
	1606886054432 [label=AccumulateGrad]
	1606886057696 -> 1606886058464
	1607405939808 [label="encoder.layer3.19.bn3.weight
 (1024)" fillcolor=lightblue]
	1607405939808 -> 1606886057696
	1606886057696 [label=AccumulateGrad]
	1606886061104 -> 1606886058464
	1607405939888 [label="encoder.layer3.19.bn3.bias
 (1024)" fillcolor=lightblue]
	1607405939888 -> 1606886061104
	1606886061104 [label=AccumulateGrad]
	1606886061152 -> 1606886061584
	1606886058992 -> 1606886048240
	1607405940288 [label="encoder.layer3.20.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1607405940288 -> 1606886058992
	1606886058992 [label=AccumulateGrad]
	1606886050688 -> 1606886060960
	1607405940368 [label="encoder.layer3.20.bn1.weight
 (256)" fillcolor=lightblue]
	1607405940368 -> 1606886050688
	1606886050688 [label=AccumulateGrad]
	1606886061296 -> 1606886060960
	1607405940448 [label="encoder.layer3.20.bn1.bias
 (256)" fillcolor=lightblue]
	1607405940448 -> 1606886061296
	1606886061296 [label=AccumulateGrad]
	1606886061392 -> 1606886060096
	1607405941008 [label="encoder.layer3.20.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1607405941008 -> 1606886061392
	1606886061392 [label=AccumulateGrad]
	1606886059040 -> 1606886048912
	1607405940928 [label="encoder.layer3.20.bn2.weight
 (256)" fillcolor=lightblue]
	1607405940928 -> 1606886059040
	1606886059040 [label=AccumulateGrad]
	1606886052320 -> 1606886048912
	1607405941088 [label="encoder.layer3.20.bn2.bias
 (256)" fillcolor=lightblue]
	1607405941088 -> 1606886052320
	1606886052320 [label=AccumulateGrad]
	1606886052896 -> 1606886058320
	1607405941568 [label="encoder.layer3.20.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1607405941568 -> 1606886052896
	1606886052896 [label=AccumulateGrad]
	1606886051504 -> 1606886047856
	1607405941648 [label="encoder.layer3.20.bn3.weight
 (1024)" fillcolor=lightblue]
	1607405941648 -> 1606886051504
	1606886051504 [label=AccumulateGrad]
	1606886060768 -> 1606886047856
	1607405941728 [label="encoder.layer3.20.bn3.bias
 (1024)" fillcolor=lightblue]
	1607405941728 -> 1606886060768
	1606886060768 [label=AccumulateGrad]
	1606886061008 -> 1606886054768
	1606886061056 -> 1606886052080
	1607405942208 [label="encoder.layer3.21.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1607405942208 -> 1606886061056
	1606886061056 [label=AccumulateGrad]
	1606886051024 -> 1606886053952
	1607405942288 [label="encoder.layer3.21.bn1.weight
 (256)" fillcolor=lightblue]
	1607405942288 -> 1606886051024
	1606886051024 [label=AccumulateGrad]
	1606886055392 -> 1606886053952
	1607405942368 [label="encoder.layer3.21.bn1.bias
 (256)" fillcolor=lightblue]
	1607405942368 -> 1606886055392
	1606886055392 [label=AccumulateGrad]
	1606886056160 -> 1606886058848
	1607405942928 [label="encoder.layer3.21.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1607405942928 -> 1606886056160
	1606886056160 [label=AccumulateGrad]
	1606886053088 -> 1606886049344
	1607405942848 [label="encoder.layer3.21.bn2.weight
 (256)" fillcolor=lightblue]
	1607405942848 -> 1606886053088
	1606886053088 [label=AccumulateGrad]
	1606886049968 -> 1606886049344
	1607405943008 [label="encoder.layer3.21.bn2.bias
 (256)" fillcolor=lightblue]
	1607405943008 -> 1606886049968
	1606886049968 [label=AccumulateGrad]
	1606886050352 -> 1606886050832
	1607405943488 [label="encoder.layer3.21.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1607405943488 -> 1606886050352
	1606886050352 [label=AccumulateGrad]
	1606886050784 -> 1606886052416
	1607405943568 [label="encoder.layer3.21.bn3.weight
 (1024)" fillcolor=lightblue]
	1607405943568 -> 1606886050784
	1606886050784 [label=AccumulateGrad]
	1606886051360 -> 1606886052416
	1607405943648 [label="encoder.layer3.21.bn3.bias
 (1024)" fillcolor=lightblue]
	1607405943648 -> 1606886051360
	1606886051360 [label=AccumulateGrad]
	1606886050112 -> 1606886060000
	1606886053424 -> 1606886056976
	1607405944048 [label="encoder.layer3.22.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1607405944048 -> 1606886053424
	1606886053424 [label=AccumulateGrad]
	1606886062304 -> 1606886050016
	1607405944128 [label="encoder.layer3.22.bn1.weight
 (256)" fillcolor=lightblue]
	1607405944128 -> 1606886062304
	1606886062304 [label=AccumulateGrad]
	1606886052272 -> 1606886050016
	1607405944208 [label="encoder.layer3.22.bn1.bias
 (256)" fillcolor=lightblue]
	1607405944208 -> 1606886052272
	1606886052272 [label=AccumulateGrad]
	1606886052368 -> 1606886054288
	1607405944768 [label="encoder.layer3.22.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1607405944768 -> 1606886052368
	1606886052368 [label=AccumulateGrad]
	1606886053376 -> 1606886057072
	1607405944608 [label="encoder.layer3.22.bn2.weight
 (256)" fillcolor=lightblue]
	1607405944608 -> 1606886053376
	1606886053376 [label=AccumulateGrad]
	1606886057168 -> 1606886057072
	1607406731344 [label="encoder.layer3.22.bn2.bias
 (256)" fillcolor=lightblue]
	1607406731344 -> 1606886057168
	1606886057168 [label=AccumulateGrad]
	1606886057312 -> 1606886060432
	1607406731824 [label="encoder.layer3.22.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1607406731824 -> 1606886057312
	1606886057312 [label=AccumulateGrad]
	1606886059520 -> 1606886063456
	1607406731904 [label="encoder.layer3.22.bn3.weight
 (1024)" fillcolor=lightblue]
	1607406731904 -> 1606886059520
	1606886059520 [label=AccumulateGrad]
	1606886060576 -> 1606886063456
	1607406731984 [label="encoder.layer3.22.bn3.bias
 (1024)" fillcolor=lightblue]
	1607406731984 -> 1606886060576
	1606886060576 [label=AccumulateGrad]
	1606886063312 -> 1606886553248
	1606886541296 -> 1606886551376
	1607406732464 [label="encoder.layer3.23.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1607406732464 -> 1606886541296
	1606886541296 [label=AccumulateGrad]
	1606886548688 -> 1606886553776
	1607406732544 [label="encoder.layer3.23.bn1.weight
 (256)" fillcolor=lightblue]
	1607406732544 -> 1606886548688
	1606886548688 [label=AccumulateGrad]
	1606886545136 -> 1606886553776
	1607406732624 [label="encoder.layer3.23.bn1.bias
 (256)" fillcolor=lightblue]
	1607406732624 -> 1606886545136
	1606886545136 [label=AccumulateGrad]
	1606886554304 -> 1606886551040
	1607406733104 [label="encoder.layer3.23.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1607406733104 -> 1606886554304
	1606886554304 [label=AccumulateGrad]
	1606886543840 -> 1606886550416
	1607406733024 [label="encoder.layer3.23.bn2.weight
 (256)" fillcolor=lightblue]
	1607406733024 -> 1606886543840
	1606886543840 [label=AccumulateGrad]
	1606886545904 -> 1606886550416
	1607406733184 [label="encoder.layer3.23.bn2.bias
 (256)" fillcolor=lightblue]
	1607406733184 -> 1606886545904
	1606886545904 [label=AccumulateGrad]
	1606886544800 -> 1606886544320
	1607406733584 [label="encoder.layer3.23.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1607406733584 -> 1606886544800
	1606886544800 [label=AccumulateGrad]
	1606886544704 -> 1606886552048
	1607406733664 [label="encoder.layer3.23.bn3.weight
 (1024)" fillcolor=lightblue]
	1607406733664 -> 1606886544704
	1606886544704 [label=AccumulateGrad]
	1606886540336 -> 1606886552048
	1607406733744 [label="encoder.layer3.23.bn3.bias
 (1024)" fillcolor=lightblue]
	1607406733744 -> 1606886540336
	1606886540336 [label=AccumulateGrad]
	1606886550944 -> 1606886542688
	1606886546048 -> 1606886545856
	1607406734144 [label="encoder.layer3.24.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1607406734144 -> 1606886546048
	1606886546048 [label=AccumulateGrad]
	1606886552672 -> 1606886539376
	1607406734224 [label="encoder.layer3.24.bn1.weight
 (256)" fillcolor=lightblue]
	1607406734224 -> 1606886552672
	1606886552672 [label=AccumulateGrad]
	1606886542976 -> 1606886539376
	1607406734304 [label="encoder.layer3.24.bn1.bias
 (256)" fillcolor=lightblue]
	1607406734304 -> 1606886542976
	1606886542976 [label=AccumulateGrad]
	1606886554448 -> 1606886540288
	1607406734864 [label="encoder.layer3.24.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1607406734864 -> 1606886554448
	1606886554448 [label=AccumulateGrad]
	1606886548976 -> 1606886539520
	1607406734784 [label="encoder.layer3.24.bn2.weight
 (256)" fillcolor=lightblue]
	1607406734784 -> 1606886548976
	1606886548976 [label=AccumulateGrad]
	1606886552960 -> 1606886539520
	1607406734944 [label="encoder.layer3.24.bn2.bias
 (256)" fillcolor=lightblue]
	1607406734944 -> 1606886552960
	1606886552960 [label=AccumulateGrad]
	1606886554400 -> 1606886543216
	1607406735424 [label="encoder.layer3.24.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1607406735424 -> 1606886554400
	1606886554400 [label=AccumulateGrad]
	1606886547920 -> 1606886550848
	1607406735504 [label="encoder.layer3.24.bn3.weight
 (1024)" fillcolor=lightblue]
	1607406735504 -> 1606886547920
	1606886547920 [label=AccumulateGrad]
	1606886544848 -> 1606886550848
	1607406735584 [label="encoder.layer3.24.bn3.bias
 (1024)" fillcolor=lightblue]
	1607406735584 -> 1606886544848
	1606886544848 [label=AccumulateGrad]
	1606886549312 -> 1606886539856
	1606886546432 -> 1606886553728
	1607406736064 [label="encoder.layer3.25.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1607406736064 -> 1606886546432
	1606886546432 [label=AccumulateGrad]
	1606886542160 -> 1606886545280
	1607406736144 [label="encoder.layer3.25.bn1.weight
 (256)" fillcolor=lightblue]
	1607406736144 -> 1606886542160
	1606886542160 [label=AccumulateGrad]
	1606886542112 -> 1606886545280
	1607406736224 [label="encoder.layer3.25.bn1.bias
 (256)" fillcolor=lightblue]
	1607406736224 -> 1606886542112
	1606886542112 [label=AccumulateGrad]
	1606886554592 -> 1606886548016
	1607406736784 [label="encoder.layer3.25.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1607406736784 -> 1606886554592
	1606886554592 [label=AccumulateGrad]
	1606886542400 -> 1606886543600
	1607406736704 [label="encoder.layer3.25.bn2.weight
 (256)" fillcolor=lightblue]
	1607406736704 -> 1606886542400
	1606886542400 [label=AccumulateGrad]
	1606886547200 -> 1606886543600
	1607406736864 [label="encoder.layer3.25.bn2.bias
 (256)" fillcolor=lightblue]
	1607406736864 -> 1606886547200
	1606886547200 [label=AccumulateGrad]
	1606886551952 -> 1606886547392
	1607406737344 [label="encoder.layer3.25.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1607406737344 -> 1606886551952
	1606886551952 [label=AccumulateGrad]
	1606886553392 -> 1606886541152
	1607406737424 [label="encoder.layer3.25.bn3.weight
 (1024)" fillcolor=lightblue]
	1607406737424 -> 1606886553392
	1606886553392 [label=AccumulateGrad]
	1606886544080 -> 1606886541152
	1607406737504 [label="encoder.layer3.25.bn3.bias
 (1024)" fillcolor=lightblue]
	1607406737504 -> 1606886544080
	1606886544080 [label=AccumulateGrad]
	1606886542064 -> 1606886544464
	1606886555024 -> 1606886542352
	1607406737984 [label="encoder.layer3.26.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1607406737984 -> 1606886555024
	1606886555024 [label=AccumulateGrad]
	1606886539328 -> 1606886542448
	1607406738064 [label="encoder.layer3.26.bn1.weight
 (256)" fillcolor=lightblue]
	1607406738064 -> 1606886539328
	1606886539328 [label=AccumulateGrad]
	1606886547008 -> 1606886542448
	1607406738144 [label="encoder.layer3.26.bn1.bias
 (256)" fillcolor=lightblue]
	1607406738144 -> 1606886547008
	1606886547008 [label=AccumulateGrad]
	1606886546384 -> 1606886548832
	1607406738624 [label="encoder.layer3.26.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1607406738624 -> 1606886546384
	1606886546384 [label=AccumulateGrad]
	1606886553056 -> 1606886555360
	1607406738544 [label="encoder.layer3.26.bn2.weight
 (256)" fillcolor=lightblue]
	1607406738544 -> 1606886553056
	1606886553056 [label=AccumulateGrad]
	1606886541488 -> 1606886555360
	1607406738704 [label="encoder.layer3.26.bn2.bias
 (256)" fillcolor=lightblue]
	1607406738704 -> 1606886541488
	1606886541488 [label=AccumulateGrad]
	1606886549216 -> 1606886547056
	1607406739104 [label="encoder.layer3.26.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1607406739104 -> 1606886549216
	1606886549216 [label=AccumulateGrad]
	1606886543888 -> 1606886552096
	1607406739184 [label="encoder.layer3.26.bn3.weight
 (1024)" fillcolor=lightblue]
	1607406739184 -> 1606886543888
	1606886543888 [label=AccumulateGrad]
	1606886541104 -> 1606886552096
	1607406739264 [label="encoder.layer3.26.bn3.bias
 (1024)" fillcolor=lightblue]
	1607406739264 -> 1606886541104
	1606886541104 [label=AccumulateGrad]
	1606886539568 -> 1606886542928
	1606886545568 -> 1606886542304
	1607406739744 [label="encoder.layer3.27.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1607406739744 -> 1606886545568
	1606886545568 [label=AccumulateGrad]
	1606886551616 -> 1606886549456
	1607406739824 [label="encoder.layer3.27.bn1.weight
 (256)" fillcolor=lightblue]
	1607406739824 -> 1606886551616
	1606886551616 [label=AccumulateGrad]
	1606886546624 -> 1606886549456
	1607406739904 [label="encoder.layer3.27.bn1.bias
 (256)" fillcolor=lightblue]
	1607406739904 -> 1606886546624
	1606886546624 [label=AccumulateGrad]
	1606886548640 -> 1606886553632
	1607406740384 [label="encoder.layer3.27.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1607406740384 -> 1606886548640
	1606886548640 [label=AccumulateGrad]
	1606886546240 -> 1606886548112
	1607406740304 [label="encoder.layer3.27.bn2.weight
 (256)" fillcolor=lightblue]
	1607406740304 -> 1606886546240
	1606886546240 [label=AccumulateGrad]
	1606886549408 -> 1606886548112
	1607406740464 [label="encoder.layer3.27.bn2.bias
 (256)" fillcolor=lightblue]
	1607406740464 -> 1606886549408
	1606886549408 [label=AccumulateGrad]
	1606886543120 -> 1606886553488
	1607406740944 [label="encoder.layer3.27.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1607406740944 -> 1606886543120
	1606886543120 [label=AccumulateGrad]
	1606886552480 -> 1606886541632
	1607406741024 [label="encoder.layer3.27.bn3.weight
 (1024)" fillcolor=lightblue]
	1607406741024 -> 1606886552480
	1606886552480 [label=AccumulateGrad]
	1606886547440 -> 1606886541632
	1607406741104 [label="encoder.layer3.27.bn3.bias
 (1024)" fillcolor=lightblue]
	1607406741104 -> 1606886547440
	1606886547440 [label=AccumulateGrad]
	1606886551280 -> 1606886553920
	1606886542544 -> 1606886541200
	1607406741504 [label="encoder.layer3.28.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1607406741504 -> 1606886542544
	1606886542544 [label=AccumulateGrad]
	1606886548544 -> 1606886552336
	1607406741584 [label="encoder.layer3.28.bn1.weight
 (256)" fillcolor=lightblue]
	1607406741584 -> 1606886548544
	1606886548544 [label=AccumulateGrad]
	1606886550368 -> 1606886552336
	1607406741664 [label="encoder.layer3.28.bn1.bias
 (256)" fillcolor=lightblue]
	1607406741664 -> 1606886550368
	1606886550368 [label=AccumulateGrad]
	1606886540960 -> 1606886553344
	1607406742144 [label="encoder.layer3.28.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1607406742144 -> 1606886540960
	1606886540960 [label=AccumulateGrad]
	1606886540144 -> 1606886541680
	1607406742064 [label="encoder.layer3.28.bn2.weight
 (256)" fillcolor=lightblue]
	1607406742064 -> 1606886540144
	1606886540144 [label=AccumulateGrad]
	1606886552720 -> 1606886541680
	1607406742224 [label="encoder.layer3.28.bn2.bias
 (256)" fillcolor=lightblue]
	1607406742224 -> 1606886552720
	1606886552720 [label=AccumulateGrad]
	1606886546096 -> 1606886551232
	1607406742704 [label="encoder.layer3.28.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1607406742704 -> 1606886546096
	1606886546096 [label=AccumulateGrad]
	1606886541824 -> 1606886546528
	1607406742784 [label="encoder.layer3.28.bn3.weight
 (1024)" fillcolor=lightblue]
	1607406742784 -> 1606886541824
	1606886541824 [label=AccumulateGrad]
	1606886550608 -> 1606886546528
	1607406742864 [label="encoder.layer3.28.bn3.bias
 (1024)" fillcolor=lightblue]
	1607406742864 -> 1606886550608
	1606886550608 [label=AccumulateGrad]
	1606886542256 -> 1606886549600
	1606886546288 -> 1606886549696
	1607406743344 [label="encoder.layer3.29.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1607406743344 -> 1606886546288
	1606886546288 [label=AccumulateGrad]
	1606886542016 -> 1606886550992
	1607406743424 [label="encoder.layer3.29.bn1.weight
 (256)" fillcolor=lightblue]
	1607406743424 -> 1606886542016
	1606886542016 [label=AccumulateGrad]
	1606886551808 -> 1606886550992
	1607406743504 [label="encoder.layer3.29.bn1.bias
 (256)" fillcolor=lightblue]
	1607406743504 -> 1606886551808
	1606886551808 [label=AccumulateGrad]
	1606886540720 -> 1606886544272
	1607406744064 [label="encoder.layer3.29.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1607406744064 -> 1606886540720
	1606886540720 [label=AccumulateGrad]
	1606886553008 -> 1606886546000
	1607406743984 [label="encoder.layer3.29.bn2.weight
 (256)" fillcolor=lightblue]
	1607406743984 -> 1606886553008
	1606886553008 [label=AccumulateGrad]
	1606886546720 -> 1606886546000
	1607406744144 [label="encoder.layer3.29.bn2.bias
 (256)" fillcolor=lightblue]
	1607406744144 -> 1606886546720
	1606886546720 [label=AccumulateGrad]
	1606886544896 -> 1606886543936
	1607406744624 [label="encoder.layer3.29.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1607406744624 -> 1606886544896
	1606886544896 [label=AccumulateGrad]
	1606886546576 -> 1606886541776
	1607406744704 [label="encoder.layer3.29.bn3.weight
 (1024)" fillcolor=lightblue]
	1607406744704 -> 1606886546576
	1606886546576 [label=AccumulateGrad]
	1606886552000 -> 1606886541776
	1607406744784 [label="encoder.layer3.29.bn3.bias
 (1024)" fillcolor=lightblue]
	1607406744784 -> 1606886552000
	1606886552000 [label=AccumulateGrad]
	1606886539904 -> 1606886550704
	1606886546144 -> 1606886540000
	1607406745184 [label="encoder.layer3.30.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1607406745184 -> 1606886546144
	1606886546144 [label=AccumulateGrad]
	1606886544560 -> 1606886550512
	1607406745264 [label="encoder.layer3.30.bn1.weight
 (256)" fillcolor=lightblue]
	1607406745264 -> 1606886544560
	1606886544560 [label=AccumulateGrad]
	1606886540432 -> 1606886550512
	1607406745344 [label="encoder.layer3.30.bn1.bias
 (256)" fillcolor=lightblue]
	1607406745344 -> 1606886540432
	1606886540432 [label=AccumulateGrad]
	1606886544992 -> 1606886553824
	1607406745824 [label="encoder.layer3.30.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1607406745824 -> 1606886544992
	1606886544992 [label=AccumulateGrad]
	1606886550032 -> 1606886548880
	1607406745744 [label="encoder.layer3.30.bn2.weight
 (256)" fillcolor=lightblue]
	1607406745744 -> 1606886550032
	1606886550032 [label=AccumulateGrad]
	1606886554832 -> 1606886548880
	1607406745904 [label="encoder.layer3.30.bn2.bias
 (256)" fillcolor=lightblue]
	1607406745904 -> 1606886554832
	1606886554832 [label=AccumulateGrad]
	1606886545712 -> 1606886552528
	1607406746384 [label="encoder.layer3.30.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1607406746384 -> 1606886545712
	1606886545712 [label=AccumulateGrad]
	1606886549264 -> 1606886549024
	1607406746464 [label="encoder.layer3.30.bn3.weight
 (1024)" fillcolor=lightblue]
	1607406746464 -> 1606886549264
	1606886549264 [label=AccumulateGrad]
	1606886543696 -> 1606886549024
	1607406746544 [label="encoder.layer3.30.bn3.bias
 (1024)" fillcolor=lightblue]
	1607406746544 -> 1606886543696
	1606886543696 [label=AccumulateGrad]
	1606886550896 -> 1606886545328
	1606886552624 -> 1606886547776
	1607406747024 [label="encoder.layer3.31.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1607406747024 -> 1606886552624
	1606886552624 [label=AccumulateGrad]
	1606886554784 -> 1606886547632
	1607406747104 [label="encoder.layer3.31.bn1.weight
 (256)" fillcolor=lightblue]
	1607406747104 -> 1606886554784
	1606886554784 [label=AccumulateGrad]
	1606886543984 -> 1606886547632
	1607406747184 [label="encoder.layer3.31.bn1.bias
 (256)" fillcolor=lightblue]
	1607406747184 -> 1606886543984
	1606886543984 [label=AccumulateGrad]
	1606886542496 -> 1606886549072
	1607457046608 [label="encoder.layer3.31.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1607457046608 -> 1606886542496
	1606886542496 [label=AccumulateGrad]
	1606886545616 -> 1606886545952
	1607406747584 [label="encoder.layer3.31.bn2.weight
 (256)" fillcolor=lightblue]
	1607406747584 -> 1606886545616
	1606886545616 [label=AccumulateGrad]
	1606886546864 -> 1606886545952
	1607457046688 [label="encoder.layer3.31.bn2.bias
 (256)" fillcolor=lightblue]
	1607457046688 -> 1606886546864
	1606886546864 [label=AccumulateGrad]
	1606886547488 -> 1606886545664
	1607457047088 [label="encoder.layer3.31.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1607457047088 -> 1606886547488
	1606886547488 [label=AccumulateGrad]
	1606886544032 -> 1606886548064
	1607457047168 [label="encoder.layer3.31.bn3.weight
 (1024)" fillcolor=lightblue]
	1607457047168 -> 1606886544032
	1606886544032 [label=AccumulateGrad]
	1606886541056 -> 1606886548064
	1607457047248 [label="encoder.layer3.31.bn3.bias
 (1024)" fillcolor=lightblue]
	1607457047248 -> 1606886541056
	1606886541056 [label=AccumulateGrad]
	1606886550656 -> 1606886542880
	1606886553968 -> 1606886551328
	1607457047728 [label="encoder.layer3.32.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1607457047728 -> 1606886553968
	1606886553968 [label=AccumulateGrad]
	1606886544512 -> 1606886550128
	1607457047808 [label="encoder.layer3.32.bn1.weight
 (256)" fillcolor=lightblue]
	1607457047808 -> 1606886544512
	1606886544512 [label=AccumulateGrad]
	1606886544416 -> 1606886550128
	1607457047888 [label="encoder.layer3.32.bn1.bias
 (256)" fillcolor=lightblue]
	1607457047888 -> 1606886544416
	1606886544416 [label=AccumulateGrad]
	1606886548352 -> 1606886543312
	1607457048448 [label="encoder.layer3.32.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1607457048448 -> 1606886548352
	1606886548352 [label=AccumulateGrad]
	1606886548256 -> 1606886541344
	1607457048368 [label="encoder.layer3.32.bn2.weight
 (256)" fillcolor=lightblue]
	1607457048368 -> 1606886548256
	1606886548256 [label=AccumulateGrad]
	1606886540672 -> 1606886541344
	1607457048528 [label="encoder.layer3.32.bn2.bias
 (256)" fillcolor=lightblue]
	1607457048528 -> 1606886540672
	1606886540672 [label=AccumulateGrad]
	1606886543360 -> 1606886550320
	1607457049008 [label="encoder.layer3.32.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1607457049008 -> 1606886543360
	1606886543360 [label=AccumulateGrad]
	1606886540768 -> 1606886547248
	1607457049088 [label="encoder.layer3.32.bn3.weight
 (1024)" fillcolor=lightblue]
	1607457049088 -> 1606886540768
	1606886540768 [label=AccumulateGrad]
	1606886551184 -> 1606886547248
	1607457049168 [label="encoder.layer3.32.bn3.bias
 (1024)" fillcolor=lightblue]
	1607457049168 -> 1606886551184
	1606886551184 [label=AccumulateGrad]
	1606886555072 -> 1606886554976
	1606886540864 -> 1606886542832
	1607457049648 [label="encoder.layer3.33.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1607457049648 -> 1606886540864
	1606886540864 [label=AccumulateGrad]
	1606886554208 -> 1606886542640
	1607457049728 [label="encoder.layer3.33.bn1.weight
 (256)" fillcolor=lightblue]
	1607457049728 -> 1606886554208
	1606886554208 [label=AccumulateGrad]
	1606886544752 -> 1606886542640
	1607457049808 [label="encoder.layer3.33.bn1.bias
 (256)" fillcolor=lightblue]
	1607457049808 -> 1606886544752
	1606886544752 [label=AccumulateGrad]
	1606886541968 -> 1606886545472
	1607457050368 [label="encoder.layer3.33.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1607457050368 -> 1606886541968
	1606886541968 [label=AccumulateGrad]
	1606886543264 -> 1606886551856
	1607457050288 [label="encoder.layer3.33.bn2.weight
 (256)" fillcolor=lightblue]
	1607457050288 -> 1606886543264
	1606886543264 [label=AccumulateGrad]
	1606886552192 -> 1606886551856
	1607457050448 [label="encoder.layer3.33.bn2.bias
 (256)" fillcolor=lightblue]
	1607457050448 -> 1606886552192
	1606886552192 [label=AccumulateGrad]
	1606886548496 -> 1606886543552
	1607457050848 [label="encoder.layer3.33.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1607457050848 -> 1606886548496
	1606886548496 [label=AccumulateGrad]
	1606886540624 -> 1606886543504
	1607457050928 [label="encoder.layer3.33.bn3.weight
 (1024)" fillcolor=lightblue]
	1607457050928 -> 1606886540624
	1606886540624 [label=AccumulateGrad]
	1606886547680 -> 1606886543504
	1607457051008 [label="encoder.layer3.33.bn3.bias
 (1024)" fillcolor=lightblue]
	1607457051008 -> 1606886547680
	1606886547680 [label=AccumulateGrad]
	1606886553104 -> 1606886554880
	1606886553584 -> 1606886327344
	1607457051488 [label="encoder.layer3.34.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1607457051488 -> 1606886553584
	1606886553584 [label=AccumulateGrad]
	1606886335312 -> 1606886337328
	1607457051568 [label="encoder.layer3.34.bn1.weight
 (256)" fillcolor=lightblue]
	1607457051568 -> 1606886335312
	1606886335312 [label=AccumulateGrad]
	1606886550224 -> 1606886337328
	1607457051648 [label="encoder.layer3.34.bn1.bias
 (256)" fillcolor=lightblue]
	1607457051648 -> 1606886550224
	1606886550224 [label=AccumulateGrad]
	1606886329504 -> 1606886337040
	1607457052208 [label="encoder.layer3.34.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1607457052208 -> 1606886329504
	1606886329504 [label=AccumulateGrad]
	1606886326336 -> 1606886330656
	1607457052128 [label="encoder.layer3.34.bn2.weight
 (256)" fillcolor=lightblue]
	1607457052128 -> 1606886326336
	1606886326336 [label=AccumulateGrad]
	1606886338480 -> 1606886330656
	1607457052288 [label="encoder.layer3.34.bn2.bias
 (256)" fillcolor=lightblue]
	1607457052288 -> 1606886338480
	1606886338480 [label=AccumulateGrad]
	1606886337664 -> 1606886339008
	1607457052768 [label="encoder.layer3.34.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1607457052768 -> 1606886337664
	1606886337664 [label=AccumulateGrad]
	1606886337712 -> 1606886326480
	1607457052848 [label="encoder.layer3.34.bn3.weight
 (1024)" fillcolor=lightblue]
	1607457052848 -> 1606886337712
	1606886337712 [label=AccumulateGrad]
	1606886331088 -> 1606886326480
	1607457052928 [label="encoder.layer3.34.bn3.bias
 (1024)" fillcolor=lightblue]
	1607457052928 -> 1606886331088
	1606886331088 [label=AccumulateGrad]
	1606886330800 -> 1606886332864
	1606886326864 -> 1606886337856
	1607457053408 [label="encoder.layer3.35.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1607457053408 -> 1606886326864
	1606886326864 [label=AccumulateGrad]
	1606886330128 -> 1606886336848
	1607457053488 [label="encoder.layer3.35.bn1.weight
 (256)" fillcolor=lightblue]
	1607457053488 -> 1606886330128
	1606886330128 [label=AccumulateGrad]
	1606886330752 -> 1606886336848
	1607457053568 [label="encoder.layer3.35.bn1.bias
 (256)" fillcolor=lightblue]
	1607457053568 -> 1606886330752
	1606886330752 [label=AccumulateGrad]
	1606886327968 -> 1606886340544
	1607457054128 [label="encoder.layer3.35.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1607457054128 -> 1606886327968
	1606886327968 [label=AccumulateGrad]
	1606886338432 -> 1606886328400
	1607457054048 [label="encoder.layer3.35.bn2.weight
 (256)" fillcolor=lightblue]
	1607457054048 -> 1606886338432
	1606886338432 [label=AccumulateGrad]
	1606886333584 -> 1606886328400
	1607457054208 [label="encoder.layer3.35.bn2.bias
 (256)" fillcolor=lightblue]
	1607457054208 -> 1606886333584
	1606886333584 [label=AccumulateGrad]
	1606886327872 -> 1606886335072
	1607457054688 [label="encoder.layer3.35.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1607457054688 -> 1606886327872
	1606886327872 [label=AccumulateGrad]
	1606886341792 -> 1606886337520
	1607457054768 [label="encoder.layer3.35.bn3.weight
 (1024)" fillcolor=lightblue]
	1607457054768 -> 1606886341792
	1606886341792 [label=AccumulateGrad]
	1606886340208 -> 1606886337520
	1607457054608 [label="encoder.layer3.35.bn3.bias
 (1024)" fillcolor=lightblue]
	1607457054608 -> 1606886340208
	1606886340208 [label=AccumulateGrad]
	1606886334208 -> 1606886333296
	1606886329552 -> 1606886338912
	1607457055888 [label="encoder.layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	1607457055888 -> 1606886329552
	1606886329552 [label=AccumulateGrad]
	1606886336560 -> 1606886338144
	1607457055968 [label="encoder.layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1607457055968 -> 1606886336560
	1606886336560 [label=AccumulateGrad]
	1606886326672 -> 1606886338144
	1607457056048 [label="encoder.layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1607457056048 -> 1606886326672
	1606886326672 [label=AccumulateGrad]
	1606886339248 -> 1606886334688
	1607457056528 [label="encoder.layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1607457056528 -> 1606886339248
	1606886339248 [label=AccumulateGrad]
	1606886331424 -> 1606886329936
	1607457056448 [label="encoder.layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1607457056448 -> 1606886331424
	1606886331424 [label=AccumulateGrad]
	1606886328928 -> 1606886329936
	1607457056608 [label="encoder.layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1607457056608 -> 1606886328928
	1606886328928 [label=AccumulateGrad]
	1606886334976 -> 1606886332576
	1607457057008 [label="encoder.layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	1607457057008 -> 1606886334976
	1606886334976 [label=AccumulateGrad]
	1606886341984 -> 1606886335648
	1607457057088 [label="encoder.layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	1607457057088 -> 1606886341984
	1606886341984 [label=AccumulateGrad]
	1606886338240 -> 1606886335648
	1607457057168 [label="encoder.layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	1607457057168 -> 1606886338240
	1606886338240 [label=AccumulateGrad]
	1606886335360 -> 1606886333104
	1606886335360 [label=CudnnBatchNormBackward0]
	1606886333056 -> 1606886335360
	1606886333056 [label=ConvolutionBackward0]
	1606886334304 -> 1606886333056
	1606886329744 -> 1606886333056
	1607457055168 [label="encoder.layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	1607457055168 -> 1606886329744
	1606886329744 [label=AccumulateGrad]
	1606886330512 -> 1606886335360
	1607457055248 [label="encoder.layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	1607457055248 -> 1606886330512
	1606886330512 [label=AccumulateGrad]
	1606886341360 -> 1606886335360
	1607457055328 [label="encoder.layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	1607457055328 -> 1606886341360
	1606886341360 [label=AccumulateGrad]
	1606886338096 -> 1606886338288
	1607457057568 [label="encoder.layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	1607457057568 -> 1606886338096
	1606886338096 [label=AccumulateGrad]
	1606886336128 -> 1606886327632
	1607457057648 [label="encoder.layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1607457057648 -> 1606886336128
	1606886336128 [label=AccumulateGrad]
	1606886332048 -> 1606886327632
	1607457057728 [label="encoder.layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1607457057728 -> 1606886332048
	1606886332048 [label=AccumulateGrad]
	1606886337952 -> 1606886340592
	1607457058288 [label="encoder.layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1607457058288 -> 1606886337952
	1606886337952 [label=AccumulateGrad]
	1606886329216 -> 1606886333440
	1607457058208 [label="encoder.layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1607457058208 -> 1606886329216
	1606886329216 [label=AccumulateGrad]
	1606886327104 -> 1606886333440
	1607457058368 [label="encoder.layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1607457058368 -> 1606886327104
	1606886327104 [label=AccumulateGrad]
	1606886331328 -> 1606886334592
	1607457058848 [label="encoder.layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	1607457058848 -> 1606886331328
	1606886331328 [label=AccumulateGrad]
	1606886340400 -> 1606886333872
	1607457058928 [label="encoder.layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	1607457058928 -> 1606886340400
	1606886340400 [label=AccumulateGrad]
	1606886336656 -> 1606886333872
	1607457059008 [label="encoder.layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	1607457059008 -> 1606886336656
	1606886336656 [label=AccumulateGrad]
	1606886337904 -> 1606886326384
	1606886331808 -> 1606886341408
	1607457059488 [label="encoder.layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	1607457059488 -> 1606886331808
	1606886331808 [label=AccumulateGrad]
	1606886339296 -> 1606886328688
	1607457059568 [label="encoder.layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	1607457059568 -> 1606886339296
	1606886339296 [label=AccumulateGrad]
	1606886340064 -> 1606886328688
	1607457059648 [label="encoder.layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	1607457059648 -> 1606886340064
	1606886340064 [label=AccumulateGrad]
	1606886340112 -> 1606886330944
	1607457060208 [label="encoder.layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1607457060208 -> 1606886340112
	1606886340112 [label=AccumulateGrad]
	1606886339488 -> 1606886327056
	1607457060128 [label="encoder.layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	1607457060128 -> 1606886339488
	1606886339488 [label=AccumulateGrad]
	1606886329312 -> 1606886327056
	1607457060288 [label="encoder.layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	1607457060288 -> 1606886329312
	1606886329312 [label=AccumulateGrad]
	1606886327920 -> 1606886332672
	1607457060688 [label="encoder.layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	1607457060688 -> 1606886327920
	1606886327920 [label=AccumulateGrad]
	1606886339680 -> 1606886326720
	1607457060768 [label="encoder.layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	1607457060768 -> 1606886339680
	1606886339680 [label=AccumulateGrad]
	1606886330368 -> 1606886326720
	1607457060848 [label="encoder.layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	1607457060848 -> 1606886330368
	1606886330368 [label=AccumulateGrad]
	1606886328256 -> 1606886335216
	1606886334304 -> 1606886329984
	1606886334112 -> 1606886335600
	1606886159008 [label="decoder.blocks.0.conv1.0.weight
 (256, 3072, 3, 3)" fillcolor=lightblue]
	1606886159008 -> 1606886334112
	1606886334112 [label=AccumulateGrad]
	1606886334448 -> 1606886332336
	1606886146848 [label="decoder.blocks.0.conv1.1.weight
 (256)" fillcolor=lightblue]
	1606886146848 -> 1606886334448
	1606886334448 [label=AccumulateGrad]
	1606886331184 -> 1606886332336
	1606886159088 [label="decoder.blocks.0.conv1.1.bias
 (256)" fillcolor=lightblue]
	1606886159088 -> 1606886331184
	1606886331184 [label=AccumulateGrad]
	1606886331472 -> 1606886329408
	1607457062448 [label="decoder.blocks.0.conv2.0.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1607457062448 -> 1606886331472
	1606886331472 [label=AccumulateGrad]
	1606886339200 -> 1606886329696
	1607457047408 [label="decoder.blocks.0.conv2.1.weight
 (256)" fillcolor=lightblue]
	1607457047408 -> 1606886339200
	1606886339200 [label=AccumulateGrad]
	1606886339632 -> 1606886329696
	1607457062208 [label="decoder.blocks.0.conv2.1.bias
 (256)" fillcolor=lightblue]
	1607457062208 -> 1606886339632
	1606886339632 [label=AccumulateGrad]
	1606886329072 -> 1606886333728
	1606886333488 -> 1606886328832
	1607457062048 [label="decoder.blocks.1.conv1.0.weight
 (128, 768, 3, 3)" fillcolor=lightblue]
	1607457062048 -> 1606886333488
	1606886333488 [label=AccumulateGrad]
	1606886332432 -> 1606886339056
	1607457061888 [label="decoder.blocks.1.conv1.1.weight
 (128)" fillcolor=lightblue]
	1607457061888 -> 1606886332432
	1606886332432 [label=AccumulateGrad]
	1606886333536 -> 1606886339056
	1607457062288 [label="decoder.blocks.1.conv1.1.bias
 (128)" fillcolor=lightblue]
	1607457062288 -> 1606886333536
	1606886333536 [label=AccumulateGrad]
	1606886341216 -> 1606886329600
	1607457062848 [label="decoder.blocks.1.conv2.0.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1607457062848 -> 1606886341216
	1606886341216 [label=AccumulateGrad]
	1606886331760 -> 1606886340976
	1607457061168 [label="decoder.blocks.1.conv2.1.weight
 (128)" fillcolor=lightblue]
	1607457061168 -> 1606886331760
	1606886331760 [label=AccumulateGrad]
	1606886330704 -> 1606886340976
	1607457061248 [label="decoder.blocks.1.conv2.1.bias
 (128)" fillcolor=lightblue]
	1607457061248 -> 1606886330704
	1606886330704 [label=AccumulateGrad]
	1606886329456 -> 1606886332384
	1606886335840 -> 1606886329264
	1607406737024 [label="decoder.blocks.2.conv1.0.weight
 (64, 384, 3, 3)" fillcolor=lightblue]
	1607406737024 -> 1606886335840
	1606886335840 [label=AccumulateGrad]
	1606886332768 -> 1606886332816
	1606885776816 [label="decoder.blocks.2.conv1.1.weight
 (64)" fillcolor=lightblue]
	1606885776816 -> 1606886332768
	1606886332768 [label=AccumulateGrad]
	1606886330080 -> 1606886332816
	1606885772256 [label="decoder.blocks.2.conv1.1.bias
 (64)" fillcolor=lightblue]
	1606885772256 -> 1606886330080
	1606886330080 [label=AccumulateGrad]
	1606886339872 -> 1606886326576
	1607457303808 [label="decoder.blocks.2.conv2.0.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1607457303808 -> 1606886339872
	1606886339872 [label=AccumulateGrad]
	1606886339824 -> 1606886340736
	1607457296368 [label="decoder.blocks.2.conv2.1.weight
 (64)" fillcolor=lightblue]
	1607457296368 -> 1606886339824
	1606886339824 [label=AccumulateGrad]
	1606886330032 -> 1606886340736
	1607457297808 [label="decoder.blocks.2.conv2.1.bias
 (64)" fillcolor=lightblue]
	1607457297808 -> 1606886330032
	1606886330032 [label=AccumulateGrad]
	1606886329360 -> 1606886332240
	1606886334928 -> 1606886342368
	1607457303968 [label="decoder.blocks.3.conv1.0.weight
 (32, 128, 3, 3)" fillcolor=lightblue]
	1607457303968 -> 1606886334928
	1606886334928 [label=AccumulateGrad]
	1606886331712 -> 1606886336272
	1607457304448 [label="decoder.blocks.3.conv1.1.weight
 (32)" fillcolor=lightblue]
	1607457304448 -> 1606886331712
	1606886331712 [label=AccumulateGrad]
	1606886332480 -> 1606886336272
	1607457293088 [label="decoder.blocks.3.conv1.1.bias
 (32)" fillcolor=lightblue]
	1607457293088 -> 1606886332480
	1606886332480 [label=AccumulateGrad]
	1606886328064 -> 1606886328880
	1607457302848 [label="decoder.blocks.3.conv2.0.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	1607457302848 -> 1606886328064
	1606886328064 [label=AccumulateGrad]
	1606886341936 -> 1606886334016
	1607457306768 [label="decoder.blocks.3.conv2.1.weight
 (32)" fillcolor=lightblue]
	1607457306768 -> 1606886341936
	1606886341936 [label=AccumulateGrad]
	1606886331904 -> 1606886334016
	1607457292768 [label="decoder.blocks.3.conv2.1.bias
 (32)" fillcolor=lightblue]
	1607457292768 -> 1606886331904
	1606886331904 [label=AccumulateGrad]
	1606597298352 -> 1606597306512
	1607457293408 [label="decoder.blocks.4.conv1.0.weight
 (16, 32, 3, 3)" fillcolor=lightblue]
	1607457293408 -> 1606597298352
	1606597298352 [label=AccumulateGrad]
	1606597311456 -> 1606597309344
	1607457304368 [label="decoder.blocks.4.conv1.1.weight
 (16)" fillcolor=lightblue]
	1607457304368 -> 1606597311456
	1606597311456 [label=AccumulateGrad]
	1606597298976 -> 1606597309344
	1607457296848 [label="decoder.blocks.4.conv1.1.bias
 (16)" fillcolor=lightblue]
	1607457296848 -> 1606597298976
	1606597298976 [label=AccumulateGrad]
	1606597300800 -> 1606552266704
	1607457303888 [label="decoder.blocks.4.conv2.0.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
	1607457303888 -> 1606597300800
	1606597300800 [label=AccumulateGrad]
	1606596983056 -> 1606704253680
	1607457295888 [label="decoder.blocks.4.conv2.1.weight
 (16)" fillcolor=lightblue]
	1607457295888 -> 1606596983056
	1606596983056 [label=AccumulateGrad]
	1606596980944 -> 1606704253680
	1607457303488 [label="decoder.blocks.4.conv2.1.bias
 (16)" fillcolor=lightblue]
	1607457303488 -> 1606596980944
	1606596980944 [label=AccumulateGrad]
	1606596755216 -> 1606591229152
	1607457296768 [label="segmentation_head.0.weight
 (5, 16, 3, 3)" fillcolor=lightblue]
	1607457296768 -> 1606596755216
	1606596755216 [label=AccumulateGrad]
	1606704251088 -> 1606591229152
	1607457307328 [label="segmentation_head.0.bias
 (5)" fillcolor=lightblue]
	1607457307328 -> 1606704251088
	1606704251088 [label=AccumulateGrad]
	1606591229152 -> 1606703954384
}
