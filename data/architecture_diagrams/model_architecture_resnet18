digraph {
	graph [size="88.95,88.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1886965466928 [label="
 (1, 5, 256, 256)" fillcolor=darkolivegreen1]
	1884006189664 [label=ConvolutionBackward0]
	1884006099088 -> 1884006189664
	1884006099088 [label=ReluBackward0]
	1884130352272 -> 1884006099088
	1884130352272 [label=CudnnBatchNormBackward0]
	1884170800528 -> 1884130352272
	1884170800528 [label=ConvolutionBackward0]
	1884169054880 -> 1884170800528
	1884169054880 [label=ReluBackward0]
	1884006333840 -> 1884169054880
	1884006333840 [label=CudnnBatchNormBackward0]
	1886964805792 -> 1884006333840
	1886964805792 [label=ConvolutionBackward0]
	1886965449920 -> 1886964805792
	1886965449920 [label=UpsampleNearest2DBackward0]
	1886965450688 -> 1886965449920
	1886965450688 [label=ReluBackward0]
	1886965450784 -> 1886965450688
	1886965450784 [label=CudnnBatchNormBackward0]
	1886965450448 -> 1886965450784
	1886965450448 [label=ConvolutionBackward0]
	1886965450880 -> 1886965450448
	1886965450880 [label=ReluBackward0]
	1886965450640 -> 1886965450880
	1886965450640 [label=CudnnBatchNormBackward0]
	1886965451072 -> 1886965450640
	1886965451072 [label=ConvolutionBackward0]
	1886965451264 -> 1886965451072
	1886965451264 [label=CatBackward0]
	1886965451360 -> 1886965451264
	1886965451360 [label=UpsampleNearest2DBackward0]
	1886965451504 -> 1886965451360
	1886965451504 [label=ReluBackward0]
	1886965451600 -> 1886965451504
	1886965451600 [label=CudnnBatchNormBackward0]
	1886965451696 -> 1886965451600
	1886965451696 [label=ConvolutionBackward0]
	1886965451888 -> 1886965451696
	1886965451888 [label=ReluBackward0]
	1886965452032 -> 1886965451888
	1886965452032 [label=CudnnBatchNormBackward0]
	1886965452128 -> 1886965452032
	1886965452128 [label=ConvolutionBackward0]
	1886965452320 -> 1886965452128
	1886965452320 [label=CatBackward0]
	1886965452464 -> 1886965452320
	1886965452464 [label=UpsampleNearest2DBackward0]
	1886965452608 -> 1886965452464
	1886965452608 [label=ReluBackward0]
	1886965452704 -> 1886965452608
	1886965452704 [label=CudnnBatchNormBackward0]
	1886965452800 -> 1886965452704
	1886965452800 [label=ConvolutionBackward0]
	1886965452992 -> 1886965452800
	1886965452992 [label=ReluBackward0]
	1886965453136 -> 1886965452992
	1886965453136 [label=CudnnBatchNormBackward0]
	1886965453232 -> 1886965453136
	1886965453232 [label=ConvolutionBackward0]
	1886965453424 -> 1886965453232
	1886965453424 [label=CatBackward0]
	1886965453568 -> 1886965453424
	1886965453568 [label=UpsampleNearest2DBackward0]
	1886965453712 -> 1886965453568
	1886965453712 [label=ReluBackward0]
	1886965453808 -> 1886965453712
	1886965453808 [label=CudnnBatchNormBackward0]
	1886965453904 -> 1886965453808
	1886965453904 [label=ConvolutionBackward0]
	1886965454096 -> 1886965453904
	1886965454096 [label=ReluBackward0]
	1886965454240 -> 1886965454096
	1886965454240 [label=CudnnBatchNormBackward0]
	1886965454336 -> 1886965454240
	1886965454336 [label=ConvolutionBackward0]
	1886965454528 -> 1886965454336
	1886965454528 [label=CatBackward0]
	1886965454672 -> 1886965454528
	1886965454672 [label=UpsampleNearest2DBackward0]
	1886965454816 -> 1886965454672
	1886965454816 [label=ReluBackward0]
	1886965454912 -> 1886965454816
	1886965454912 [label=AddBackward0]
	1886965455008 -> 1886965454912
	1886965455008 [label=CudnnBatchNormBackward0]
	1886965455152 -> 1886965455008
	1886965455152 [label=ConvolutionBackward0]
	1886965455344 -> 1886965455152
	1886965455344 [label=ReluBackward0]
	1886965455488 -> 1886965455344
	1886965455488 [label=CudnnBatchNormBackward0]
	1886965455584 -> 1886965455488
	1886965455584 [label=ConvolutionBackward0]
	1886965455056 -> 1886965455584
	1886965455056 [label=ReluBackward0]
	1886965455872 -> 1886965455056
	1886965455872 [label=AddBackward0]
	1886965455968 -> 1886965455872
	1886965455968 [label=CudnnBatchNormBackward0]
	1886965456112 -> 1886965455968
	1886965456112 [label=ConvolutionBackward0]
	1886965456304 -> 1886965456112
	1886965456304 [label=ReluBackward0]
	1886965456448 -> 1886965456304
	1886965456448 [label=CudnnBatchNormBackward0]
	1886965456544 -> 1886965456448
	1886965456544 [label=ConvolutionBackward0]
	1886965454720 -> 1886965456544
	1886965454720 [label=ReluBackward0]
	1886965456832 -> 1886965454720
	1886965456832 [label=AddBackward0]
	1886965456928 -> 1886965456832
	1886965456928 [label=CudnnBatchNormBackward0]
	1886965457072 -> 1886965456928
	1886965457072 [label=ConvolutionBackward0]
	1886965457264 -> 1886965457072
	1886965457264 [label=ReluBackward0]
	1886965457408 -> 1886965457264
	1886965457408 [label=CudnnBatchNormBackward0]
	1886965457504 -> 1886965457408
	1886965457504 [label=ConvolutionBackward0]
	1886965456976 -> 1886965457504
	1886965456976 [label=ReluBackward0]
	1886965457792 -> 1886965456976
	1886965457792 [label=AddBackward0]
	1886965457888 -> 1886965457792
	1886965457888 [label=CudnnBatchNormBackward0]
	1886965458032 -> 1886965457888
	1886965458032 [label=ConvolutionBackward0]
	1886965458224 -> 1886965458032
	1886965458224 [label=ReluBackward0]
	1886965458368 -> 1886965458224
	1886965458368 [label=CudnnBatchNormBackward0]
	1886965458464 -> 1886965458368
	1886965458464 [label=ConvolutionBackward0]
	1886965453616 -> 1886965458464
	1886965453616 [label=ReluBackward0]
	1886965458752 -> 1886965453616
	1886965458752 [label=AddBackward0]
	1886965458896 -> 1886965458752
	1886965458896 [label=CudnnBatchNormBackward0]
	1886965459040 -> 1886965458896
	1886965459040 [label=ConvolutionBackward0]
	1886965459232 -> 1886965459040
	1886965459232 [label=ReluBackward0]
	1886965459376 -> 1886965459232
	1886965459376 [label=CudnnBatchNormBackward0]
	1886965459520 -> 1886965459376
	1886965459520 [label=ConvolutionBackward0]
	1886965458608 -> 1886965459520
	1886965458608 [label=ReluBackward0]
	1886965459808 -> 1886965458608
	1886965459808 [label=AddBackward0]
	1886965459952 -> 1886965459808
	1886965459952 [label=CudnnBatchNormBackward0]
	1886965460096 -> 1886965459952
	1886965460096 [label=ConvolutionBackward0]
	1886965460288 -> 1886965460096
	1886965460288 [label=ReluBackward0]
	1886965460432 -> 1886965460288
	1886965460432 [label=CudnnBatchNormBackward0]
	1886965460576 -> 1886965460432
	1886965460576 [label=ConvolutionBackward0]
	1886965452512 -> 1886965460576
	1886965452512 [label=ReluBackward0]
	1886965460864 -> 1886965452512
	1886965460864 [label=AddBackward0]
	1886965461008 -> 1886965460864
	1886965461008 [label=CudnnBatchNormBackward0]
	1886965461152 -> 1886965461008
	1886965461152 [label=ConvolutionBackward0]
	1886965461344 -> 1886965461152
	1886965461344 [label=ReluBackward0]
	1886965461488 -> 1886965461344
	1886965461488 [label=CudnnBatchNormBackward0]
	1886965461632 -> 1886965461488
	1886965461632 [label=ConvolutionBackward0]
	1886965460720 -> 1886965461632
	1886965460720 [label=ReluBackward0]
	1886965461920 -> 1886965460720
	1886965461920 [label=AddBackward0]
	1886965462064 -> 1886965461920
	1886965462064 [label=CudnnBatchNormBackward0]
	1886965462208 -> 1886965462064
	1886965462208 [label=ConvolutionBackward0]
	1886965462400 -> 1886965462208
	1886965462400 [label=ReluBackward0]
	1886965462544 -> 1886965462400
	1886965462544 [label=CudnnBatchNormBackward0]
	1886965462688 -> 1886965462544
	1886965462688 [label=ConvolutionBackward0]
	1886965461776 -> 1886965462688
	1886965461776 [label=MaxPool2DWithIndicesBackward0]
	1886965451408 -> 1886965461776
	1886965451408 [label=ReluBackward0]
	1886965463072 -> 1886965451408
	1886965463072 [label=CudnnBatchNormBackward0]
	1886965463216 -> 1886965463072
	1886965463216 [label=ConvolutionBackward0]
	1886965463408 -> 1886965463216
	1884006158880 [label="encoder.conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	1884006158880 -> 1886965463408
	1886965463408 [label=AccumulateGrad]
	1886965463120 -> 1886965463072
	1884006160000 [label="encoder.bn1.weight
 (64)" fillcolor=lightblue]
	1884006160000 -> 1886965463120
	1886965463120 [label=AccumulateGrad]
	1886965463264 -> 1886965463072
	1884006161040 [label="encoder.bn1.bias
 (64)" fillcolor=lightblue]
	1884006161040 -> 1886965463264
	1886965463264 [label=AccumulateGrad]
	1886965462880 -> 1886965462688
	1884006155840 [label="encoder.layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1884006155840 -> 1886965462880
	1886965462880 [label=AccumulateGrad]
	1886965462496 -> 1886965462544
	1884006156400 [label="encoder.layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1884006156400 -> 1886965462496
	1886965462496 [label=AccumulateGrad]
	1886965462736 -> 1886965462544
	1884006155920 [label="encoder.layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1884006155920 -> 1886965462736
	1886965462736 [label=AccumulateGrad]
	1886965462448 -> 1886965462208
	1884006155760 [label="encoder.layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1884006155760 -> 1886965462448
	1886965462448 [label=AccumulateGrad]
	1886965462256 -> 1886965462064
	1884006155680 [label="encoder.layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1884006155680 -> 1886965462256
	1886965462256 [label=AccumulateGrad]
	1886965462160 -> 1886965462064
	1884006155600 [label="encoder.layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1884006155600 -> 1886965462160
	1886965462160 [label=AccumulateGrad]
	1886965461776 -> 1886965461920
	1886965461824 -> 1886965461632
	1884006154640 [label="encoder.layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1884006154640 -> 1886965461824
	1886965461824 [label=AccumulateGrad]
	1886965461440 -> 1886965461488
	1884006156320 [label="encoder.layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1884006156320 -> 1886965461440
	1886965461440 [label=AccumulateGrad]
	1886965461680 -> 1886965461488
	1884006154720 [label="encoder.layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1884006154720 -> 1886965461680
	1886965461680 [label=AccumulateGrad]
	1886965461392 -> 1886965461152
	1885069394960 [label="encoder.layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1885069394960 -> 1886965461392
	1886965461392 [label=AccumulateGrad]
	1886965461200 -> 1886965461008
	1884006154560 [label="encoder.layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1884006154560 -> 1886965461200
	1886965461200 [label=AccumulateGrad]
	1886965461104 -> 1886965461008
	1885069404160 [label="encoder.layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1885069404160 -> 1886965461104
	1886965461104 [label=AccumulateGrad]
	1886965460720 -> 1886965460864
	1886965460768 -> 1886965460576
	1885069402320 [label="encoder.layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	1885069402320 -> 1886965460768
	1886965460768 [label=AccumulateGrad]
	1886965460384 -> 1886965460432
	1885069396400 [label="encoder.layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1885069396400 -> 1886965460384
	1886965460384 [label=AccumulateGrad]
	1886965460624 -> 1886965460432
	1885069404960 [label="encoder.layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1885069404960 -> 1886965460624
	1886965460624 [label=AccumulateGrad]
	1886965460336 -> 1886965460096
	1885069404640 [label="encoder.layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1885069404640 -> 1886965460336
	1886965460336 [label=AccumulateGrad]
	1886965460144 -> 1886965459952
	1885069407760 [label="encoder.layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1885069407760 -> 1886965460144
	1886965460144 [label=AccumulateGrad]
	1886965460048 -> 1886965459952
	1885069399680 [label="encoder.layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1885069399680 -> 1886965460048
	1886965460048 [label=AccumulateGrad]
	1886965459664 -> 1886965459808
	1886965459664 [label=CudnnBatchNormBackward0]
	1884130150048 -> 1886965459664
	1884130150048 [label=ConvolutionBackward0]
	1886965452512 -> 1884130150048
	1886965460816 -> 1884130150048
	1885069405280 [label="encoder.layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1885069405280 -> 1886965460816
	1886965460816 [label=AccumulateGrad]
	1886965460240 -> 1886965459664
	1885069402720 [label="encoder.layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	1885069402720 -> 1886965460240
	1886965460240 [label=AccumulateGrad]
	1886965460192 -> 1886965459664
	1885069400560 [label="encoder.layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	1885069400560 -> 1886965460192
	1886965460192 [label=AccumulateGrad]
	1886965459712 -> 1886965459520
	1885069401040 [label="encoder.layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1885069401040 -> 1886965459712
	1886965459712 [label=AccumulateGrad]
	1886965459328 -> 1886965459376
	1885069409360 [label="encoder.layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1885069409360 -> 1886965459328
	1886965459328 [label=AccumulateGrad]
	1886965459568 -> 1886965459376
	1885069397840 [label="encoder.layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1885069397840 -> 1886965459568
	1886965459568 [label=AccumulateGrad]
	1886965459280 -> 1886965459040
	1885069408480 [label="encoder.layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1885069408480 -> 1886965459280
	1886965459280 [label=AccumulateGrad]
	1886965459088 -> 1886965458896
	1885069394560 [label="encoder.layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1885069394560 -> 1886965459088
	1886965459088 [label=AccumulateGrad]
	1886965458992 -> 1886965458896
	1885069409280 [label="encoder.layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1885069409280 -> 1886965458992
	1886965458992 [label=AccumulateGrad]
	1886965458608 -> 1886965458752
	1886965458656 -> 1886965458464
	1885069405120 [label="encoder.layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	1885069405120 -> 1886965458656
	1886965458656 [label=AccumulateGrad]
	1886965458512 -> 1886965458368
	1885069400160 [label="encoder.layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1885069400160 -> 1886965458512
	1886965458512 [label=AccumulateGrad]
	1886965458320 -> 1886965458368
	1885069401120 [label="encoder.layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1885069401120 -> 1886965458320
	1886965458320 [label=AccumulateGrad]
	1886965458272 -> 1886965458032
	1885069406240 [label="encoder.layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1885069406240 -> 1886965458272
	1886965458272 [label=AccumulateGrad]
	1886965458080 -> 1886965457888
	1885069408640 [label="encoder.layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1885069408640 -> 1886965458080
	1886965458080 [label=AccumulateGrad]
	1886965457984 -> 1886965457888
	1885069406320 [label="encoder.layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1885069406320 -> 1886965457984
	1886965457984 [label=AccumulateGrad]
	1886965457936 -> 1886965457792
	1886965457936 [label=CudnnBatchNormBackward0]
	1886965458416 -> 1886965457936
	1886965458416 [label=ConvolutionBackward0]
	1886965453616 -> 1886965458416
	1886965458704 -> 1886965458416
	1885069397120 [label="encoder.layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1885069397120 -> 1886965458704
	1886965458704 [label=AccumulateGrad]
	1886965458176 -> 1886965457936
	1885069407120 [label="encoder.layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1885069407120 -> 1886965458176
	1886965458176 [label=AccumulateGrad]
	1886965458128 -> 1886965457936
	1885069402400 [label="encoder.layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1885069402400 -> 1886965458128
	1886965458128 [label=AccumulateGrad]
	1886965457696 -> 1886965457504
	1885069402880 [label="encoder.layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1885069402880 -> 1886965457696
	1886965457696 [label=AccumulateGrad]
	1886965457552 -> 1886965457408
	1885069396160 [label="encoder.layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1885069396160 -> 1886965457552
	1886965457552 [label=AccumulateGrad]
	1886965457360 -> 1886965457408
	1885069397600 [label="encoder.layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1885069397600 -> 1886965457360
	1886965457360 [label=AccumulateGrad]
	1886965457312 -> 1886965457072
	1885069403760 [label="encoder.layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1885069403760 -> 1886965457312
	1886965457312 [label=AccumulateGrad]
	1886965457120 -> 1886965456928
	1885069401680 [label="encoder.layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1885069401680 -> 1886965457120
	1886965457120 [label=AccumulateGrad]
	1886965457024 -> 1886965456928
	1885069395920 [label="encoder.layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1885069395920 -> 1886965457024
	1886965457024 [label=AccumulateGrad]
	1886965456976 -> 1886965456832
	1886965456736 -> 1886965456544
	1885069394800 [label="encoder.layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	1885069394800 -> 1886965456736
	1886965456736 [label=AccumulateGrad]
	1886965456592 -> 1886965456448
	1885069402640 [label="encoder.layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1885069402640 -> 1886965456592
	1886965456592 [label=AccumulateGrad]
	1886965456400 -> 1886965456448
	1885069404000 [label="encoder.layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1885069404000 -> 1886965456400
	1886965456400 [label=AccumulateGrad]
	1886965456352 -> 1886965456112
	1885069399280 [label="encoder.layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1885069399280 -> 1886965456352
	1886965456352 [label=AccumulateGrad]
	1886965456160 -> 1886965455968
	1885069394880 [label="encoder.layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1885069394880 -> 1886965456160
	1886965456160 [label=AccumulateGrad]
	1886965456064 -> 1886965455968
	1885069403440 [label="encoder.layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1885069403440 -> 1886965456064
	1886965456064 [label=AccumulateGrad]
	1886965456016 -> 1886965455872
	1886965456016 [label=CudnnBatchNormBackward0]
	1886965456496 -> 1886965456016
	1886965456496 [label=ConvolutionBackward0]
	1886965454720 -> 1886965456496
	1886965456784 -> 1886965456496
	1885069404880 [label="encoder.layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1885069404880 -> 1886965456784
	1886965456784 [label=AccumulateGrad]
	1886965456256 -> 1886965456016
	1885069407360 [label="encoder.layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1885069407360 -> 1886965456256
	1886965456256 [label=AccumulateGrad]
	1886965456208 -> 1886965456016
	1885069407520 [label="encoder.layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1885069407520 -> 1886965456208
	1886965456208 [label=AccumulateGrad]
	1886965455776 -> 1886965455584
	1885069401200 [label="encoder.layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1885069401200 -> 1886965455776
	1886965455776 [label=AccumulateGrad]
	1886965455632 -> 1886965455488
	1885069396640 [label="encoder.layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1885069396640 -> 1886965455632
	1886965455632 [label=AccumulateGrad]
	1886965455440 -> 1886965455488
	1885069403920 [label="encoder.layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1885069403920 -> 1886965455440
	1886965455440 [label=AccumulateGrad]
	1886965455392 -> 1886965455152
	1885069395360 [label="encoder.layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1885069395360 -> 1886965455392
	1886965455392 [label=AccumulateGrad]
	1886965455200 -> 1886965455008
	1885069405040 [label="encoder.layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1885069405040 -> 1886965455200
	1886965455200 [label=AccumulateGrad]
	1886965455104 -> 1886965455008
	1885069398080 [label="encoder.layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1885069398080 -> 1886965455104
	1886965455104 [label=AccumulateGrad]
	1886965455056 -> 1886965454912
	1886965454720 -> 1886965454528
	1886965454576 -> 1886965454336
	1884176053616 [label="decoder.blocks.0.conv1.0.weight
 (256, 768, 3, 3)" fillcolor=lightblue]
	1884176053616 -> 1886965454576
	1886965454576 [label=AccumulateGrad]
	1886965454384 -> 1886965454240
	1884176053936 [label="decoder.blocks.0.conv1.1.weight
 (256)" fillcolor=lightblue]
	1884176053936 -> 1886965454384
	1886965454384 [label=AccumulateGrad]
	1886965454192 -> 1886965454240
	1884176053536 [label="decoder.blocks.0.conv1.1.bias
 (256)" fillcolor=lightblue]
	1884176053536 -> 1886965454192
	1886965454192 [label=AccumulateGrad]
	1886965454144 -> 1886965453904
	1884284754176 [label="decoder.blocks.0.conv2.0.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1884284754176 -> 1886965454144
	1886965454144 [label=AccumulateGrad]
	1886965453952 -> 1886965453808
	1884284756496 [label="decoder.blocks.0.conv2.1.weight
 (256)" fillcolor=lightblue]
	1884284756496 -> 1886965453952
	1886965453952 [label=AccumulateGrad]
	1886965453664 -> 1886965453808
	1884284753936 [label="decoder.blocks.0.conv2.1.bias
 (256)" fillcolor=lightblue]
	1884284753936 -> 1886965453664
	1886965453664 [label=AccumulateGrad]
	1886965453616 -> 1886965453424
	1886965453472 -> 1886965453232
	1884284754496 [label="decoder.blocks.1.conv1.0.weight
 (128, 384, 3, 3)" fillcolor=lightblue]
	1884284754496 -> 1886965453472
	1886965453472 [label=AccumulateGrad]
	1886965453280 -> 1886965453136
	1884284753056 [label="decoder.blocks.1.conv1.1.weight
 (128)" fillcolor=lightblue]
	1884284753056 -> 1886965453280
	1886965453280 [label=AccumulateGrad]
	1886965453088 -> 1886965453136
	1884284752176 [label="decoder.blocks.1.conv1.1.bias
 (128)" fillcolor=lightblue]
	1884284752176 -> 1886965453088
	1886965453088 [label=AccumulateGrad]
	1886965453040 -> 1886965452800
	1884284890928 [label="decoder.blocks.1.conv2.0.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1884284890928 -> 1886965453040
	1886965453040 [label=AccumulateGrad]
	1886965452848 -> 1886965452704
	1884284887088 [label="decoder.blocks.1.conv2.1.weight
 (128)" fillcolor=lightblue]
	1884284887088 -> 1886965452848
	1886965452848 [label=AccumulateGrad]
	1886965452560 -> 1886965452704
	1884284880288 [label="decoder.blocks.1.conv2.1.bias
 (128)" fillcolor=lightblue]
	1884284880288 -> 1886965452560
	1886965452560 [label=AccumulateGrad]
	1886965452512 -> 1886965452320
	1886965452368 -> 1886965452128
	1884284894528 [label="decoder.blocks.2.conv1.0.weight
 (64, 192, 3, 3)" fillcolor=lightblue]
	1884284894528 -> 1886965452368
	1886965452368 [label=AccumulateGrad]
	1886965452176 -> 1886965452032
	1884284880928 [label="decoder.blocks.2.conv1.1.weight
 (64)" fillcolor=lightblue]
	1884284880928 -> 1886965452176
	1886965452176 [label=AccumulateGrad]
	1886965451984 -> 1886965452032
	1884284883728 [label="decoder.blocks.2.conv1.1.bias
 (64)" fillcolor=lightblue]
	1884284883728 -> 1886965451984
	1886965451984 [label=AccumulateGrad]
	1886965451936 -> 1886965451696
	1884284878384 [label="decoder.blocks.2.conv2.0.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1884284878384 -> 1886965451936
	1886965451936 [label=AccumulateGrad]
	1886965451744 -> 1886965451600
	1884284872704 [label="decoder.blocks.2.conv2.1.weight
 (64)" fillcolor=lightblue]
	1884284872704 -> 1886965451744
	1886965451744 [label=AccumulateGrad]
	1886965451456 -> 1886965451600
	1885069302496 [label="decoder.blocks.2.conv2.1.bias
 (64)" fillcolor=lightblue]
	1885069302496 -> 1886965451456
	1886965451456 [label=AccumulateGrad]
	1886965451408 -> 1886965451264
	1886965451024 -> 1886965451072
	1885069398640 [label="decoder.blocks.3.conv1.0.weight
 (32, 128, 3, 3)" fillcolor=lightblue]
	1885069398640 -> 1886965451024
	1886965451024 [label=AccumulateGrad]
	1886965451120 -> 1886965450640
	1885069401360 [label="decoder.blocks.3.conv1.1.weight
 (32)" fillcolor=lightblue]
	1885069401360 -> 1886965451120
	1886965451120 [label=AccumulateGrad]
	1886965450544 -> 1886965450640
	1885069405680 [label="decoder.blocks.3.conv1.1.bias
 (32)" fillcolor=lightblue]
	1885069405680 -> 1886965450544
	1886965450544 [label=AccumulateGrad]
	1886965450832 -> 1886965450448
	1885069408080 [label="decoder.blocks.3.conv2.0.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	1885069408080 -> 1886965450832
	1886965450832 [label=AccumulateGrad]
	1886965450304 -> 1886965450784
	1885069401920 [label="decoder.blocks.3.conv2.1.weight
 (32)" fillcolor=lightblue]
	1885069401920 -> 1886965450304
	1886965450304 [label=AccumulateGrad]
	1886965450400 -> 1886965450784
	1885069395680 [label="decoder.blocks.3.conv2.1.bias
 (32)" fillcolor=lightblue]
	1885069395680 -> 1886965450400
	1886965450400 [label=AccumulateGrad]
	1886965450352 -> 1886964805792
	1885069402160 [label="decoder.blocks.4.conv1.0.weight
 (16, 32, 3, 3)" fillcolor=lightblue]
	1885069402160 -> 1886965450352
	1886965450352 [label=AccumulateGrad]
	1886965449776 -> 1884006333840
	1885069399920 [label="decoder.blocks.4.conv1.1.weight
 (16)" fillcolor=lightblue]
	1885069399920 -> 1886965449776
	1886965449776 [label=AccumulateGrad]
	1886965450208 -> 1884006333840
	1885069409680 [label="decoder.blocks.4.conv1.1.bias
 (16)" fillcolor=lightblue]
	1885069409680 -> 1886965450208
	1886965450208 [label=AccumulateGrad]
	1884169058144 -> 1884170800528
	1885069409120 [label="decoder.blocks.4.conv2.0.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
	1885069409120 -> 1884169058144
	1884169058144 [label=AccumulateGrad]
	1884005994592 -> 1884130352272
	1885069400720 [label="decoder.blocks.4.conv2.1.weight
 (16)" fillcolor=lightblue]
	1885069400720 -> 1884005994592
	1884005994592 [label=AccumulateGrad]
	1886965356464 -> 1884130352272
	1885069403120 [label="decoder.blocks.4.conv2.1.bias
 (16)" fillcolor=lightblue]
	1885069403120 -> 1886965356464
	1886965356464 [label=AccumulateGrad]
	1885069147248 -> 1884006189664
	1885069399440 [label="segmentation_head.0.weight
 (5, 16, 3, 3)" fillcolor=lightblue]
	1885069399440 -> 1885069147248
	1885069147248 [label=AccumulateGrad]
	1885069144800 -> 1884006189664
	1885069408560 [label="segmentation_head.0.bias
 (5)" fillcolor=lightblue]
	1885069408560 -> 1885069144800
	1885069144800 [label=AccumulateGrad]
	1884006189664 -> 1886965466928
}
