digraph {
	graph [size="326.7,326.7"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2067552270240 [label="
 (1, 5, 256, 256)" fillcolor=darkolivegreen1]
	2062726860848 [label=ConvolutionBackward0]
	2062839713456 -> 2062726860848
	2062839713456 [label=ReluBackward0]
	2062725331120 -> 2062839713456
	2062725331120 [label=CudnnBatchNormBackward0]
	2062732664528 -> 2062725331120
	2062732664528 [label=ConvolutionBackward0]
	2062841207760 -> 2062732664528
	2062841207760 [label=ReluBackward0]
	2062839832320 -> 2062841207760
	2062839832320 [label=CudnnBatchNormBackward0]
	2062839835968 -> 2062839832320
	2062839835968 [label=ConvolutionBackward0]
	2062616526368 -> 2062839835968
	2062616526368 [label=UpsampleNearest2DBackward0]
	2062616525120 -> 2062616526368
	2062616525120 [label=ReluBackward0]
	2062616525312 -> 2062616525120
	2062616525312 [label=CudnnBatchNormBackward0]
	2062616524592 -> 2062616525312
	2062616524592 [label=ConvolutionBackward0]
	2062616523968 -> 2062616524592
	2062616523968 [label=ReluBackward0]
	2062616524208 -> 2062616523968
	2062616524208 [label=CudnnBatchNormBackward0]
	2062616523392 -> 2062616524208
	2062616523392 [label=ConvolutionBackward0]
	2062616522816 -> 2062616523392
	2062616522816 [label=CatBackward0]
	2062841126656 -> 2062616522816
	2062841126656 [label=UpsampleNearest2DBackward0]
	2062616522336 -> 2062841126656
	2062616522336 [label=ReluBackward0]
	2062616521520 -> 2062616522336
	2062616521520 [label=CudnnBatchNormBackward0]
	2062616521712 -> 2062616521520
	2062616521712 [label=ConvolutionBackward0]
	2062616521088 -> 2062616521712
	2062616521088 [label=ReluBackward0]
	2062616520416 -> 2062616521088
	2062616520416 [label=CudnnBatchNormBackward0]
	2062616520608 -> 2062616520416
	2062616520608 [label=ConvolutionBackward0]
	2062616520032 -> 2062616520608
	2062616520032 [label=CatBackward0]
	2062616519456 -> 2062616520032
	2062616519456 [label=UpsampleNearest2DBackward0]
	2062616518784 -> 2062616519456
	2062616518784 [label=ReluBackward0]
	2062616518928 -> 2062616518784
	2062616518928 [label=CudnnBatchNormBackward0]
	2062616518208 -> 2062616518928
	2062616518208 [label=ConvolutionBackward0]
	2062616517632 -> 2062616518208
	2062616517632 [label=ReluBackward0]
	2062616517008 -> 2062616517632
	2062616517008 [label=CudnnBatchNormBackward0]
	2062616517152 -> 2062616517008
	2062616517152 [label=ConvolutionBackward0]
	2062616516576 -> 2062616517152
	2062616516576 [label=CatBackward0]
	2062616515904 -> 2062616516576
	2062616515904 [label=UpsampleNearest2DBackward0]
	2062616515376 -> 2062616515904
	2062616515376 [label=ReluBackward0]
	2062616515568 -> 2062616515376
	2062616515568 [label=CudnnBatchNormBackward0]
	2062616514752 -> 2062616515568
	2062616514752 [label=ConvolutionBackward0]
	2062616514128 -> 2062616514752
	2062616514128 [label=ReluBackward0]
	2062616514368 -> 2062616514128
	2062616514368 [label=CudnnBatchNormBackward0]
	2062616513600 -> 2062616514368
	2062616513600 [label=ConvolutionBackward0]
	2062616513024 -> 2062616513600
	2062616513024 [label=CatBackward0]
	2062616512400 -> 2062616513024
	2062616512400 [label=UpsampleNearest2DBackward0]
	2062616512640 -> 2062616512400
	2062616512640 [label=ReluBackward0]
	2062616511872 -> 2062616512640
	2062616511872 [label=AddBackward0]
	2062616512016 -> 2062616511872
	2062616512016 [label=CudnnBatchNormBackward0]
	2062616511440 -> 2062616512016
	2062616511440 [label=ConvolutionBackward0]
	2062616510960 -> 2062616511440
	2062616510960 [label=ReluBackward0]
	2062616522432 -> 2062616510960
	2062616522432 [label=CudnnBatchNormBackward0]
	2062616526800 -> 2062616522432
	2062616526800 [label=ConvolutionBackward0]
	2062616526272 -> 2062616526800
	2062616526272 [label=ReluBackward0]
	2062616526320 -> 2062616526272
	2062616526320 [label=CudnnBatchNormBackward0]
	2062616526224 -> 2062616526320
	2062616526224 [label=ConvolutionBackward0]
	2062616511296 -> 2062616526224
	2062616511296 [label=ReluBackward0]
	2062616525696 -> 2062616511296
	2062616525696 [label=AddBackward0]
	2062616525600 -> 2062616525696
	2062616525600 [label=CudnnBatchNormBackward0]
	2062616525024 -> 2062616525600
	2062616525024 [label=ConvolutionBackward0]
	2062616524976 -> 2062616525024
	2062616524976 [label=ReluBackward0]
	2062616524496 -> 2062616524976
	2062616524496 [label=CudnnBatchNormBackward0]
	2062616524400 -> 2062616524496
	2062616524400 [label=ConvolutionBackward0]
	2062616524112 -> 2062616524400
	2062616524112 [label=ReluBackward0]
	2062616523680 -> 2062616524112
	2062616523680 [label=CudnnBatchNormBackward0]
	2062616523920 -> 2062616523680
	2062616523920 [label=ConvolutionBackward0]
	2062616525264 -> 2062616523920
	2062616525264 [label=ReluBackward0]
	2062616523056 -> 2062616525264
	2062616523056 [label=AddBackward0]
	2062616523296 -> 2062616523056
	2062616523296 [label=CudnnBatchNormBackward0]
	2062616522864 -> 2062616523296
	2062616522864 [label=ConvolutionBackward0]
	2062616522720 -> 2062616522864
	2062616522720 [label=ReluBackward0]
	2062616522288 -> 2062616522720
	2062616522288 [label=CudnnBatchNormBackward0]
	2062616522048 -> 2062616522288
	2062616522048 [label=ConvolutionBackward0]
	2062616521904 -> 2062616522048
	2062616521904 [label=ReluBackward0]
	2062616521616 -> 2062616521904
	2062616521616 [label=CudnnBatchNormBackward0]
	2062616521328 -> 2062616521616
	2062616521328 [label=ConvolutionBackward0]
	2062616512448 -> 2062616521328
	2062616512448 [label=ReluBackward0]
	2062616520800 -> 2062616512448
	2062616520800 [label=AddBackward0]
	2062616520848 -> 2062616520800
	2062616520848 [label=CudnnBatchNormBackward0]
	2062616520272 -> 2062616520848
	2062616520272 [label=ConvolutionBackward0]
	2062616520128 -> 2062616520272
	2062616520128 [label=ReluBackward0]
	2062616519744 -> 2062616520128
	2062616519744 [label=CudnnBatchNormBackward0]
	2062616519792 -> 2062616519744
	2062616519792 [label=ConvolutionBackward0]
	2062616519360 -> 2062616519792
	2062616519360 [label=ReluBackward0]
	2062616519072 -> 2062616519360
	2062616519072 [label=CudnnBatchNormBackward0]
	2062616519120 -> 2062616519072
	2062616519120 [label=ConvolutionBackward0]
	2062616521376 -> 2062616519120
	2062616521376 [label=ReluBackward0]
	2062616518496 -> 2062616521376
	2062616518496 [label=AddBackward0]
	2062616518064 -> 2062616518496
	2062616518064 [label=CudnnBatchNormBackward0]
	2062616518112 -> 2062616518064
	2062616518112 [label=ConvolutionBackward0]
	2062616517488 -> 2062616518112
	2062616517488 [label=ReluBackward0]
	2062616517536 -> 2062616517488
	2062616517536 [label=CudnnBatchNormBackward0]
	2062616517104 -> 2062616517536
	2062616517104 [label=ConvolutionBackward0]
	2062616516960 -> 2062616517104
	2062616516960 [label=ReluBackward0]
	2062616516528 -> 2062616516960
	2062616516528 [label=CudnnBatchNormBackward0]
	2062616516096 -> 2062616516528
	2062616516096 [label=ConvolutionBackward0]
	2062616518448 -> 2062616516096
	2062616518448 [label=ReluBackward0]
	2062616515760 -> 2062616518448
	2062616515760 [label=AddBackward0]
	2062616515712 -> 2062616515760
	2062616515712 [label=CudnnBatchNormBackward0]
	2062616515280 -> 2062616515712
	2062616515280 [label=ConvolutionBackward0]
	2062616515136 -> 2062616515280
	2062616515136 [label=ReluBackward0]
	2062616514848 -> 2062616515136
	2062616514848 [label=CudnnBatchNormBackward0]
	2062616514560 -> 2062616514848
	2062616514560 [label=ConvolutionBackward0]
	2062616514272 -> 2062616514560
	2062616514272 [label=ReluBackward0]
	2062616513840 -> 2062616514272
	2062616513840 [label=CudnnBatchNormBackward0]
	2062616513888 -> 2062616513840
	2062616513888 [label=ConvolutionBackward0]
	2062616516144 -> 2062616513888
	2062616516144 [label=ReluBackward0]
	2062616513360 -> 2062616516144
	2062616513360 [label=AddBackward0]
	2062616513408 -> 2062616513360
	2062616513408 [label=CudnnBatchNormBackward0]
	2062616512880 -> 2062616513408
	2062616512880 [label=ConvolutionBackward0]
	2062616512736 -> 2062616512880
	2062616512736 [label=ReluBackward0]
	2062616512496 -> 2062616512736
	2062616512496 [label=CudnnBatchNormBackward0]
	2062616512208 -> 2062616512496
	2062616512208 [label=ConvolutionBackward0]
	2062616511920 -> 2062616512208
	2062616511920 [label=ReluBackward0]
	2062616511632 -> 2062616511920
	2062616511632 [label=CudnnBatchNormBackward0]
	2062616511680 -> 2062616511632
	2062616511680 [label=ConvolutionBackward0]
	2062616513648 -> 2062616511680
	2062616513648 [label=ReluBackward0]
	2062616511152 -> 2062616513648
	2062616511152 [label=AddBackward0]
	2062616510672 -> 2062616511152
	2062616510672 [label=CudnnBatchNormBackward0]
	2062616510528 -> 2062616510672
	2062616510528 [label=ConvolutionBackward0]
	2062616517824 -> 2062616510528
	2062616517824 [label=ReluBackward0]
	2062616514416 -> 2062616517824
	2062616514416 [label=CudnnBatchNormBackward0]
	2062616606800 -> 2062616514416
	2062616606800 [label=ConvolutionBackward0]
	2062616595568 -> 2062616606800
	2062616595568 [label=ReluBackward0]
	2062616595088 -> 2062616595568
	2062616595088 [label=CudnnBatchNormBackward0]
	2062616594416 -> 2062616595088
	2062616594416 [label=ConvolutionBackward0]
	2062616511104 -> 2062616594416
	2062616511104 [label=ReluBackward0]
	2062616594032 -> 2062616511104
	2062616594032 [label=AddBackward0]
	2062616593408 -> 2062616594032
	2062616593408 [label=CudnnBatchNormBackward0]
	2062616592640 -> 2062616593408
	2062616592640 [label=ConvolutionBackward0]
	2062616595616 -> 2062616592640
	2062616595616 [label=ReluBackward0]
	2062616595328 -> 2062616595616
	2062616595328 [label=CudnnBatchNormBackward0]
	2062616595376 -> 2062616595328
	2062616595376 [label=ConvolutionBackward0]
	2062616594608 -> 2062616595376
	2062616594608 [label=ReluBackward0]
	2062616594656 -> 2062616594608
	2062616594656 [label=CudnnBatchNormBackward0]
	2062616594272 -> 2062616594656
	2062616594272 [label=ConvolutionBackward0]
	2062616593840 -> 2062616594272
	2062616593840 [label=ReluBackward0]
	2062616593936 -> 2062616593840
	2062616593936 [label=AddBackward0]
	2062616593648 -> 2062616593936
	2062616593648 [label=CudnnBatchNormBackward0]
	2062616593696 -> 2062616593648
	2062616593696 [label=ConvolutionBackward0]
	2062616592928 -> 2062616593696
	2062616592928 [label=ReluBackward0]
	2062616592976 -> 2062616592928
	2062616592976 [label=CudnnBatchNormBackward0]
	2062616592688 -> 2062616592976
	2062616592688 [label=ConvolutionBackward0]
	2062616596048 -> 2062616592688
	2062616596048 [label=ReluBackward0]
	2062616595904 -> 2062616596048
	2062616595904 [label=CudnnBatchNormBackward0]
	2062616596336 -> 2062616595904
	2062616596336 [label=ConvolutionBackward0]
	2062616594320 -> 2062616596336
	2062616594320 [label=ReluBackward0]
	2062668227120 -> 2062616594320
	2062668227120 [label=AddBackward0]
	2062668227264 -> 2062668227120
	2062668227264 [label=CudnnBatchNormBackward0]
	2062668227408 -> 2062668227264
	2062668227408 [label=ConvolutionBackward0]
	2062668227600 -> 2062668227408
	2062668227600 [label=ReluBackward0]
	2062668227744 -> 2062668227600
	2062668227744 [label=CudnnBatchNormBackward0]
	2062668227888 -> 2062668227744
	2062668227888 [label=ConvolutionBackward0]
	2062668228080 -> 2062668227888
	2062668228080 [label=ReluBackward0]
	2062668228224 -> 2062668228080
	2062668228224 [label=CudnnBatchNormBackward0]
	2062668228368 -> 2062668228224
	2062668228368 [label=ConvolutionBackward0]
	2062668226784 -> 2062668228368
	2062668226784 [label=ReluBackward0]
	2062668228656 -> 2062668226784
	2062668228656 [label=AddBackward0]
	2062668228800 -> 2062668228656
	2062668228800 [label=CudnnBatchNormBackward0]
	2062668228944 -> 2062668228800
	2062668228944 [label=ConvolutionBackward0]
	2062668229136 -> 2062668228944
	2062668229136 [label=ReluBackward0]
	2062668229280 -> 2062668229136
	2062668229280 [label=CudnnBatchNormBackward0]
	2062668229424 -> 2062668229280
	2062668229424 [label=ConvolutionBackward0]
	2062668229616 -> 2062668229424
	2062668229616 [label=ReluBackward0]
	2062668229760 -> 2062668229616
	2062668229760 [label=CudnnBatchNormBackward0]
	2062668229904 -> 2062668229760
	2062668229904 [label=ConvolutionBackward0]
	2062668228512 -> 2062668229904
	2062668228512 [label=ReluBackward0]
	2062668230192 -> 2062668228512
	2062668230192 [label=AddBackward0]
	2062668230336 -> 2062668230192
	2062668230336 [label=CudnnBatchNormBackward0]
	2062668230480 -> 2062668230336
	2062668230480 [label=ConvolutionBackward0]
	2062668230672 -> 2062668230480
	2062668230672 [label=ReluBackward0]
	2062668230816 -> 2062668230672
	2062668230816 [label=CudnnBatchNormBackward0]
	2062668230960 -> 2062668230816
	2062668230960 [label=ConvolutionBackward0]
	2062668231152 -> 2062668230960
	2062668231152 [label=ReluBackward0]
	2062668231296 -> 2062668231152
	2062668231296 [label=CudnnBatchNormBackward0]
	2062668231440 -> 2062668231296
	2062668231440 [label=ConvolutionBackward0]
	2062668230048 -> 2062668231440
	2062668230048 [label=ReluBackward0]
	2062668231728 -> 2062668230048
	2062668231728 [label=AddBackward0]
	2062668231872 -> 2062668231728
	2062668231872 [label=CudnnBatchNormBackward0]
	2062668232016 -> 2062668231872
	2062668232016 [label=ConvolutionBackward0]
	2062668232208 -> 2062668232016
	2062668232208 [label=ReluBackward0]
	2062668232352 -> 2062668232208
	2062668232352 [label=CudnnBatchNormBackward0]
	2062668232496 -> 2062668232352
	2062668232496 [label=ConvolutionBackward0]
	2062668232688 -> 2062668232496
	2062668232688 [label=ReluBackward0]
	2062668232832 -> 2062668232688
	2062668232832 [label=CudnnBatchNormBackward0]
	2062668232976 -> 2062668232832
	2062668232976 [label=ConvolutionBackward0]
	2062668231584 -> 2062668232976
	2062668231584 [label=ReluBackward0]
	2062668233264 -> 2062668231584
	2062668233264 [label=AddBackward0]
	2062668233408 -> 2062668233264
	2062668233408 [label=CudnnBatchNormBackward0]
	2062668233552 -> 2062668233408
	2062668233552 [label=ConvolutionBackward0]
	2062668233744 -> 2062668233552
	2062668233744 [label=ReluBackward0]
	2062668233888 -> 2062668233744
	2062668233888 [label=CudnnBatchNormBackward0]
	2062668234032 -> 2062668233888
	2062668234032 [label=ConvolutionBackward0]
	2062668234224 -> 2062668234032
	2062668234224 [label=ReluBackward0]
	2062668234368 -> 2062668234224
	2062668234368 [label=CudnnBatchNormBackward0]
	2062668234512 -> 2062668234368
	2062668234512 [label=ConvolutionBackward0]
	2062668233120 -> 2062668234512
	2062668233120 [label=ReluBackward0]
	2067585925232 -> 2062668233120
	2067585925232 [label=AddBackward0]
	2067585925376 -> 2067585925232
	2067585925376 [label=CudnnBatchNormBackward0]
	2067585925520 -> 2067585925376
	2067585925520 [label=ConvolutionBackward0]
	2067585925712 -> 2067585925520
	2067585925712 [label=ReluBackward0]
	2067585925856 -> 2067585925712
	2067585925856 [label=CudnnBatchNormBackward0]
	2067585926000 -> 2067585925856
	2067585926000 [label=ConvolutionBackward0]
	2067585926192 -> 2067585926000
	2067585926192 [label=ReluBackward0]
	2067585926336 -> 2067585926192
	2067585926336 [label=CudnnBatchNormBackward0]
	2067585926480 -> 2067585926336
	2067585926480 [label=ConvolutionBackward0]
	2067585925184 -> 2067585926480
	2067585925184 [label=ReluBackward0]
	2067585926768 -> 2067585925184
	2067585926768 [label=AddBackward0]
	2067585926912 -> 2067585926768
	2067585926912 [label=CudnnBatchNormBackward0]
	2067585927056 -> 2067585926912
	2067585927056 [label=ConvolutionBackward0]
	2067585927248 -> 2067585927056
	2067585927248 [label=ReluBackward0]
	2067585927392 -> 2067585927248
	2067585927392 [label=CudnnBatchNormBackward0]
	2067585927536 -> 2067585927392
	2067585927536 [label=ConvolutionBackward0]
	2067585927728 -> 2067585927536
	2067585927728 [label=ReluBackward0]
	2067585927872 -> 2067585927728
	2067585927872 [label=CudnnBatchNormBackward0]
	2067585928016 -> 2067585927872
	2067585928016 [label=ConvolutionBackward0]
	2067585926624 -> 2067585928016
	2067585926624 [label=ReluBackward0]
	2067585928304 -> 2067585926624
	2067585928304 [label=AddBackward0]
	2067585928448 -> 2067585928304
	2067585928448 [label=CudnnBatchNormBackward0]
	2067585928592 -> 2067585928448
	2067585928592 [label=ConvolutionBackward0]
	2067585928784 -> 2067585928592
	2067585928784 [label=ReluBackward0]
	2067585928928 -> 2067585928784
	2067585928928 [label=CudnnBatchNormBackward0]
	2067585929072 -> 2067585928928
	2067585929072 [label=ConvolutionBackward0]
	2067585929264 -> 2067585929072
	2067585929264 [label=ReluBackward0]
	2067585929408 -> 2067585929264
	2067585929408 [label=CudnnBatchNormBackward0]
	2067585929552 -> 2067585929408
	2067585929552 [label=ConvolutionBackward0]
	2067585928160 -> 2067585929552
	2067585928160 [label=ReluBackward0]
	2067585929840 -> 2067585928160
	2067585929840 [label=AddBackward0]
	2067585929984 -> 2067585929840
	2067585929984 [label=CudnnBatchNormBackward0]
	2067585930128 -> 2067585929984
	2067585930128 [label=ConvolutionBackward0]
	2067585930320 -> 2067585930128
	2067585930320 [label=ReluBackward0]
	2067585930464 -> 2067585930320
	2067585930464 [label=CudnnBatchNormBackward0]
	2067585930608 -> 2067585930464
	2067585930608 [label=ConvolutionBackward0]
	2067585930800 -> 2067585930608
	2067585930800 [label=ReluBackward0]
	2067585930944 -> 2067585930800
	2067585930944 [label=CudnnBatchNormBackward0]
	2067585931088 -> 2067585930944
	2067585931088 [label=ConvolutionBackward0]
	2067585929696 -> 2067585931088
	2067585929696 [label=ReluBackward0]
	2067585931376 -> 2067585929696
	2067585931376 [label=AddBackward0]
	2067585931520 -> 2067585931376
	2067585931520 [label=CudnnBatchNormBackward0]
	2067585931664 -> 2067585931520
	2067585931664 [label=ConvolutionBackward0]
	2067585931856 -> 2067585931664
	2067585931856 [label=ReluBackward0]
	2067585932000 -> 2067585931856
	2067585932000 [label=CudnnBatchNormBackward0]
	2067585932144 -> 2067585932000
	2067585932144 [label=ConvolutionBackward0]
	2067585932336 -> 2067585932144
	2067585932336 [label=ReluBackward0]
	2067585932480 -> 2067585932336
	2067585932480 [label=CudnnBatchNormBackward0]
	2067585932624 -> 2067585932480
	2067585932624 [label=ConvolutionBackward0]
	2067585931232 -> 2067585932624
	2067585931232 [label=ReluBackward0]
	2067585932912 -> 2067585931232
	2067585932912 [label=AddBackward0]
	2067585933056 -> 2067585932912
	2067585933056 [label=CudnnBatchNormBackward0]
	2067585933200 -> 2067585933056
	2067585933200 [label=ConvolutionBackward0]
	2067585933392 -> 2067585933200
	2067585933392 [label=ReluBackward0]
	2067585933536 -> 2067585933392
	2067585933536 [label=CudnnBatchNormBackward0]
	2067585933680 -> 2067585933536
	2067585933680 [label=ConvolutionBackward0]
	2067585933872 -> 2067585933680
	2067585933872 [label=ReluBackward0]
	2067585934016 -> 2067585933872
	2067585934016 [label=CudnnBatchNormBackward0]
	2067585934160 -> 2067585934016
	2067585934160 [label=ConvolutionBackward0]
	2067585932768 -> 2067585934160
	2067585932768 [label=ReluBackward0]
	2067585934448 -> 2067585932768
	2067585934448 [label=AddBackward0]
	2067585934592 -> 2067585934448
	2067585934592 [label=CudnnBatchNormBackward0]
	2067585934736 -> 2067585934592
	2067585934736 [label=ConvolutionBackward0]
	2067585934928 -> 2067585934736
	2067585934928 [label=ReluBackward0]
	2067585935072 -> 2067585934928
	2067585935072 [label=CudnnBatchNormBackward0]
	2067585935216 -> 2067585935072
	2067585935216 [label=ConvolutionBackward0]
	2067585935408 -> 2067585935216
	2067585935408 [label=ReluBackward0]
	2067585935552 -> 2067585935408
	2067585935552 [label=CudnnBatchNormBackward0]
	2067585935696 -> 2067585935552
	2067585935696 [label=ConvolutionBackward0]
	2067585934304 -> 2067585935696
	2067585934304 [label=ReluBackward0]
	2067585935984 -> 2067585934304
	2067585935984 [label=AddBackward0]
	2067585936128 -> 2067585935984
	2067585936128 [label=CudnnBatchNormBackward0]
	2067585936272 -> 2067585936128
	2067585936272 [label=ConvolutionBackward0]
	2067585936464 -> 2067585936272
	2067585936464 [label=ReluBackward0]
	2067585936608 -> 2067585936464
	2067585936608 [label=CudnnBatchNormBackward0]
	2067585936752 -> 2067585936608
	2067585936752 [label=ConvolutionBackward0]
	2067585936944 -> 2067585936752
	2067585936944 [label=ReluBackward0]
	2067585937088 -> 2067585936944
	2067585937088 [label=CudnnBatchNormBackward0]
	2067585937232 -> 2067585937088
	2067585937232 [label=ConvolutionBackward0]
	2067585935840 -> 2067585937232
	2067585935840 [label=ReluBackward0]
	2067585937520 -> 2067585935840
	2067585937520 [label=AddBackward0]
	2067585937664 -> 2067585937520
	2067585937664 [label=CudnnBatchNormBackward0]
	2067585937808 -> 2067585937664
	2067585937808 [label=ConvolutionBackward0]
	2067585938000 -> 2067585937808
	2067585938000 [label=ReluBackward0]
	2067585938144 -> 2067585938000
	2067585938144 [label=CudnnBatchNormBackward0]
	2067585938288 -> 2067585938144
	2067585938288 [label=ConvolutionBackward0]
	2067585938480 -> 2067585938288
	2067585938480 [label=ReluBackward0]
	2067585938624 -> 2067585938480
	2067585938624 [label=CudnnBatchNormBackward0]
	2067585938768 -> 2067585938624
	2067585938768 [label=ConvolutionBackward0]
	2067585937376 -> 2067585938768
	2067585937376 [label=ReluBackward0]
	2067585939056 -> 2067585937376
	2067585939056 [label=AddBackward0]
	2067585939200 -> 2067585939056
	2067585939200 [label=CudnnBatchNormBackward0]
	2067585939344 -> 2067585939200
	2067585939344 [label=ConvolutionBackward0]
	2067585939536 -> 2067585939344
	2067585939536 [label=ReluBackward0]
	2067585939680 -> 2067585939536
	2067585939680 [label=CudnnBatchNormBackward0]
	2067585939824 -> 2067585939680
	2067585939824 [label=ConvolutionBackward0]
	2067585940016 -> 2067585939824
	2067585940016 [label=ReluBackward0]
	2067585940160 -> 2067585940016
	2067585940160 [label=CudnnBatchNormBackward0]
	2067585940304 -> 2067585940160
	2067585940304 [label=ConvolutionBackward0]
	2067585938912 -> 2067585940304
	2067585938912 [label=ReluBackward0]
	2067585940592 -> 2067585938912
	2067585940592 [label=AddBackward0]
	2067585940736 -> 2067585940592
	2067585940736 [label=CudnnBatchNormBackward0]
	2067585940880 -> 2067585940736
	2067585940880 [label=ConvolutionBackward0]
	2067585941072 -> 2067585940880
	2067585941072 [label=ReluBackward0]
	2067585941216 -> 2067585941072
	2067585941216 [label=CudnnBatchNormBackward0]
	2067585941360 -> 2067585941216
	2067585941360 [label=ConvolutionBackward0]
	2067585974384 -> 2067585941360
	2067585974384 [label=ReluBackward0]
	2067585974528 -> 2067585974384
	2067585974528 [label=CudnnBatchNormBackward0]
	2067585974672 -> 2067585974528
	2067585974672 [label=ConvolutionBackward0]
	2062616516000 -> 2067585974672
	2062616516000 [label=ReluBackward0]
	2067585974960 -> 2062616516000
	2067585974960 [label=AddBackward0]
	2067585975104 -> 2067585974960
	2067585975104 [label=CudnnBatchNormBackward0]
	2067585975248 -> 2067585975104
	2067585975248 [label=ConvolutionBackward0]
	2067585975440 -> 2067585975248
	2067585975440 [label=ReluBackward0]
	2067585975584 -> 2067585975440
	2067585975584 [label=CudnnBatchNormBackward0]
	2067585975728 -> 2067585975584
	2067585975728 [label=ConvolutionBackward0]
	2067585975920 -> 2067585975728
	2067585975920 [label=ReluBackward0]
	2067585976064 -> 2067585975920
	2067585976064 [label=CudnnBatchNormBackward0]
	2067585976208 -> 2067585976064
	2067585976208 [label=ConvolutionBackward0]
	2067585974816 -> 2067585976208
	2067585974816 [label=ReluBackward0]
	2067585976496 -> 2067585974816
	2067585976496 [label=AddBackward0]
	2067585976640 -> 2067585976496
	2067585976640 [label=CudnnBatchNormBackward0]
	2067585976784 -> 2067585976640
	2067585976784 [label=ConvolutionBackward0]
	2067585976976 -> 2067585976784
	2067585976976 [label=ReluBackward0]
	2067585977120 -> 2067585976976
	2067585977120 [label=CudnnBatchNormBackward0]
	2067585977264 -> 2067585977120
	2067585977264 [label=ConvolutionBackward0]
	2067585977456 -> 2067585977264
	2067585977456 [label=ReluBackward0]
	2067585977600 -> 2067585977456
	2067585977600 [label=CudnnBatchNormBackward0]
	2067585977744 -> 2067585977600
	2067585977744 [label=ConvolutionBackward0]
	2067585976352 -> 2067585977744
	2067585976352 [label=ReluBackward0]
	2067585978032 -> 2067585976352
	2067585978032 [label=AddBackward0]
	2067585978176 -> 2067585978032
	2067585978176 [label=CudnnBatchNormBackward0]
	2067585978320 -> 2067585978176
	2067585978320 [label=ConvolutionBackward0]
	2067585978512 -> 2067585978320
	2067585978512 [label=ReluBackward0]
	2067585978656 -> 2067585978512
	2067585978656 [label=CudnnBatchNormBackward0]
	2067585978800 -> 2067585978656
	2067585978800 [label=ConvolutionBackward0]
	2067585978992 -> 2067585978800
	2067585978992 [label=ReluBackward0]
	2067585979136 -> 2067585978992
	2067585979136 [label=CudnnBatchNormBackward0]
	2067585979280 -> 2067585979136
	2067585979280 [label=ConvolutionBackward0]
	2067585977888 -> 2067585979280
	2067585977888 [label=ReluBackward0]
	2067585979568 -> 2067585977888
	2067585979568 [label=AddBackward0]
	2067585979712 -> 2067585979568
	2067585979712 [label=CudnnBatchNormBackward0]
	2067585979856 -> 2067585979712
	2067585979856 [label=ConvolutionBackward0]
	2067585980048 -> 2067585979856
	2067585980048 [label=ReluBackward0]
	2067585980192 -> 2067585980048
	2067585980192 [label=CudnnBatchNormBackward0]
	2067585980336 -> 2067585980192
	2067585980336 [label=ConvolutionBackward0]
	2067585980528 -> 2067585980336
	2067585980528 [label=ReluBackward0]
	2067585980672 -> 2067585980528
	2067585980672 [label=CudnnBatchNormBackward0]
	2067585980816 -> 2067585980672
	2067585980816 [label=ConvolutionBackward0]
	2062616519504 -> 2067585980816
	2062616519504 [label=ReluBackward0]
	2067585981104 -> 2062616519504
	2067585981104 [label=AddBackward0]
	2067585981248 -> 2067585981104
	2067585981248 [label=CudnnBatchNormBackward0]
	2067585981392 -> 2067585981248
	2067585981392 [label=ConvolutionBackward0]
	2067585981584 -> 2067585981392
	2067585981584 [label=ReluBackward0]
	2067585981728 -> 2067585981584
	2067585981728 [label=CudnnBatchNormBackward0]
	2067585981872 -> 2067585981728
	2067585981872 [label=ConvolutionBackward0]
	2067585982064 -> 2067585981872
	2067585982064 [label=ReluBackward0]
	2067585982208 -> 2067585982064
	2067585982208 [label=CudnnBatchNormBackward0]
	2067585982352 -> 2067585982208
	2067585982352 [label=ConvolutionBackward0]
	2067585980960 -> 2067585982352
	2067585980960 [label=ReluBackward0]
	2067585982640 -> 2067585980960
	2067585982640 [label=AddBackward0]
	2067585982784 -> 2067585982640
	2067585982784 [label=CudnnBatchNormBackward0]
	2067585982928 -> 2067585982784
	2067585982928 [label=ConvolutionBackward0]
	2067585983120 -> 2067585982928
	2067585983120 [label=ReluBackward0]
	2067585983264 -> 2067585983120
	2067585983264 [label=CudnnBatchNormBackward0]
	2067585983408 -> 2067585983264
	2067585983408 [label=ConvolutionBackward0]
	2067585983600 -> 2067585983408
	2067585983600 [label=ReluBackward0]
	2067585983744 -> 2067585983600
	2067585983744 [label=CudnnBatchNormBackward0]
	2067585983888 -> 2067585983744
	2067585983888 [label=ConvolutionBackward0]
	2067585982496 -> 2067585983888
	2067585982496 [label=ReluBackward0]
	2067585984176 -> 2067585982496
	2067585984176 [label=AddBackward0]
	2067585984320 -> 2067585984176
	2067585984320 [label=CudnnBatchNormBackward0]
	2067585984464 -> 2067585984320
	2067585984464 [label=ConvolutionBackward0]
	2067585984656 -> 2067585984464
	2067585984656 [label=ReluBackward0]
	2067585984800 -> 2067585984656
	2067585984800 [label=CudnnBatchNormBackward0]
	2067585984944 -> 2067585984800
	2067585984944 [label=ConvolutionBackward0]
	2067585985136 -> 2067585984944
	2067585985136 [label=ReluBackward0]
	2067585985280 -> 2067585985136
	2067585985280 [label=CudnnBatchNormBackward0]
	2067585985424 -> 2067585985280
	2067585985424 [label=ConvolutionBackward0]
	2067585985616 -> 2067585985424
	2067585985616 [label=MaxPool2DWithIndicesBackward0]
	2062616522144 -> 2067585985616
	2062616522144 [label=ReluBackward0]
	2067585985856 -> 2062616522144
	2067585985856 [label=CudnnBatchNormBackward0]
	2067585986000 -> 2067585985856
	2067585986000 [label=ConvolutionBackward0]
	2067585986192 -> 2067585986000
	2063088330272 [label="encoder.conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	2063088330272 -> 2067585986192
	2067585986192 [label=AccumulateGrad]
	2067585985904 -> 2067585985856
	2062616543392 [label="encoder.bn1.weight
 (64)" fillcolor=lightblue]
	2062616543392 -> 2067585985904
	2067585985904 [label=AccumulateGrad]
	2067585986048 -> 2067585985856
	2062616543872 [label="encoder.bn1.bias
 (64)" fillcolor=lightblue]
	2062616543872 -> 2067585986048
	2067585986048 [label=AccumulateGrad]
	2067585985664 -> 2067585985424
	2063088332432 [label="encoder.layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2063088332432 -> 2067585985664
	2067585985664 [label=AccumulateGrad]
	2067585985232 -> 2067585985280
	2063088332272 [label="encoder.layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2063088332272 -> 2067585985232
	2067585985232 [label=AccumulateGrad]
	2067585985472 -> 2067585985280
	2063088331712 [label="encoder.layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2063088331712 -> 2067585985472
	2067585985472 [label=AccumulateGrad]
	2067585985184 -> 2067585984944
	2063088331552 [label="encoder.layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2063088331552 -> 2067585985184
	2067585985184 [label=AccumulateGrad]
	2067585984752 -> 2067585984800
	2063088331472 [label="encoder.layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2063088331472 -> 2067585984752
	2067585984752 [label=AccumulateGrad]
	2067585984992 -> 2067585984800
	2063088331632 [label="encoder.layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2063088331632 -> 2067585984992
	2067585984992 [label=AccumulateGrad]
	2067585984704 -> 2067585984464
	2063088330592 [label="encoder.layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2063088330592 -> 2067585984704
	2067585984704 [label=AccumulateGrad]
	2067585984512 -> 2067585984320
	2063088330672 [label="encoder.layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	2063088330672 -> 2067585984512
	2067585984512 [label=AccumulateGrad]
	2067585984416 -> 2067585984320
	2063088330832 [label="encoder.layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	2063088330832 -> 2067585984416
	2067585984416 [label=AccumulateGrad]
	2067585984032 -> 2067585984176
	2067585984032 [label=CudnnBatchNormBackward0]
	2067585984848 -> 2067585984032
	2067585984848 [label=ConvolutionBackward0]
	2067585985616 -> 2067585984848
	2067585985088 -> 2067585984848
	2063088333552 [label="encoder.layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2063088333552 -> 2067585985088
	2067585985088 [label=AccumulateGrad]
	2067585984608 -> 2067585984032
	2063088332512 [label="encoder.layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2063088332512 -> 2067585984608
	2067585984608 [label=AccumulateGrad]
	2067585984560 -> 2067585984032
	2063088333232 [label="encoder.layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2063088333232 -> 2067585984560
	2067585984560 [label=AccumulateGrad]
	2067585984080 -> 2067585983888
	2063088323632 [label="encoder.layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2063088323632 -> 2067585984080
	2067585984080 [label=AccumulateGrad]
	2067585983696 -> 2067585983744
	2063088333072 [label="encoder.layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2063088333072 -> 2067585983696
	2067585983696 [label=AccumulateGrad]
	2067585983936 -> 2067585983744
	2063088323792 [label="encoder.layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2063088323792 -> 2067585983936
	2067585983936 [label=AccumulateGrad]
	2067585983648 -> 2067585983408
	2063088324672 [label="encoder.layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2063088324672 -> 2067585983648
	2067585983648 [label=AccumulateGrad]
	2067585983216 -> 2067585983264
	2063088324912 [label="encoder.layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2063088324912 -> 2067585983216
	2067585983216 [label=AccumulateGrad]
	2067585983456 -> 2067585983264
	2063088324592 [label="encoder.layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2063088324592 -> 2067585983456
	2067585983456 [label=AccumulateGrad]
	2067585983168 -> 2067585982928
	2063088325392 [label="encoder.layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2063088325392 -> 2067585983168
	2067585983168 [label=AccumulateGrad]
	2067585982976 -> 2067585982784
	2063088326272 [label="encoder.layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	2063088326272 -> 2067585982976
	2067585982976 [label=AccumulateGrad]
	2067585982880 -> 2067585982784
	2063088326112 [label="encoder.layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	2063088326112 -> 2067585982880
	2067585982880 [label=AccumulateGrad]
	2067585982496 -> 2067585982640
	2067585982544 -> 2067585982352
	2063088326592 [label="encoder.layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2063088326592 -> 2067585982544
	2067585982544 [label=AccumulateGrad]
	2067585982160 -> 2067585982208
	2063088326432 [label="encoder.layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	2063088326432 -> 2067585982160
	2067585982160 [label=AccumulateGrad]
	2067585982400 -> 2067585982208
	2063088326672 [label="encoder.layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	2063088326672 -> 2067585982400
	2067585982400 [label=AccumulateGrad]
	2067585982112 -> 2067585981872
	2063088327872 [label="encoder.layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2063088327872 -> 2067585982112
	2067585982112 [label=AccumulateGrad]
	2067585981680 -> 2067585981728
	2063088327952 [label="encoder.layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	2063088327952 -> 2067585981680
	2067585981680 [label=AccumulateGrad]
	2067585981920 -> 2067585981728
	2063088327712 [label="encoder.layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	2063088327712 -> 2067585981920
	2067585981920 [label=AccumulateGrad]
	2067585981632 -> 2067585981392
	2063088328512 [label="encoder.layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2063088328512 -> 2067585981632
	2067585981632 [label=AccumulateGrad]
	2067585981440 -> 2067585981248
	2063088328992 [label="encoder.layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	2063088328992 -> 2067585981440
	2067585981440 [label=AccumulateGrad]
	2067585981344 -> 2067585981248
	2063088328832 [label="encoder.layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	2063088328832 -> 2067585981344
	2067585981344 [label=AccumulateGrad]
	2067585980960 -> 2067585981104
	2067585981008 -> 2067585980816
	2063088207664 [label="encoder.layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	2063088207664 -> 2067585981008
	2067585981008 [label=AccumulateGrad]
	2067585980624 -> 2067585980672
	2063088207584 [label="encoder.layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2063088207584 -> 2067585980624
	2067585980624 [label=AccumulateGrad]
	2067585980864 -> 2067585980672
	2063088207344 [label="encoder.layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2063088207344 -> 2067585980864
	2067585980864 [label=AccumulateGrad]
	2067585980576 -> 2067585980336
	2063088208624 [label="encoder.layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2063088208624 -> 2067585980576
	2067585980576 [label=AccumulateGrad]
	2067585980144 -> 2067585980192
	2063088207904 [label="encoder.layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2063088207904 -> 2067585980144
	2067585980144 [label=AccumulateGrad]
	2067585980384 -> 2067585980192
	2063088208544 [label="encoder.layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2063088208544 -> 2067585980384
	2067585980384 [label=AccumulateGrad]
	2067585980096 -> 2067585979856
	2063088209024 [label="encoder.layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2063088209024 -> 2067585980096
	2067585980096 [label=AccumulateGrad]
	2067585979904 -> 2067585979712
	2063088208944 [label="encoder.layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	2063088208944 -> 2067585979904
	2067585979904 [label=AccumulateGrad]
	2067585979808 -> 2067585979712
	2063088209104 [label="encoder.layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	2063088209104 -> 2067585979808
	2067585979808 [label=AccumulateGrad]
	2067585979424 -> 2067585979568
	2067585979424 [label=CudnnBatchNormBackward0]
	2067585980240 -> 2067585979424
	2067585980240 [label=ConvolutionBackward0]
	2062616519504 -> 2067585980240
	2067585980480 -> 2067585980240
	2063088206224 [label="encoder.layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2063088206224 -> 2067585980480
	2067585980480 [label=AccumulateGrad]
	2067585980000 -> 2067585979424
	2063088206064 [label="encoder.layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2063088206064 -> 2067585980000
	2067585980000 [label=AccumulateGrad]
	2067585979952 -> 2067585979424
	2063088215024 [label="encoder.layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2063088215024 -> 2067585979952
	2067585979952 [label=AccumulateGrad]
	2067585979472 -> 2067585979280
	2063088209744 [label="encoder.layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2063088209744 -> 2067585979472
	2067585979472 [label=AccumulateGrad]
	2067585979088 -> 2067585979136
	2063088210544 [label="encoder.layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2063088210544 -> 2067585979088
	2067585979088 [label=AccumulateGrad]
	2067585979328 -> 2067585979136
	2063088210384 [label="encoder.layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2063088210384 -> 2067585979328
	2067585979328 [label=AccumulateGrad]
	2067585979040 -> 2067585978800
	2063088211104 [label="encoder.layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2063088211104 -> 2067585979040
	2067585979040 [label=AccumulateGrad]
	2067585978608 -> 2067585978656
	2063088210624 [label="encoder.layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2063088210624 -> 2067585978608
	2067585978608 [label=AccumulateGrad]
	2067585978848 -> 2067585978656
	2063088211584 [label="encoder.layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2063088211584 -> 2067585978848
	2067585978848 [label=AccumulateGrad]
	2067585978560 -> 2067585978320
	2063088212704 [label="encoder.layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2063088212704 -> 2067585978560
	2067585978560 [label=AccumulateGrad]
	2067585978368 -> 2067585978176
	2063088211904 [label="encoder.layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	2063088211904 -> 2067585978368
	2067585978368 [label=AccumulateGrad]
	2067585978272 -> 2067585978176
	2063088211744 [label="encoder.layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	2063088211744 -> 2067585978272
	2067585978272 [label=AccumulateGrad]
	2067585977888 -> 2067585978032
	2067585977936 -> 2067585977744
	2063088214304 [label="encoder.layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2063088214304 -> 2067585977936
	2067585977936 [label=AccumulateGrad]
	2067585977552 -> 2067585977600
	2063088214224 [label="encoder.layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	2063088214224 -> 2067585977552
	2067585977552 [label=AccumulateGrad]
	2067585977792 -> 2067585977600
	2063088213744 [label="encoder.layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	2063088213744 -> 2067585977792
	2067585977792 [label=AccumulateGrad]
	2067585977504 -> 2067585977264
	2063088215184 [label="encoder.layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2063088215184 -> 2067585977504
	2067585977504 [label=AccumulateGrad]
	2067585977072 -> 2067585977120
	2063088214464 [label="encoder.layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	2063088214464 -> 2067585977072
	2067585977072 [label=AccumulateGrad]
	2067585977312 -> 2067585977120
	2063088215664 [label="encoder.layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	2063088215664 -> 2067585977312
	2067585977312 [label=AccumulateGrad]
	2067585977024 -> 2067585976784
	2063088207184 [label="encoder.layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2063088207184 -> 2067585977024
	2067585977024 [label=AccumulateGrad]
	2067585976832 -> 2067585976640
	2063088215824 [label="encoder.layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	2063088215824 -> 2067585976832
	2067585976832 [label=AccumulateGrad]
	2067585976736 -> 2067585976640
	2063088215744 [label="encoder.layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	2063088215744 -> 2067585976736
	2067585976736 [label=AccumulateGrad]
	2067585976352 -> 2067585976496
	2067585976400 -> 2067585976208
	2063088216224 [label="encoder.layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2063088216224 -> 2067585976400
	2067585976400 [label=AccumulateGrad]
	2067585976016 -> 2067585976064
	2063088216704 [label="encoder.layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	2063088216704 -> 2067585976016
	2067585976016 [label=AccumulateGrad]
	2067585976256 -> 2067585976064
	2063088216384 [label="encoder.layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	2063088216384 -> 2067585976256
	2067585976256 [label=AccumulateGrad]
	2067585975968 -> 2067585975728
	2063088216624 [label="encoder.layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2063088216624 -> 2067585975968
	2067585975968 [label=AccumulateGrad]
	2067585975536 -> 2067585975584
	2063088216784 [label="encoder.layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	2063088216784 -> 2067585975536
	2067585975536 [label=AccumulateGrad]
	2067585975776 -> 2067585975584
	2063088216864 [label="encoder.layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	2063088216864 -> 2067585975776
	2067585975776 [label=AccumulateGrad]
	2067585975488 -> 2067585975248
	2063088217904 [label="encoder.layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2063088217904 -> 2067585975488
	2067585975488 [label=AccumulateGrad]
	2067585975296 -> 2067585975104
	2063088217744 [label="encoder.layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	2063088217744 -> 2067585975296
	2067585975296 [label=AccumulateGrad]
	2067585975200 -> 2067585975104
	2063088217664 [label="encoder.layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	2063088217664 -> 2067585975200
	2067585975200 [label=AccumulateGrad]
	2067585974816 -> 2067585974960
	2067585974864 -> 2067585974672
	2063088218864 [label="encoder.layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	2063088218864 -> 2067585974864
	2067585974864 [label=AccumulateGrad]
	2067585974480 -> 2067585974528
	2063088218784 [label="encoder.layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2063088218784 -> 2067585974480
	2067585974480 [label=AccumulateGrad]
	2067585974720 -> 2067585974528
	2063088218624 [label="encoder.layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2063088218624 -> 2067585974720
	2067585974720 [label=AccumulateGrad]
	2067585974432 -> 2067585941360
	2063088219744 [label="encoder.layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2063088219744 -> 2067585974432
	2067585974432 [label=AccumulateGrad]
	2067585941168 -> 2067585941216
	2063088219904 [label="encoder.layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2063088219904 -> 2067585941168
	2067585941168 [label=AccumulateGrad]
	2067585941408 -> 2067585941216
	2063088219664 [label="encoder.layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2063088219664 -> 2067585941408
	2067585941408 [label=AccumulateGrad]
	2067585941120 -> 2067585940880
	2063088219984 [label="encoder.layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2063088219984 -> 2067585941120
	2067585941120 [label=AccumulateGrad]
	2067585940928 -> 2067585940736
	2063088220224 [label="encoder.layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	2063088220224 -> 2067585940928
	2067585940928 [label=AccumulateGrad]
	2067585940832 -> 2067585940736
	2063088220944 [label="encoder.layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	2063088220944 -> 2067585940832
	2067585940832 [label=AccumulateGrad]
	2067585940448 -> 2067585940592
	2067585940448 [label=CudnnBatchNormBackward0]
	2067585941456 -> 2067585940448
	2067585941456 [label=ConvolutionBackward0]
	2062616516000 -> 2067585941456
	2067585974336 -> 2067585941456
	2063088217984 [label="encoder.layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	2063088217984 -> 2067585974336
	2067585974336 [label=AccumulateGrad]
	2067585941024 -> 2067585940448
	2063088217824 [label="encoder.layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	2063088217824 -> 2067585941024
	2067585941024 [label=AccumulateGrad]
	2067585940976 -> 2067585940448
	2063088218704 [label="encoder.layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	2063088218704 -> 2067585940976
	2067585940976 [label=AccumulateGrad]
	2067585940496 -> 2067585940304
	2063088221584 [label="encoder.layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2063088221584 -> 2067585940496
	2067585940496 [label=AccumulateGrad]
	2067585940112 -> 2067585940160
	2063088221504 [label="encoder.layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2063088221504 -> 2067585940112
	2067585940112 [label=AccumulateGrad]
	2067585940352 -> 2067585940160
	2063088221264 [label="encoder.layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2063088221264 -> 2067585940352
	2067585940352 [label=AccumulateGrad]
	2067585940064 -> 2067585939824
	2063088221904 [label="encoder.layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2063088221904 -> 2067585940064
	2067585940064 [label=AccumulateGrad]
	2067585939632 -> 2067585939680
	2063088221664 [label="encoder.layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2063088221664 -> 2067585939632
	2067585939632 [label=AccumulateGrad]
	2067585939872 -> 2067585939680
	2063088207504 [label="encoder.layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2063088207504 -> 2067585939872
	2067585939872 [label=AccumulateGrad]
	2067585939584 -> 2067585939344
	2063088213104 [label="encoder.layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2063088213104 -> 2067585939584
	2067585939584 [label=AccumulateGrad]
	2067585939392 -> 2067585939200
	2063088213424 [label="encoder.layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	2063088213424 -> 2067585939392
	2067585939392 [label=AccumulateGrad]
	2067585939296 -> 2067585939200
	2063088211824 [label="encoder.layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	2063088211824 -> 2067585939296
	2067585939296 [label=AccumulateGrad]
	2067585938912 -> 2067585939056
	2067585938960 -> 2067585938768
	2063088210304 [label="encoder.layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2063088210304 -> 2067585938960
	2067585938960 [label=AccumulateGrad]
	2067585938576 -> 2067585938624
	2063088207264 [label="encoder.layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	2063088207264 -> 2067585938576
	2067585938576 [label=AccumulateGrad]
	2067585938816 -> 2067585938624
	2063088212384 [label="encoder.layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	2063088212384 -> 2067585938816
	2067585938816 [label=AccumulateGrad]
	2067585938528 -> 2067585938288
	2063088209584 [label="encoder.layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2063088209584 -> 2067585938528
	2067585938528 [label=AccumulateGrad]
	2067585938096 -> 2067585938144
	2063088209264 [label="encoder.layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	2063088209264 -> 2067585938096
	2067585938096 [label=AccumulateGrad]
	2067585938336 -> 2067585938144
	2063088210784 [label="encoder.layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	2063088210784 -> 2067585938336
	2067585938336 [label=AccumulateGrad]
	2067585938048 -> 2067585937808
	2063088212784 [label="encoder.layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2063088212784 -> 2067585938048
	2067585938048 [label=AccumulateGrad]
	2067585937856 -> 2067585937664
	2063088212944 [label="encoder.layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	2063088212944 -> 2067585937856
	2067585937856 [label=AccumulateGrad]
	2067585937760 -> 2067585937664
	2063088214384 [label="encoder.layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	2063088214384 -> 2067585937760
	2067585937760 [label=AccumulateGrad]
	2067585937376 -> 2067585937520
	2067585937424 -> 2067585937232
	2063088206944 [label="encoder.layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2063088206944 -> 2067585937424
	2067585937424 [label=AccumulateGrad]
	2067585937040 -> 2067585937088
	2063088210464 [label="encoder.layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	2063088210464 -> 2067585937040
	2067585937040 [label=AccumulateGrad]
	2067585937280 -> 2067585937088
	2063088211504 [label="encoder.layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	2063088211504 -> 2067585937280
	2067585937280 [label=AccumulateGrad]
	2067585936992 -> 2067585936752
	2063088123840 [label="encoder.layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2063088123840 -> 2067585936992
	2067585936992 [label=AccumulateGrad]
	2067585936560 -> 2067585936608
	2063088112960 [label="encoder.layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	2063088112960 -> 2067585936560
	2067585936560 [label=AccumulateGrad]
	2067585936800 -> 2067585936608
	2063088107600 [label="encoder.layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	2063088107600 -> 2067585936800
	2067585936800 [label=AccumulateGrad]
	2067585936512 -> 2067585936272
	2063088110000 [label="encoder.layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2063088110000 -> 2067585936512
	2067585936512 [label=AccumulateGrad]
	2067585936320 -> 2067585936128
	2063088120480 [label="encoder.layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	2063088120480 -> 2067585936320
	2067585936320 [label=AccumulateGrad]
	2067585936224 -> 2067585936128
	2063088113840 [label="encoder.layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	2063088113840 -> 2067585936224
	2067585936224 [label=AccumulateGrad]
	2067585935840 -> 2067585935984
	2067585935888 -> 2067585935696
	2063376927872 [label="encoder.layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2063376927872 -> 2067585935888
	2067585935888 [label=AccumulateGrad]
	2067585935504 -> 2067585935552
	2063376936592 [label="encoder.layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	2063376936592 -> 2067585935504
	2067585935504 [label=AccumulateGrad]
	2067585935744 -> 2067585935552
	2063376930192 [label="encoder.layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	2063376930192 -> 2067585935744
	2067585935744 [label=AccumulateGrad]
	2067585935456 -> 2067585935216
	2063376933472 [label="encoder.layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2063376933472 -> 2067585935456
	2067585935456 [label=AccumulateGrad]
	2067585935024 -> 2067585935072
	2063376927952 [label="encoder.layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	2063376927952 -> 2067585935024
	2067585935024 [label=AccumulateGrad]
	2067585935264 -> 2067585935072
	2063376937392 [label="encoder.layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	2063376937392 -> 2067585935264
	2067585935264 [label=AccumulateGrad]
	2067585934976 -> 2067585934736
	2063376930912 [label="encoder.layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2063376930912 -> 2067585934976
	2067585934976 [label=AccumulateGrad]
	2067585934784 -> 2067585934592
	2063376928752 [label="encoder.layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	2063376928752 -> 2067585934784
	2067585934784 [label=AccumulateGrad]
	2067585934688 -> 2067585934592
	2063376927792 [label="encoder.layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	2063376927792 -> 2067585934688
	2067585934688 [label=AccumulateGrad]
	2067585934304 -> 2067585934448
	2067585934352 -> 2067585934160
	2063376930832 [label="encoder.layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2063376930832 -> 2067585934352
	2067585934352 [label=AccumulateGrad]
	2067585933968 -> 2067585934016
	2063376936912 [label="encoder.layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	2063376936912 -> 2067585933968
	2067585933968 [label=AccumulateGrad]
	2067585934208 -> 2067585934016
	2063376931072 [label="encoder.layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	2063376931072 -> 2067585934208
	2067585934208 [label=AccumulateGrad]
	2067585933920 -> 2067585933680
	2063376939952 [label="encoder.layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2063376939952 -> 2067585933920
	2067585933920 [label=AccumulateGrad]
	2067585933488 -> 2067585933536
	2063376939472 [label="encoder.layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	2063376939472 -> 2067585933488
	2067585933488 [label=AccumulateGrad]
	2067585933728 -> 2067585933536
	2063376926352 [label="encoder.layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	2063376926352 -> 2067585933728
	2067585933728 [label=AccumulateGrad]
	2067585933440 -> 2067585933200
	2063376926752 [label="encoder.layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2063376926752 -> 2067585933440
	2067585933440 [label=AccumulateGrad]
	2067585933248 -> 2067585933056
	2063376930112 [label="encoder.layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	2063376930112 -> 2067585933248
	2067585933248 [label=AccumulateGrad]
	2067585933152 -> 2067585933056
	2063376927152 [label="encoder.layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	2063376927152 -> 2067585933152
	2067585933152 [label=AccumulateGrad]
	2067585932768 -> 2067585932912
	2067585932816 -> 2067585932624
	2063376932432 [label="encoder.layer3.6.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2063376932432 -> 2067585932816
	2067585932816 [label=AccumulateGrad]
	2067585932432 -> 2067585932480
	2063376938672 [label="encoder.layer3.6.bn1.weight
 (256)" fillcolor=lightblue]
	2063376938672 -> 2067585932432
	2067585932432 [label=AccumulateGrad]
	2067585932672 -> 2067585932480
	2063376930032 [label="encoder.layer3.6.bn1.bias
 (256)" fillcolor=lightblue]
	2063376930032 -> 2067585932672
	2067585932672 [label=AccumulateGrad]
	2067585932384 -> 2067585932144
	2063376935552 [label="encoder.layer3.6.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2063376935552 -> 2067585932384
	2067585932384 [label=AccumulateGrad]
	2067585931952 -> 2067585932000
	2063376932512 [label="encoder.layer3.6.bn2.weight
 (256)" fillcolor=lightblue]
	2063376932512 -> 2067585931952
	2067585931952 [label=AccumulateGrad]
	2067585932192 -> 2067585932000
	2063376926112 [label="encoder.layer3.6.bn2.bias
 (256)" fillcolor=lightblue]
	2063376926112 -> 2067585932192
	2067585932192 [label=AccumulateGrad]
	2067585931904 -> 2067585931664
	2063376938032 [label="encoder.layer3.6.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2063376938032 -> 2067585931904
	2067585931904 [label=AccumulateGrad]
	2067585931712 -> 2067585931520
	2063376929872 [label="encoder.layer3.6.bn3.weight
 (1024)" fillcolor=lightblue]
	2063376929872 -> 2067585931712
	2067585931712 [label=AccumulateGrad]
	2067585931616 -> 2067585931520
	2063376925792 [label="encoder.layer3.6.bn3.bias
 (1024)" fillcolor=lightblue]
	2063376925792 -> 2067585931616
	2067585931616 [label=AccumulateGrad]
	2067585931232 -> 2067585931376
	2067585931280 -> 2067585931088
	2063376937472 [label="encoder.layer3.7.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2063376937472 -> 2067585931280
	2067585931280 [label=AccumulateGrad]
	2067585930896 -> 2067585930944
	2063376933792 [label="encoder.layer3.7.bn1.weight
 (256)" fillcolor=lightblue]
	2063376933792 -> 2067585930896
	2067585930896 [label=AccumulateGrad]
	2067585931136 -> 2067585930944
	2063376929072 [label="encoder.layer3.7.bn1.bias
 (256)" fillcolor=lightblue]
	2063376929072 -> 2067585931136
	2067585931136 [label=AccumulateGrad]
	2067585930848 -> 2067585930608
	2063376932672 [label="encoder.layer3.7.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2063376932672 -> 2067585930848
	2067585930848 [label=AccumulateGrad]
	2067585930416 -> 2067585930464
	2063376927072 [label="encoder.layer3.7.bn2.weight
 (256)" fillcolor=lightblue]
	2063376927072 -> 2067585930416
	2067585930416 [label=AccumulateGrad]
	2067585930656 -> 2067585930464
	2063376938112 [label="encoder.layer3.7.bn2.bias
 (256)" fillcolor=lightblue]
	2063376938112 -> 2067585930656
	2067585930656 [label=AccumulateGrad]
	2067585930368 -> 2067585930128
	2063376927632 [label="encoder.layer3.7.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2063376927632 -> 2067585930368
	2067585930368 [label=AccumulateGrad]
	2067585930176 -> 2067585929984
	2063376933552 [label="encoder.layer3.7.bn3.weight
 (1024)" fillcolor=lightblue]
	2063376933552 -> 2067585930176
	2067585930176 [label=AccumulateGrad]
	2067585930080 -> 2067585929984
	2063376931952 [label="encoder.layer3.7.bn3.bias
 (1024)" fillcolor=lightblue]
	2063376931952 -> 2067585930080
	2067585930080 [label=AccumulateGrad]
	2067585929696 -> 2067585929840
	2067585929744 -> 2067585929552
	2063376925632 [label="encoder.layer3.8.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2063376925632 -> 2067585929744
	2067585929744 [label=AccumulateGrad]
	2067585929360 -> 2067585929408
	2063376932912 [label="encoder.layer3.8.bn1.weight
 (256)" fillcolor=lightblue]
	2063376932912 -> 2067585929360
	2067585929360 [label=AccumulateGrad]
	2067585929600 -> 2067585929408
	2063376939152 [label="encoder.layer3.8.bn1.bias
 (256)" fillcolor=lightblue]
	2063376939152 -> 2067585929600
	2067585929600 [label=AccumulateGrad]
	2067585929312 -> 2067585929072
	2063376932192 [label="encoder.layer3.8.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2063376932192 -> 2067585929312
	2067585929312 [label=AccumulateGrad]
	2067585928880 -> 2067585928928
	2063376935072 [label="encoder.layer3.8.bn2.weight
 (256)" fillcolor=lightblue]
	2063376935072 -> 2067585928880
	2067585928880 [label=AccumulateGrad]
	2067585929120 -> 2067585928928
	2063376926032 [label="encoder.layer3.8.bn2.bias
 (256)" fillcolor=lightblue]
	2063376926032 -> 2067585929120
	2067585929120 [label=AccumulateGrad]
	2067585928832 -> 2067585928592
	2063376928832 [label="encoder.layer3.8.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2063376928832 -> 2067585928832
	2067585928832 [label=AccumulateGrad]
	2067585928640 -> 2067585928448
	2063376929312 [label="encoder.layer3.8.bn3.weight
 (1024)" fillcolor=lightblue]
	2063376929312 -> 2067585928640
	2067585928640 [label=AccumulateGrad]
	2067585928544 -> 2067585928448
	2063376935712 [label="encoder.layer3.8.bn3.bias
 (1024)" fillcolor=lightblue]
	2063376935712 -> 2067585928544
	2067585928544 [label=AccumulateGrad]
	2067585928160 -> 2067585928304
	2067585928208 -> 2067585928016
	2063376925152 [label="encoder.layer3.9.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2063376925152 -> 2067585928208
	2067585928208 [label=AccumulateGrad]
	2067585927824 -> 2067585927872
	2063376932272 [label="encoder.layer3.9.bn1.weight
 (256)" fillcolor=lightblue]
	2063376932272 -> 2067585927824
	2067585927824 [label=AccumulateGrad]
	2067585928064 -> 2067585927872
	2063376925312 [label="encoder.layer3.9.bn1.bias
 (256)" fillcolor=lightblue]
	2063376925312 -> 2067585928064
	2067585928064 [label=AccumulateGrad]
	2067585927776 -> 2067585927536
	2063376930352 [label="encoder.layer3.9.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2063376930352 -> 2067585927776
	2067585927776 [label=AccumulateGrad]
	2067585927344 -> 2067585927392
	2063376928192 [label="encoder.layer3.9.bn2.weight
 (256)" fillcolor=lightblue]
	2063376928192 -> 2067585927344
	2067585927344 [label=AccumulateGrad]
	2067585927584 -> 2067585927392
	2063376937632 [label="encoder.layer3.9.bn2.bias
 (256)" fillcolor=lightblue]
	2063376937632 -> 2067585927584
	2067585927584 [label=AccumulateGrad]
	2067585927296 -> 2067585927056
	2063376936992 [label="encoder.layer3.9.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2063376936992 -> 2067585927296
	2067585927296 [label=AccumulateGrad]
	2067585927104 -> 2067585926912
	2063376930672 [label="encoder.layer3.9.bn3.weight
 (1024)" fillcolor=lightblue]
	2063376930672 -> 2067585927104
	2067585927104 [label=AccumulateGrad]
	2067585927008 -> 2067585926912
	2063376928432 [label="encoder.layer3.9.bn3.bias
 (1024)" fillcolor=lightblue]
	2063376928432 -> 2067585927008
	2067585927008 [label=AccumulateGrad]
	2067585926624 -> 2067585926768
	2067585926672 -> 2067585926480
	2063376937232 [label="encoder.layer3.10.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2063376937232 -> 2067585926672
	2067585926672 [label=AccumulateGrad]
	2067585926288 -> 2067585926336
	2063376936512 [label="encoder.layer3.10.bn1.weight
 (256)" fillcolor=lightblue]
	2063376936512 -> 2067585926288
	2067585926288 [label=AccumulateGrad]
	2067585926528 -> 2067585926336
	2063376932112 [label="encoder.layer3.10.bn1.bias
 (256)" fillcolor=lightblue]
	2063376932112 -> 2067585926528
	2067585926528 [label=AccumulateGrad]
	2067585926240 -> 2067585926000
	2063376936112 [label="encoder.layer3.10.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2063376936112 -> 2067585926240
	2067585926240 [label=AccumulateGrad]
	2067585925808 -> 2067585925856
	2063376932832 [label="encoder.layer3.10.bn2.weight
 (256)" fillcolor=lightblue]
	2063376932832 -> 2067585925808
	2067585925808 [label=AccumulateGrad]
	2067585926048 -> 2067585925856
	2063376936192 [label="encoder.layer3.10.bn2.bias
 (256)" fillcolor=lightblue]
	2063376936192 -> 2067585926048
	2067585926048 [label=AccumulateGrad]
	2067585925760 -> 2067585925520
	2063376927552 [label="encoder.layer3.10.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2063376927552 -> 2067585925760
	2067585925760 [label=AccumulateGrad]
	2067585925568 -> 2067585925376
	2063376929712 [label="encoder.layer3.10.bn3.weight
 (1024)" fillcolor=lightblue]
	2063376929712 -> 2067585925568
	2067585925568 [label=AccumulateGrad]
	2067585925472 -> 2067585925376
	2063376940032 [label="encoder.layer3.10.bn3.bias
 (1024)" fillcolor=lightblue]
	2063376940032 -> 2067585925472
	2067585925472 [label=AccumulateGrad]
	2067585925184 -> 2067585925232
	2062668234656 -> 2062668234512
	2063376930512 [label="encoder.layer3.11.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2063376930512 -> 2062668234656
	2062668234656 [label=AccumulateGrad]
	2062668234320 -> 2062668234368
	2063376939232 [label="encoder.layer3.11.bn1.weight
 (256)" fillcolor=lightblue]
	2063376939232 -> 2062668234320
	2062668234320 [label=AccumulateGrad]
	2062668234560 -> 2062668234368
	2063376927232 [label="encoder.layer3.11.bn1.bias
 (256)" fillcolor=lightblue]
	2063376927232 -> 2062668234560
	2062668234560 [label=AccumulateGrad]
	2062668234272 -> 2062668234032
	2063376926512 [label="encoder.layer3.11.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2063376926512 -> 2062668234272
	2062668234272 [label=AccumulateGrad]
	2062668233840 -> 2062668233888
	2063376937552 [label="encoder.layer3.11.bn2.weight
 (256)" fillcolor=lightblue]
	2063376937552 -> 2062668233840
	2062668233840 [label=AccumulateGrad]
	2062668234080 -> 2062668233888
	2063376930432 [label="encoder.layer3.11.bn2.bias
 (256)" fillcolor=lightblue]
	2063376930432 -> 2062668234080
	2062668234080 [label=AccumulateGrad]
	2062668233792 -> 2062668233552
	2063376890320 [label="encoder.layer3.11.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2063376890320 -> 2062668233792
	2062668233792 [label=AccumulateGrad]
	2062668233600 -> 2062668233408
	2063376888080 [label="encoder.layer3.11.bn3.weight
 (1024)" fillcolor=lightblue]
	2063376888080 -> 2062668233600
	2062668233600 [label=AccumulateGrad]
	2062668233504 -> 2062668233408
	2063376887840 [label="encoder.layer3.11.bn3.bias
 (1024)" fillcolor=lightblue]
	2063376887840 -> 2062668233504
	2062668233504 [label=AccumulateGrad]
	2062668233120 -> 2062668233264
	2062668233168 -> 2062668232976
	2063376884800 [label="encoder.layer3.12.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2063376884800 -> 2062668233168
	2062668233168 [label=AccumulateGrad]
	2062668232784 -> 2062668232832
	2063376884240 [label="encoder.layer3.12.bn1.weight
 (256)" fillcolor=lightblue]
	2063376884240 -> 2062668232784
	2062668232784 [label=AccumulateGrad]
	2062668233024 -> 2062668232832
	2063376884960 [label="encoder.layer3.12.bn1.bias
 (256)" fillcolor=lightblue]
	2063376884960 -> 2062668233024
	2062668233024 [label=AccumulateGrad]
	2062668232736 -> 2062668232496
	2063376889600 [label="encoder.layer3.12.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2063376889600 -> 2062668232736
	2062668232736 [label=AccumulateGrad]
	2062668232304 -> 2062668232352
	2063376886960 [label="encoder.layer3.12.bn2.weight
 (256)" fillcolor=lightblue]
	2063376886960 -> 2062668232304
	2062668232304 [label=AccumulateGrad]
	2062668232544 -> 2062668232352
	2063376888480 [label="encoder.layer3.12.bn2.bias
 (256)" fillcolor=lightblue]
	2063376888480 -> 2062668232544
	2062668232544 [label=AccumulateGrad]
	2062668232256 -> 2062668232016
	2063376886720 [label="encoder.layer3.12.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2063376886720 -> 2062668232256
	2062668232256 [label=AccumulateGrad]
	2062668232064 -> 2062668231872
	2063376888720 [label="encoder.layer3.12.bn3.weight
 (1024)" fillcolor=lightblue]
	2063376888720 -> 2062668232064
	2062668232064 [label=AccumulateGrad]
	2062668231968 -> 2062668231872
	2063376886080 [label="encoder.layer3.12.bn3.bias
 (1024)" fillcolor=lightblue]
	2063376886080 -> 2062668231968
	2062668231968 [label=AccumulateGrad]
	2062668231584 -> 2062668231728
	2062668231632 -> 2062668231440
	2063376890560 [label="encoder.layer3.13.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2063376890560 -> 2062668231632
	2062668231632 [label=AccumulateGrad]
	2062668231248 -> 2062668231296
	2063376891360 [label="encoder.layer3.13.bn1.weight
 (256)" fillcolor=lightblue]
	2063376891360 -> 2062668231248
	2062668231248 [label=AccumulateGrad]
	2062668231488 -> 2062668231296
	2063376886880 [label="encoder.layer3.13.bn1.bias
 (256)" fillcolor=lightblue]
	2063376886880 -> 2062668231488
	2062668231488 [label=AccumulateGrad]
	2062668231200 -> 2062668230960
	2063376889360 [label="encoder.layer3.13.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2063376889360 -> 2062668231200
	2062668231200 [label=AccumulateGrad]
	2062668230768 -> 2062668230816
	2063376886800 [label="encoder.layer3.13.bn2.weight
 (256)" fillcolor=lightblue]
	2063376886800 -> 2062668230768
	2062668230768 [label=AccumulateGrad]
	2062668231008 -> 2062668230816
	2063376883520 [label="encoder.layer3.13.bn2.bias
 (256)" fillcolor=lightblue]
	2063376883520 -> 2062668231008
	2062668231008 [label=AccumulateGrad]
	2062668230720 -> 2062668230480
	2063376884080 [label="encoder.layer3.13.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2063376884080 -> 2062668230720
	2062668230720 [label=AccumulateGrad]
	2062668230528 -> 2062668230336
	2063376891200 [label="encoder.layer3.13.bn3.weight
 (1024)" fillcolor=lightblue]
	2063376891200 -> 2062668230528
	2062668230528 [label=AccumulateGrad]
	2062668230432 -> 2062668230336
	2063376883840 [label="encoder.layer3.13.bn3.bias
 (1024)" fillcolor=lightblue]
	2063376883840 -> 2062668230432
	2062668230432 [label=AccumulateGrad]
	2062668230048 -> 2062668230192
	2062668230096 -> 2062668229904
	2063376883760 [label="encoder.layer3.14.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2063376883760 -> 2062668230096
	2062668230096 [label=AccumulateGrad]
	2062668229712 -> 2062668229760
	2063376883680 [label="encoder.layer3.14.bn1.weight
 (256)" fillcolor=lightblue]
	2063376883680 -> 2062668229712
	2062668229712 [label=AccumulateGrad]
	2062668229952 -> 2062668229760
	2063376889280 [label="encoder.layer3.14.bn1.bias
 (256)" fillcolor=lightblue]
	2063376889280 -> 2062668229952
	2062668229952 [label=AccumulateGrad]
	2062668229664 -> 2062668229424
	2063376891760 [label="encoder.layer3.14.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2063376891760 -> 2062668229664
	2062668229664 [label=AccumulateGrad]
	2062668229232 -> 2062668229280
	2063376885920 [label="encoder.layer3.14.bn2.weight
 (256)" fillcolor=lightblue]
	2063376885920 -> 2062668229232
	2062668229232 [label=AccumulateGrad]
	2062668229472 -> 2062668229280
	2063376890960 [label="encoder.layer3.14.bn2.bias
 (256)" fillcolor=lightblue]
	2063376890960 -> 2062668229472
	2062668229472 [label=AccumulateGrad]
	2062668229184 -> 2062668228944
	2063376889520 [label="encoder.layer3.14.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2063376889520 -> 2062668229184
	2062668229184 [label=AccumulateGrad]
	2062668228992 -> 2062668228800
	2063376891280 [label="encoder.layer3.14.bn3.weight
 (1024)" fillcolor=lightblue]
	2063376891280 -> 2062668228992
	2062668228992 [label=AccumulateGrad]
	2062668228896 -> 2062668228800
	2063376882960 [label="encoder.layer3.14.bn3.bias
 (1024)" fillcolor=lightblue]
	2063376882960 -> 2062668228896
	2062668228896 [label=AccumulateGrad]
	2062668228512 -> 2062668228656
	2062668228560 -> 2062668228368
	2063376891440 [label="encoder.layer3.15.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2063376891440 -> 2062668228560
	2062668228560 [label=AccumulateGrad]
	2062668228176 -> 2062668228224
	2063376888640 [label="encoder.layer3.15.bn1.weight
 (256)" fillcolor=lightblue]
	2063376888640 -> 2062668228176
	2062668228176 [label=AccumulateGrad]
	2062668228416 -> 2062668228224
	2063376887440 [label="encoder.layer3.15.bn1.bias
 (256)" fillcolor=lightblue]
	2063376887440 -> 2062668228416
	2062668228416 [label=AccumulateGrad]
	2062668228128 -> 2062668227888
	2063376886480 [label="encoder.layer3.15.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2063376886480 -> 2062668228128
	2062668228128 [label=AccumulateGrad]
	2062668227696 -> 2062668227744
	2063376890400 [label="encoder.layer3.15.bn2.weight
 (256)" fillcolor=lightblue]
	2063376890400 -> 2062668227696
	2062668227696 [label=AccumulateGrad]
	2062668227936 -> 2062668227744
	2063376891600 [label="encoder.layer3.15.bn2.bias
 (256)" fillcolor=lightblue]
	2063376891600 -> 2062668227936
	2062668227936 [label=AccumulateGrad]
	2062668227648 -> 2062668227408
	2063376890800 [label="encoder.layer3.15.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2063376890800 -> 2062668227648
	2062668227648 [label=AccumulateGrad]
	2062668227456 -> 2062668227264
	2063376890160 [label="encoder.layer3.15.bn3.weight
 (1024)" fillcolor=lightblue]
	2063376890160 -> 2062668227456
	2062668227456 [label=AccumulateGrad]
	2062668227360 -> 2062668227264
	2063376889120 [label="encoder.layer3.15.bn3.bias
 (1024)" fillcolor=lightblue]
	2063376889120 -> 2062668227360
	2062668227360 [label=AccumulateGrad]
	2062668226784 -> 2062668227120
	2062668227072 -> 2062616596336
	2063376883440 [label="encoder.layer3.16.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2063376883440 -> 2062668227072
	2062668227072 [label=AccumulateGrad]
	2062616595856 -> 2062616595904
	2063376885680 [label="encoder.layer3.16.bn1.weight
 (256)" fillcolor=lightblue]
	2063376885680 -> 2062616595856
	2062616595856 [label=AccumulateGrad]
	2062616595184 -> 2062616595904
	2063376883920 [label="encoder.layer3.16.bn1.bias
 (256)" fillcolor=lightblue]
	2063376883920 -> 2062616595184
	2062616595184 [label=AccumulateGrad]
	2062616596000 -> 2062616592688
	2063376833168 [label="encoder.layer3.16.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2063376833168 -> 2062616596000
	2062616596000 [label=AccumulateGrad]
	2062616593168 -> 2062616592976
	2063376884400 [label="encoder.layer3.16.bn2.weight
 (256)" fillcolor=lightblue]
	2063376884400 -> 2062616593168
	2062616593168 [label=AccumulateGrad]
	2062616592496 -> 2062616592976
	2063376834128 [label="encoder.layer3.16.bn2.bias
 (256)" fillcolor=lightblue]
	2063376834128 -> 2062616592496
	2062616592496 [label=AccumulateGrad]
	2062616593024 -> 2062616593696
	2063087748592 [label="encoder.layer3.16.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2063087748592 -> 2062616593024
	2062616593024 [label=AccumulateGrad]
	2062616593360 -> 2062616593648
	2063087761792 [label="encoder.layer3.16.bn3.weight
 (1024)" fillcolor=lightblue]
	2063087761792 -> 2062616593360
	2062616593360 [label=AccumulateGrad]
	2062616593600 -> 2062616593648
	2063087751792 [label="encoder.layer3.16.bn3.bias
 (1024)" fillcolor=lightblue]
	2063087751792 -> 2062616593600
	2062616593600 [label=AccumulateGrad]
	2062616594320 -> 2062616593936
	2062616594128 -> 2062616594272
	2062841486368 [label="encoder.layer3.17.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2062841486368 -> 2062616594128
	2062616594128 [label=AccumulateGrad]
	2062616594848 -> 2062616594656
	2062841480288 [label="encoder.layer3.17.bn1.weight
 (256)" fillcolor=lightblue]
	2062841480288 -> 2062616594848
	2062616594848 [label=AccumulateGrad]
	2062616594080 -> 2062616594656
	2062841480128 [label="encoder.layer3.17.bn1.bias
 (256)" fillcolor=lightblue]
	2062841480128 -> 2062616594080
	2062616594080 [label=AccumulateGrad]
	2062616594704 -> 2062616595376
	2062841483088 [label="encoder.layer3.17.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2062841483088 -> 2062616594704
	2062616594704 [label=AccumulateGrad]
	2062616595232 -> 2062616595328
	2062841482688 [label="encoder.layer3.17.bn2.weight
 (256)" fillcolor=lightblue]
	2062841482688 -> 2062616595232
	2062616595232 [label=AccumulateGrad]
	2062616595040 -> 2062616595328
	2062841481248 [label="encoder.layer3.17.bn2.bias
 (256)" fillcolor=lightblue]
	2062841481248 -> 2062616595040
	2062616595040 [label=AccumulateGrad]
	2062616595424 -> 2062616592640
	2062841493808 [label="encoder.layer3.17.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2062841493808 -> 2062616595424
	2062616595424 [label=AccumulateGrad]
	2062616592784 -> 2062616593408
	2062841493168 [label="encoder.layer3.17.bn3.weight
 (1024)" fillcolor=lightblue]
	2062841493168 -> 2062616592784
	2062616592784 [label=AccumulateGrad]
	2062616592592 -> 2062616593408
	2062841481328 [label="encoder.layer3.17.bn3.bias
 (1024)" fillcolor=lightblue]
	2062841481328 -> 2062616592592
	2062616592592 [label=AccumulateGrad]
	2062616593840 -> 2062616594032
	2062616593888 -> 2062616594416
	2063087914272 [label="encoder.layer3.18.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2063087914272 -> 2062616593888
	2062616593888 [label=AccumulateGrad]
	2062616594944 -> 2062616595088
	2063087912272 [label="encoder.layer3.18.bn1.weight
 (256)" fillcolor=lightblue]
	2063087912272 -> 2062616594944
	2062616594944 [label=AccumulateGrad]
	2062616594512 -> 2062616595088
	2063087915552 [label="encoder.layer3.18.bn1.bias
 (256)" fillcolor=lightblue]
	2063087915552 -> 2062616594512
	2062616594512 [label=AccumulateGrad]
	2062616594896 -> 2062616606800
	2063087927232 [label="encoder.layer3.18.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2063087927232 -> 2062616594896
	2062616594896 [label=AccumulateGrad]
	2062616606656 -> 2062616514416
	2063087923072 [label="encoder.layer3.18.bn2.weight
 (256)" fillcolor=lightblue]
	2063087923072 -> 2062616606656
	2062616606656 [label=AccumulateGrad]
	2062616595712 -> 2062616514416
	2063087925792 [label="encoder.layer3.18.bn2.bias
 (256)" fillcolor=lightblue]
	2063087925792 -> 2062616595712
	2062616595712 [label=AccumulateGrad]
	2062616524256 -> 2062616510528
	2063087920352 [label="encoder.layer3.18.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2063087920352 -> 2062616524256
	2062616524256 [label=AccumulateGrad]
	2062616510624 -> 2062616510672
	2063087925392 [label="encoder.layer3.18.bn3.weight
 (1024)" fillcolor=lightblue]
	2063087925392 -> 2062616510624
	2062616510624 [label=AccumulateGrad]
	2062616510720 -> 2062616510672
	2063087921472 [label="encoder.layer3.18.bn3.bias
 (1024)" fillcolor=lightblue]
	2063087921472 -> 2062616510720
	2062616510720 [label=AccumulateGrad]
	2062616511104 -> 2062616511152
	2062616511248 -> 2062616511680
	2063087924032 [label="encoder.layer3.19.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2063087924032 -> 2062616511248
	2062616511248 [label=AccumulateGrad]
	2062616511536 -> 2062616511632
	2063087920592 [label="encoder.layer3.19.bn1.weight
 (256)" fillcolor=lightblue]
	2063087920592 -> 2062616511536
	2062616511536 [label=AccumulateGrad]
	2062616511392 -> 2062616511632
	2063087924672 [label="encoder.layer3.19.bn1.bias
 (256)" fillcolor=lightblue]
	2063087924672 -> 2062616511392
	2062616511392 [label=AccumulateGrad]
	2062616511728 -> 2062616512208
	2063087926432 [label="encoder.layer3.19.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2063087926432 -> 2062616511728
	2062616511728 [label=AccumulateGrad]
	2062616512544 -> 2062616512496
	2063087924752 [label="encoder.layer3.19.bn2.weight
 (256)" fillcolor=lightblue]
	2063087924752 -> 2062616512544
	2062616512544 [label=AccumulateGrad]
	2062616512352 -> 2062616512496
	2063087922992 [label="encoder.layer3.19.bn2.bias
 (256)" fillcolor=lightblue]
	2063087922992 -> 2062616512352
	2062616512352 [label=AccumulateGrad]
	2062616512832 -> 2062616512880
	2063087926672 [label="encoder.layer3.19.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2063087926672 -> 2062616512832
	2062616512832 [label=AccumulateGrad]
	2062616512688 -> 2062616513408
	2063087921072 [label="encoder.layer3.19.bn3.weight
 (1024)" fillcolor=lightblue]
	2063087921072 -> 2062616512688
	2062616512688 [label=AccumulateGrad]
	2062616513072 -> 2062616513408
	2063087921232 [label="encoder.layer3.19.bn3.bias
 (1024)" fillcolor=lightblue]
	2063087921232 -> 2062616513072
	2062616513072 [label=AccumulateGrad]
	2062616513648 -> 2062616513360
	2062616513456 -> 2062616513888
	2063087921632 [label="encoder.layer3.20.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2063087921632 -> 2062616513456
	2062616513456 [label=AccumulateGrad]
	2062616514032 -> 2062616513840
	2063087922352 [label="encoder.layer3.20.bn1.weight
 (256)" fillcolor=lightblue]
	2063087922352 -> 2062616514032
	2062616514032 [label=AccumulateGrad]
	2062616513984 -> 2062616513840
	2063087920832 [label="encoder.layer3.20.bn1.bias
 (256)" fillcolor=lightblue]
	2063087920832 -> 2062616513984
	2062616513984 [label=AccumulateGrad]
	2062616514224 -> 2062616514560
	2063087922432 [label="encoder.layer3.20.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2063087922432 -> 2062616514224
	2062616514224 [label=AccumulateGrad]
	2062616514896 -> 2062616514848
	2063087925472 [label="encoder.layer3.20.bn2.weight
 (256)" fillcolor=lightblue]
	2063087925472 -> 2062616514896
	2062616514896 [label=AccumulateGrad]
	2062616514704 -> 2062616514848
	2063087926272 [label="encoder.layer3.20.bn2.bias
 (256)" fillcolor=lightblue]
	2063087926272 -> 2062616514704
	2062616514704 [label=AccumulateGrad]
	2062616515232 -> 2062616515280
	2063087926352 [label="encoder.layer3.20.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2063087926352 -> 2062616515232
	2062616515232 [label=AccumulateGrad]
	2062616515088 -> 2062616515712
	2063087913552 [label="encoder.layer3.20.bn3.weight
 (1024)" fillcolor=lightblue]
	2063087913552 -> 2062616515088
	2062616515088 [label=AccumulateGrad]
	2062616515472 -> 2062616515712
	2063087925952 [label="encoder.layer3.20.bn3.bias
 (1024)" fillcolor=lightblue]
	2063087925952 -> 2062616515472
	2062616515472 [label=AccumulateGrad]
	2062616516144 -> 2062616515760
	2062616516240 -> 2062616516096
	2063087922832 [label="encoder.layer3.21.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2063087922832 -> 2062616516240
	2062616516240 [label=AccumulateGrad]
	2062616516864 -> 2062616516528
	2063087924352 [label="encoder.layer3.21.bn1.weight
 (256)" fillcolor=lightblue]
	2063087924352 -> 2062616516864
	2062616516864 [label=AccumulateGrad]
	2062616516192 -> 2062616516528
	2063087919952 [label="encoder.layer3.21.bn1.bias
 (256)" fillcolor=lightblue]
	2063087919952 -> 2062616516192
	2062616516192 [label=AccumulateGrad]
	2062616516768 -> 2062616517104
	2063087923152 [label="encoder.layer3.21.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2063087923152 -> 2062616516768
	2062616516768 [label=AccumulateGrad]
	2062616517392 -> 2062616517536
	2063087925152 [label="encoder.layer3.21.bn2.weight
 (256)" fillcolor=lightblue]
	2063087925152 -> 2062616517392
	2062616517392 [label=AccumulateGrad]
	2062616516912 -> 2062616517536
	2063087925072 [label="encoder.layer3.21.bn2.bias
 (256)" fillcolor=lightblue]
	2063087925072 -> 2062616516912
	2062616516912 [label=AccumulateGrad]
	2062616517296 -> 2062616518112
	2063087927072 [label="encoder.layer3.21.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2063087927072 -> 2062616517296
	2062616517296 [label=AccumulateGrad]
	2062616517920 -> 2062616518064
	2063087922752 [label="encoder.layer3.21.bn3.weight
 (1024)" fillcolor=lightblue]
	2063087922752 -> 2062616517920
	2062616517920 [label=AccumulateGrad]
	2062616517968 -> 2062616518064
	2063087920992 [label="encoder.layer3.21.bn3.bias
 (1024)" fillcolor=lightblue]
	2063087920992 -> 2062616517968
	2062616517968 [label=AccumulateGrad]
	2062616518448 -> 2062616518496
	2062616518544 -> 2062616519120
	2063087919872 [label="encoder.layer3.22.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2063087919872 -> 2062616518544
	2062616518544 [label=AccumulateGrad]
	2062616518976 -> 2062616519072
	2063087923392 [label="encoder.layer3.22.bn1.weight
 (256)" fillcolor=lightblue]
	2063087923392 -> 2062616518976
	2062616518976 [label=AccumulateGrad]
	2062616518832 -> 2062616519072
	2063087922192 [label="encoder.layer3.22.bn1.bias
 (256)" fillcolor=lightblue]
	2063087922192 -> 2062616518832
	2062616518832 [label=AccumulateGrad]
	2062616519168 -> 2062616519792
	2063087913872 [label="encoder.layer3.22.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2063087913872 -> 2062616519168
	2062616519168 [label=AccumulateGrad]
	2062616519936 -> 2062616519744
	2063087911712 [label="encoder.layer3.22.bn2.weight
 (256)" fillcolor=lightblue]
	2063087911712 -> 2062616519936
	2062616519936 [label=AccumulateGrad]
	2062616519600 -> 2062616519744
	2063087912992 [label="encoder.layer3.22.bn2.bias
 (256)" fillcolor=lightblue]
	2063087912992 -> 2062616519600
	2062616519600 [label=AccumulateGrad]
	2062616520224 -> 2062616520272
	2063087914192 [label="encoder.layer3.22.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2063087914192 -> 2062616520224
	2062616520224 [label=AccumulateGrad]
	2062616520080 -> 2062616520848
	2063087914832 [label="encoder.layer3.22.bn3.weight
 (1024)" fillcolor=lightblue]
	2063087914832 -> 2062616520080
	2062616520080 [label=AccumulateGrad]
	2062616520464 -> 2062616520848
	2063087915632 [label="encoder.layer3.22.bn3.bias
 (1024)" fillcolor=lightblue]
	2063087915632 -> 2062616520464
	2062616520464 [label=AccumulateGrad]
	2062616521376 -> 2062616520800
	2062616520896 -> 2062616521328
	2063087918592 [label="encoder.layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	2063087918592 -> 2062616520896
	2062616520896 [label=AccumulateGrad]
	2062616521664 -> 2062616521616
	2063087918192 [label="encoder.layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2063087918192 -> 2062616521664
	2062616521664 [label=AccumulateGrad]
	2062616521472 -> 2062616521616
	2063087918112 [label="encoder.layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2063087918112 -> 2062616521472
	2062616521472 [label=AccumulateGrad]
	2062616522000 -> 2062616522048
	2063087918752 [label="encoder.layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2063087918752 -> 2062616522000
	2062616522000 [label=AccumulateGrad]
	2062616521856 -> 2062616522288
	2063087918912 [label="encoder.layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2063087918912 -> 2062616521856
	2062616521856 [label=AccumulateGrad]
	2062616522624 -> 2062616522288
	2063087919232 [label="encoder.layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2063087919232 -> 2062616522624
	2062616522624 [label=AccumulateGrad]
	2062616522528 -> 2062616522864
	2062841279696 [label="encoder.layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2062841279696 -> 2062616522528
	2062616522528 [label=AccumulateGrad]
	2062616522672 -> 2062616523296
	2062841282336 [label="encoder.layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	2062841282336 -> 2062616522672
	2062616522672 [label=AccumulateGrad]
	2062616523200 -> 2062616523296
	2062841277296 [label="encoder.layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	2062841277296 -> 2062616523200
	2062616523200 [label=AccumulateGrad]
	2062616523104 -> 2062616523056
	2062616523104 [label=CudnnBatchNormBackward0]
	2062616522240 -> 2062616523104
	2062616522240 [label=ConvolutionBackward0]
	2062616512448 -> 2062616522240
	2062616522096 -> 2062616522240
	2063087913472 [label="encoder.layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	2063087913472 -> 2062616522096
	2062616522096 [label=AccumulateGrad]
	2062616522576 -> 2062616523104
	2063087913152 [label="encoder.layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	2063087913152 -> 2062616522576
	2062616522576 [label=AccumulateGrad]
	2062616522480 -> 2062616523104
	2063087913632 [label="encoder.layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	2063087913632 -> 2062616522480
	2062616522480 [label=AccumulateGrad]
	2062616523440 -> 2062616523920
	2062841174432 [label="encoder.layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2062841174432 -> 2062616523440
	2062616523440 [label=AccumulateGrad]
	2062616523728 -> 2062616523680
	2062841174112 [label="encoder.layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2062841174112 -> 2062616523728
	2062616523728 [label=AccumulateGrad]
	2062616523872 -> 2062616523680
	2062841174752 [label="encoder.layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2062841174752 -> 2062616523872
	2062616523872 [label=AccumulateGrad]
	2062616524064 -> 2062616524400
	2063088302704 [label="encoder.layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2063088302704 -> 2062616524064
	2062616524064 [label=AccumulateGrad]
	2062616524544 -> 2062616524496
	2063088303504 [label="encoder.layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2063088303504 -> 2062616524544
	2062616524544 [label=AccumulateGrad]
	2062616524688 -> 2062616524496
	2063088297744 [label="encoder.layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2063088297744 -> 2062616524688
	2062616524688 [label=AccumulateGrad]
	2062616524736 -> 2062616525024
	2063088222688 [label="encoder.layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2063088222688 -> 2062616524736
	2062616524736 [label=AccumulateGrad]
	2062616524928 -> 2062616525600
	2063088223488 [label="encoder.layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	2063088223488 -> 2062616524928
	2062616524928 [label=AccumulateGrad]
	2062616525216 -> 2062616525600
	2063088223328 [label="encoder.layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	2063088223328 -> 2062616525216
	2062616525216 [label=AccumulateGrad]
	2062616525264 -> 2062616525696
	2062616525456 -> 2062616526224
	2063088223648 [label="encoder.layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2063088223648 -> 2062616525456
	2062616525456 [label=AccumulateGrad]
	2062616525888 -> 2062616526320
	2063088223808 [label="encoder.layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	2063088223808 -> 2062616525888
	2062616525888 [label=AccumulateGrad]
	2062616526176 -> 2062616526320
	2063088224608 [label="encoder.layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	2063088224608 -> 2062616526176
	2062616526176 [label=AccumulateGrad]
	2062616526080 -> 2062616526800
	2063088224928 [label="encoder.layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2063088224928 -> 2062616526080
	2062616526080 [label=AccumulateGrad]
	2062616526752 -> 2062616522432
	2063088225088 [label="encoder.layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	2063088225088 -> 2062616526752
	2062616526752 [label=AccumulateGrad]
	2062616523008 -> 2062616522432
	2063088224768 [label="encoder.layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	2063088224768 -> 2062616523008
	2062616523008 [label=AccumulateGrad]
	2062616511008 -> 2062616511440
	2063088225648 [label="encoder.layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2063088225648 -> 2062616511008
	2062616511008 [label=AccumulateGrad]
	2062616511488 -> 2062616512016
	2063088226288 [label="encoder.layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	2063088226288 -> 2062616511488
	2062616511488 [label=AccumulateGrad]
	2062616511344 -> 2062616512016
	2063088227168 [label="encoder.layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	2063088227168 -> 2062616511344
	2062616511344 [label=AccumulateGrad]
	2062616511296 -> 2062616511872
	2062616512448 -> 2062616513024
	2062616513168 -> 2062616513600
	2063376888320 [label="decoder.blocks.0.conv1.0.weight
 (256, 3072, 3, 3)" fillcolor=lightblue]
	2063376888320 -> 2062616513168
	2062616513168 [label=AccumulateGrad]
	2062616513744 -> 2062616514368
	2062668183136 [label="decoder.blocks.0.conv1.1.weight
 (256)" fillcolor=lightblue]
	2062668183136 -> 2062616513744
	2062616513744 [label=AccumulateGrad]
	2062616514320 -> 2062616514368
	2062668173056 [label="decoder.blocks.0.conv1.1.bias
 (256)" fillcolor=lightblue]
	2062668173056 -> 2062616514320
	2062616514320 [label=AccumulateGrad]
	2062616514176 -> 2062616514752
	2063088228128 [label="decoder.blocks.0.conv2.0.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2063088228128 -> 2062616514176
	2062616514176 [label=AccumulateGrad]
	2062616514800 -> 2062616515568
	2063088227488 [label="decoder.blocks.0.conv2.1.weight
 (256)" fillcolor=lightblue]
	2063088227488 -> 2062616514800
	2062616514800 [label=AccumulateGrad]
	2062616516048 -> 2062616515568
	2062841262672 [label="decoder.blocks.0.conv2.1.bias
 (256)" fillcolor=lightblue]
	2062841262672 -> 2062616516048
	2062616516048 [label=AccumulateGrad]
	2062616516000 -> 2062616516576
	2062616516624 -> 2062616517152
	2062841265232 [label="decoder.blocks.1.conv1.0.weight
 (128, 768, 3, 3)" fillcolor=lightblue]
	2062841265232 -> 2062616516624
	2062616516624 [label=AccumulateGrad]
	2062616517200 -> 2062616517008
	2062841255152 [label="decoder.blocks.1.conv1.1.weight
 (128)" fillcolor=lightblue]
	2062841255152 -> 2062616517200
	2062616517200 [label=AccumulateGrad]
	2062616517776 -> 2062616517008
	2062841260272 [label="decoder.blocks.1.conv1.1.bias
 (128)" fillcolor=lightblue]
	2062841260272 -> 2062616517776
	2062616517776 [label=AccumulateGrad]
	2062616517728 -> 2062616518208
	2062668312128 [label="decoder.blocks.1.conv2.0.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2062668312128 -> 2062616517728
	2062616517728 [label=AccumulateGrad]
	2062616518304 -> 2062616518928
	2062668309808 [label="decoder.blocks.1.conv2.1.weight
 (128)" fillcolor=lightblue]
	2062668309808 -> 2062616518304
	2062616518304 [label=AccumulateGrad]
	2062616518736 -> 2062616518928
	2062668313328 [label="decoder.blocks.1.conv2.1.bias
 (128)" fillcolor=lightblue]
	2062668313328 -> 2062616518736
	2062616518736 [label=AccumulateGrad]
	2062616519504 -> 2062616520032
	2062616519264 -> 2062616520608
	2062668312048 [label="decoder.blocks.2.conv1.0.weight
 (64, 384, 3, 3)" fillcolor=lightblue]
	2062668312048 -> 2062616519264
	2062616519264 [label=AccumulateGrad]
	2062616519840 -> 2062616520416
	2062668313888 [label="decoder.blocks.2.conv1.1.weight
 (64)" fillcolor=lightblue]
	2062668313888 -> 2062616519840
	2062616519840 [label=AccumulateGrad]
	2062616520368 -> 2062616520416
	2062668313408 [label="decoder.blocks.2.conv1.1.bias
 (64)" fillcolor=lightblue]
	2062668313408 -> 2062616520368
	2062616520368 [label=AccumulateGrad]
	2062616521136 -> 2062616521712
	2062668313088 [label="decoder.blocks.2.conv2.0.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2062668313088 -> 2062616521136
	2062616521136 [label=AccumulateGrad]
	2062616521760 -> 2062616521520
	2062668314288 [label="decoder.blocks.2.conv2.1.weight
 (64)" fillcolor=lightblue]
	2062668314288 -> 2062616521760
	2062616521760 [label=AccumulateGrad]
	2062616522960 -> 2062616521520
	2062668309888 [label="decoder.blocks.2.conv2.1.bias
 (64)" fillcolor=lightblue]
	2062668309888 -> 2062616522960
	2062616522960 [label=AccumulateGrad]
	2062616522144 -> 2062616522816
	2062616522912 -> 2062616523392
	2062668312768 [label="decoder.blocks.3.conv1.0.weight
 (32, 128, 3, 3)" fillcolor=lightblue]
	2062668312768 -> 2062616522912
	2062616522912 [label=AccumulateGrad]
	2062616523536 -> 2062616524208
	2062668309728 [label="decoder.blocks.3.conv1.1.weight
 (32)" fillcolor=lightblue]
	2062668309728 -> 2062616523536
	2062616523536 [label=AccumulateGrad]
	2062616524160 -> 2062616524208
	2062668312608 [label="decoder.blocks.3.conv1.1.bias
 (32)" fillcolor=lightblue]
	2062668312608 -> 2062616524160
	2062616524160 [label=AccumulateGrad]
	2062616524016 -> 2062616524592
	2062668315888 [label="decoder.blocks.3.conv2.0.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	2062668315888 -> 2062616524016
	2062616524016 [label=AccumulateGrad]
	2062616524640 -> 2062616525312
	2062668316208 [label="decoder.blocks.3.conv2.1.weight
 (32)" fillcolor=lightblue]
	2062668316208 -> 2062616524640
	2062616524640 [label=AccumulateGrad]
	2062616525984 -> 2062616525312
	2062668314448 [label="decoder.blocks.3.conv2.1.bias
 (32)" fillcolor=lightblue]
	2062668314448 -> 2062616525984
	2062616525984 [label=AccumulateGrad]
	2062616525936 -> 2062839835968
	2062668315408 [label="decoder.blocks.4.conv1.0.weight
 (16, 32, 3, 3)" fillcolor=lightblue]
	2062668315408 -> 2062616525936
	2062616525936 [label=AccumulateGrad]
	2062616525744 -> 2062839832320
	2062668314368 [label="decoder.blocks.4.conv1.1.weight
 (16)" fillcolor=lightblue]
	2062668314368 -> 2062616525744
	2062616525744 [label=AccumulateGrad]
	2062616526560 -> 2062839832320
	2062668312288 [label="decoder.blocks.4.conv1.1.bias
 (16)" fillcolor=lightblue]
	2062668312288 -> 2062616526560
	2062616526560 [label=AccumulateGrad]
	2062841216352 -> 2062732664528
	2062668313168 [label="decoder.blocks.4.conv2.0.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
	2062668313168 -> 2062841216352
	2062841216352 [label=AccumulateGrad]
	2063088478976 -> 2062725331120
	2062668311648 [label="decoder.blocks.4.conv2.1.weight
 (16)" fillcolor=lightblue]
	2062668311648 -> 2063088478976
	2063088478976 [label=AccumulateGrad]
	2063088482096 -> 2062725331120
	2062668311328 [label="decoder.blocks.4.conv2.1.bias
 (16)" fillcolor=lightblue]
	2062668311328 -> 2063088482096
	2063088482096 [label=AccumulateGrad]
	2062839711920 -> 2062726860848
	2062668314928 [label="segmentation_head.0.weight
 (5, 16, 3, 3)" fillcolor=lightblue]
	2062668314928 -> 2062839711920
	2062839711920 [label=AccumulateGrad]
	2062839714608 -> 2062726860848
	2062668312928 [label="segmentation_head.0.bias
 (5)" fillcolor=lightblue]
	2062668312928 -> 2062839714608
	2062839714608 [label=AccumulateGrad]
	2062726860848 -> 2067552270240
}
