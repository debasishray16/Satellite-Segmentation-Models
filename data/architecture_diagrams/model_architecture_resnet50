digraph {
	graph [size="181.35,181.35"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2500373336128 [label="
 (1, 5, 256, 256)" fillcolor=darkolivegreen1]
	2500373307232 [label=ConvolutionBackward0]
	2496230378656 -> 2500373307232
	2496230378656 [label=ReluBackward0]
	2500373306752 -> 2496230378656
	2500373306752 [label=CudnnBatchNormBackward0]
	2500373307328 -> 2500373306752
	2500373307328 [label=ConvolutionBackward0]
	2500373307424 -> 2500373307328
	2500373307424 [label=ReluBackward0]
	2500373308048 -> 2500373307424
	2500373308048 [label=CudnnBatchNormBackward0]
	2500373307712 -> 2500373308048
	2500373307712 [label=ConvolutionBackward0]
	2500373308000 -> 2500373307712
	2500373308000 [label=UpsampleNearest2DBackward0]
	2500373307856 -> 2500373308000
	2500373307856 [label=ReluBackward0]
	2500373308096 -> 2500373307856
	2500373308096 [label=CudnnBatchNormBackward0]
	2500373308192 -> 2500373308096
	2500373308192 [label=ConvolutionBackward0]
	2500373308384 -> 2500373308192
	2500373308384 [label=ReluBackward0]
	2500373308528 -> 2500373308384
	2500373308528 [label=CudnnBatchNormBackward0]
	2500373308624 -> 2500373308528
	2500373308624 [label=ConvolutionBackward0]
	2500373308816 -> 2500373308624
	2500373308816 [label=CatBackward0]
	2500373308960 -> 2500373308816
	2500373308960 [label=UpsampleNearest2DBackward0]
	2500373309104 -> 2500373308960
	2500373309104 [label=ReluBackward0]
	2500373309200 -> 2500373309104
	2500373309200 [label=CudnnBatchNormBackward0]
	2500373309296 -> 2500373309200
	2500373309296 [label=ConvolutionBackward0]
	2500373309488 -> 2500373309296
	2500373309488 [label=ReluBackward0]
	2500373309632 -> 2500373309488
	2500373309632 [label=CudnnBatchNormBackward0]
	2500373309728 -> 2500373309632
	2500373309728 [label=ConvolutionBackward0]
	2500373309920 -> 2500373309728
	2500373309920 [label=CatBackward0]
	2500373310064 -> 2500373309920
	2500373310064 [label=UpsampleNearest2DBackward0]
	2500373310208 -> 2500373310064
	2500373310208 [label=ReluBackward0]
	2500373310304 -> 2500373310208
	2500373310304 [label=CudnnBatchNormBackward0]
	2500373310400 -> 2500373310304
	2500373310400 [label=ConvolutionBackward0]
	2500373310592 -> 2500373310400
	2500373310592 [label=ReluBackward0]
	2500373310736 -> 2500373310592
	2500373310736 [label=CudnnBatchNormBackward0]
	2500373310832 -> 2500373310736
	2500373310832 [label=ConvolutionBackward0]
	2500373311024 -> 2500373310832
	2500373311024 [label=CatBackward0]
	2500373311168 -> 2500373311024
	2500373311168 [label=UpsampleNearest2DBackward0]
	2500373311312 -> 2500373311168
	2500373311312 [label=ReluBackward0]
	2500373311408 -> 2500373311312
	2500373311408 [label=CudnnBatchNormBackward0]
	2500373311504 -> 2500373311408
	2500373311504 [label=ConvolutionBackward0]
	2500373311696 -> 2500373311504
	2500373311696 [label=ReluBackward0]
	2500373311840 -> 2500373311696
	2500373311840 [label=CudnnBatchNormBackward0]
	2500373311936 -> 2500373311840
	2500373311936 [label=ConvolutionBackward0]
	2500373312128 -> 2500373311936
	2500373312128 [label=CatBackward0]
	2500373312272 -> 2500373312128
	2500373312272 [label=UpsampleNearest2DBackward0]
	2500373312416 -> 2500373312272
	2500373312416 [label=ReluBackward0]
	2500373312512 -> 2500373312416
	2500373312512 [label=AddBackward0]
	2500373312608 -> 2500373312512
	2500373312608 [label=CudnnBatchNormBackward0]
	2500373312752 -> 2500373312608
	2500373312752 [label=ConvolutionBackward0]
	2500373312944 -> 2500373312752
	2500373312944 [label=ReluBackward0]
	2500373313088 -> 2500373312944
	2500373313088 [label=CudnnBatchNormBackward0]
	2500373313184 -> 2500373313088
	2500373313184 [label=ConvolutionBackward0]
	2500373313376 -> 2500373313184
	2500373313376 [label=ReluBackward0]
	2500373313520 -> 2500373313376
	2500373313520 [label=CudnnBatchNormBackward0]
	2500373313616 -> 2500373313520
	2500373313616 [label=ConvolutionBackward0]
	2500373312656 -> 2500373313616
	2500373312656 [label=ReluBackward0]
	2500373313904 -> 2500373312656
	2500373313904 [label=AddBackward0]
	2500373314000 -> 2500373313904
	2500373314000 [label=CudnnBatchNormBackward0]
	2500373314144 -> 2500373314000
	2500373314144 [label=ConvolutionBackward0]
	2500373314336 -> 2500373314144
	2500373314336 [label=ReluBackward0]
	2500373314480 -> 2500373314336
	2500373314480 [label=CudnnBatchNormBackward0]
	2500373314576 -> 2500373314480
	2500373314576 [label=ConvolutionBackward0]
	2500373314768 -> 2500373314576
	2500373314768 [label=ReluBackward0]
	2500373314912 -> 2500373314768
	2500373314912 [label=CudnnBatchNormBackward0]
	2500373315008 -> 2500373314912
	2500373315008 [label=ConvolutionBackward0]
	2500373314048 -> 2500373315008
	2500373314048 [label=ReluBackward0]
	2500373315296 -> 2500373314048
	2500373315296 [label=AddBackward0]
	2500373315392 -> 2500373315296
	2500373315392 [label=CudnnBatchNormBackward0]
	2500373315488 -> 2500373315392
	2500373315488 [label=ConvolutionBackward0]
	2500373381328 -> 2500373315488
	2500373381328 [label=ReluBackward0]
	2500373381472 -> 2500373381328
	2500373381472 [label=CudnnBatchNormBackward0]
	2500373381568 -> 2500373381472
	2500373381568 [label=ConvolutionBackward0]
	2500373381760 -> 2500373381568
	2500373381760 [label=ReluBackward0]
	2500373381904 -> 2500373381760
	2500373381904 [label=CudnnBatchNormBackward0]
	2500373382048 -> 2500373381904
	2500373382048 [label=ConvolutionBackward0]
	2500373312320 -> 2500373382048
	2500373312320 [label=ReluBackward0]
	2500373382336 -> 2500373312320
	2500373382336 [label=AddBackward0]
	2500373382480 -> 2500373382336
	2500373382480 [label=CudnnBatchNormBackward0]
	2500373382624 -> 2500373382480
	2500373382624 [label=ConvolutionBackward0]
	2500373382816 -> 2500373382624
	2500373382816 [label=ReluBackward0]
	2500373382960 -> 2500373382816
	2500373382960 [label=CudnnBatchNormBackward0]
	2500373383104 -> 2500373382960
	2500373383104 [label=ConvolutionBackward0]
	2500373383296 -> 2500373383104
	2500373383296 [label=ReluBackward0]
	2500373383440 -> 2500373383296
	2500373383440 [label=CudnnBatchNormBackward0]
	2500373383584 -> 2500373383440
	2500373383584 [label=ConvolutionBackward0]
	2500373382192 -> 2500373383584
	2500373382192 [label=ReluBackward0]
	2500373383872 -> 2500373382192
	2500373383872 [label=AddBackward0]
	2500373384016 -> 2500373383872
	2500373384016 [label=CudnnBatchNormBackward0]
	2500373384160 -> 2500373384016
	2500373384160 [label=ConvolutionBackward0]
	2500373384352 -> 2500373384160
	2500373384352 [label=ReluBackward0]
	2500373384496 -> 2500373384352
	2500373384496 [label=CudnnBatchNormBackward0]
	2500373384640 -> 2500373384496
	2500373384640 [label=ConvolutionBackward0]
	2500373384832 -> 2500373384640
	2500373384832 [label=ReluBackward0]
	2500373384976 -> 2500373384832
	2500373384976 [label=CudnnBatchNormBackward0]
	2500373385120 -> 2500373384976
	2500373385120 [label=ConvolutionBackward0]
	2500373383728 -> 2500373385120
	2500373383728 [label=ReluBackward0]
	2500373385408 -> 2500373383728
	2500373385408 [label=AddBackward0]
	2500373385552 -> 2500373385408
	2500373385552 [label=CudnnBatchNormBackward0]
	2500373385696 -> 2500373385552
	2500373385696 [label=ConvolutionBackward0]
	2500373385888 -> 2500373385696
	2500373385888 [label=ReluBackward0]
	2500373386032 -> 2500373385888
	2500373386032 [label=CudnnBatchNormBackward0]
	2500373386176 -> 2500373386032
	2500373386176 [label=ConvolutionBackward0]
	2500373386368 -> 2500373386176
	2500373386368 [label=ReluBackward0]
	2500373386512 -> 2500373386368
	2500373386512 [label=CudnnBatchNormBackward0]
	2500373386656 -> 2500373386512
	2500373386656 [label=ConvolutionBackward0]
	2500373385264 -> 2500373386656
	2500373385264 [label=ReluBackward0]
	2500373386944 -> 2500373385264
	2500373386944 [label=AddBackward0]
	2500373387088 -> 2500373386944
	2500373387088 [label=CudnnBatchNormBackward0]
	2500373387232 -> 2500373387088
	2500373387232 [label=ConvolutionBackward0]
	2500373387424 -> 2500373387232
	2500373387424 [label=ReluBackward0]
	2500373387568 -> 2500373387424
	2500373387568 [label=CudnnBatchNormBackward0]
	2500373387712 -> 2500373387568
	2500373387712 [label=ConvolutionBackward0]
	2500373387904 -> 2500373387712
	2500373387904 [label=ReluBackward0]
	2500373388048 -> 2500373387904
	2500373388048 [label=CudnnBatchNormBackward0]
	2500373388192 -> 2500373388048
	2500373388192 [label=ConvolutionBackward0]
	2500373386800 -> 2500373388192
	2500373386800 [label=ReluBackward0]
	2500373388480 -> 2500373386800
	2500373388480 [label=AddBackward0]
	2500373388624 -> 2500373388480
	2500373388624 [label=CudnnBatchNormBackward0]
	2500373388768 -> 2500373388624
	2500373388768 [label=ConvolutionBackward0]
	2500373388960 -> 2500373388768
	2500373388960 [label=ReluBackward0]
	2500373389104 -> 2500373388960
	2500373389104 [label=CudnnBatchNormBackward0]
	2500373389248 -> 2500373389104
	2500373389248 [label=ConvolutionBackward0]
	2500373389440 -> 2500373389248
	2500373389440 [label=ReluBackward0]
	2500373389584 -> 2500373389440
	2500373389584 [label=CudnnBatchNormBackward0]
	2500373389728 -> 2500373389584
	2500373389728 [label=ConvolutionBackward0]
	2500373388336 -> 2500373389728
	2500373388336 [label=ReluBackward0]
	2500373390016 -> 2500373388336
	2500373390016 [label=AddBackward0]
	2500373390160 -> 2500373390016
	2500373390160 [label=CudnnBatchNormBackward0]
	2500373390304 -> 2500373390160
	2500373390304 [label=ConvolutionBackward0]
	2500373390496 -> 2500373390304
	2500373390496 [label=ReluBackward0]
	2500373390640 -> 2500373390496
	2500373390640 [label=CudnnBatchNormBackward0]
	2500373390784 -> 2500373390640
	2500373390784 [label=ConvolutionBackward0]
	2500373390976 -> 2500373390784
	2500373390976 [label=ReluBackward0]
	2500373391120 -> 2500373390976
	2500373391120 [label=CudnnBatchNormBackward0]
	2500373391264 -> 2500373391120
	2500373391264 [label=ConvolutionBackward0]
	2500373311216 -> 2500373391264
	2500373311216 [label=ReluBackward0]
	2500373391552 -> 2500373311216
	2500373391552 [label=AddBackward0]
	2500373391696 -> 2500373391552
	2500373391696 [label=CudnnBatchNormBackward0]
	2500373391840 -> 2500373391696
	2500373391840 [label=ConvolutionBackward0]
	2500373392032 -> 2500373391840
	2500373392032 [label=ReluBackward0]
	2500373392176 -> 2500373392032
	2500373392176 [label=CudnnBatchNormBackward0]
	2500373392320 -> 2500373392176
	2500373392320 [label=ConvolutionBackward0]
	2500373392512 -> 2500373392320
	2500373392512 [label=ReluBackward0]
	2500373392656 -> 2500373392512
	2500373392656 [label=CudnnBatchNormBackward0]
	2500373392800 -> 2500373392656
	2500373392800 [label=ConvolutionBackward0]
	2500373391408 -> 2500373392800
	2500373391408 [label=ReluBackward0]
	2500373393088 -> 2500373391408
	2500373393088 [label=AddBackward0]
	2500373393232 -> 2500373393088
	2500373393232 [label=CudnnBatchNormBackward0]
	2500373393376 -> 2500373393232
	2500373393376 [label=ConvolutionBackward0]
	2500373393568 -> 2500373393376
	2500373393568 [label=ReluBackward0]
	2500373393712 -> 2500373393568
	2500373393712 [label=CudnnBatchNormBackward0]
	2500373393856 -> 2500373393712
	2500373393856 [label=ConvolutionBackward0]
	2500373394048 -> 2500373393856
	2500373394048 [label=ReluBackward0]
	2500373394192 -> 2500373394048
	2500373394192 [label=CudnnBatchNormBackward0]
	2500373394336 -> 2500373394192
	2500373394336 [label=ConvolutionBackward0]
	2500373392944 -> 2500373394336
	2500373392944 [label=ReluBackward0]
	2500373394624 -> 2500373392944
	2500373394624 [label=AddBackward0]
	2500373394768 -> 2500373394624
	2500373394768 [label=CudnnBatchNormBackward0]
	2500373394912 -> 2500373394768
	2500373394912 [label=ConvolutionBackward0]
	2500373395104 -> 2500373394912
	2500373395104 [label=ReluBackward0]
	2500373395248 -> 2500373395104
	2500373395248 [label=CudnnBatchNormBackward0]
	2500373395392 -> 2500373395248
	2500373395392 [label=ConvolutionBackward0]
	2500373395584 -> 2500373395392
	2500373395584 [label=ReluBackward0]
	2500373395728 -> 2500373395584
	2500373395728 [label=CudnnBatchNormBackward0]
	2500373395872 -> 2500373395728
	2500373395872 [label=ConvolutionBackward0]
	2500373394480 -> 2500373395872
	2500373394480 [label=ReluBackward0]
	2500373396160 -> 2500373394480
	2500373396160 [label=AddBackward0]
	2500373396304 -> 2500373396160
	2500373396304 [label=CudnnBatchNormBackward0]
	2500373396448 -> 2500373396304
	2500373396448 [label=ConvolutionBackward0]
	2500373396640 -> 2500373396448
	2500373396640 [label=ReluBackward0]
	2500373396784 -> 2500373396640
	2500373396784 [label=CudnnBatchNormBackward0]
	2500373396928 -> 2500373396784
	2500373396928 [label=ConvolutionBackward0]
	2500373397120 -> 2500373396928
	2500373397120 [label=ReluBackward0]
	2500373397264 -> 2500373397120
	2500373397264 [label=CudnnBatchNormBackward0]
	2500373397408 -> 2500373397264
	2500373397408 [label=ConvolutionBackward0]
	2500373310112 -> 2500373397408
	2500373310112 [label=ReluBackward0]
	2500373446912 -> 2500373310112
	2500373446912 [label=AddBackward0]
	2500373447056 -> 2500373446912
	2500373447056 [label=CudnnBatchNormBackward0]
	2500373447200 -> 2500373447056
	2500373447200 [label=ConvolutionBackward0]
	2500373447392 -> 2500373447200
	2500373447392 [label=ReluBackward0]
	2500373447536 -> 2500373447392
	2500373447536 [label=CudnnBatchNormBackward0]
	2500373447680 -> 2500373447536
	2500373447680 [label=ConvolutionBackward0]
	2500373447872 -> 2500373447680
	2500373447872 [label=ReluBackward0]
	2500373448016 -> 2500373447872
	2500373448016 [label=CudnnBatchNormBackward0]
	2500373448160 -> 2500373448016
	2500373448160 [label=ConvolutionBackward0]
	2500373446768 -> 2500373448160
	2500373446768 [label=ReluBackward0]
	2500373448448 -> 2500373446768
	2500373448448 [label=AddBackward0]
	2500373448592 -> 2500373448448
	2500373448592 [label=CudnnBatchNormBackward0]
	2500373448736 -> 2500373448592
	2500373448736 [label=ConvolutionBackward0]
	2500373448928 -> 2500373448736
	2500373448928 [label=ReluBackward0]
	2500373449072 -> 2500373448928
	2500373449072 [label=CudnnBatchNormBackward0]
	2500373449216 -> 2500373449072
	2500373449216 [label=ConvolutionBackward0]
	2500373449408 -> 2500373449216
	2500373449408 [label=ReluBackward0]
	2500373449552 -> 2500373449408
	2500373449552 [label=CudnnBatchNormBackward0]
	2500373449696 -> 2500373449552
	2500373449696 [label=ConvolutionBackward0]
	2500373448304 -> 2500373449696
	2500373448304 [label=ReluBackward0]
	2500373449984 -> 2500373448304
	2500373449984 [label=AddBackward0]
	2500373450128 -> 2500373449984
	2500373450128 [label=CudnnBatchNormBackward0]
	2500373450272 -> 2500373450128
	2500373450272 [label=ConvolutionBackward0]
	2500373450464 -> 2500373450272
	2500373450464 [label=ReluBackward0]
	2500373450608 -> 2500373450464
	2500373450608 [label=CudnnBatchNormBackward0]
	2500373450752 -> 2500373450608
	2500373450752 [label=ConvolutionBackward0]
	2500373450944 -> 2500373450752
	2500373450944 [label=ReluBackward0]
	2500373451088 -> 2500373450944
	2500373451088 [label=CudnnBatchNormBackward0]
	2500373451232 -> 2500373451088
	2500373451232 [label=ConvolutionBackward0]
	2500373451424 -> 2500373451232
	2500373451424 [label=MaxPool2DWithIndicesBackward0]
	2500373309008 -> 2500373451424
	2500373309008 [label=ReluBackward0]
	2500373451664 -> 2500373309008
	2500373451664 [label=CudnnBatchNormBackward0]
	2500373451808 -> 2500373451664
	2500373451808 [label=ConvolutionBackward0]
	2500373452000 -> 2500373451808
	2496354047696 [label="encoder.conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	2496354047696 -> 2500373452000
	2500373452000 [label=AccumulateGrad]
	2500373451712 -> 2500373451664
	2496643694288 [label="encoder.bn1.weight
 (64)" fillcolor=lightblue]
	2496643694288 -> 2500373451712
	2500373451712 [label=AccumulateGrad]
	2500373451856 -> 2500373451664
	2496643692608 [label="encoder.bn1.bias
 (64)" fillcolor=lightblue]
	2496643692608 -> 2500373451856
	2500373451856 [label=AccumulateGrad]
	2500373451472 -> 2500373451232
	2496643691728 [label="encoder.layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2496643691728 -> 2500373451472
	2500373451472 [label=AccumulateGrad]
	2500373451040 -> 2500373451088
	2496643691168 [label="encoder.layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2496643691168 -> 2500373451040
	2500373451040 [label=AccumulateGrad]
	2500373451280 -> 2500373451088
	2496643691248 [label="encoder.layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2496643691248 -> 2500373451280
	2500373451280 [label=AccumulateGrad]
	2500373450992 -> 2500373450752
	2496643691088 [label="encoder.layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2496643691088 -> 2500373450992
	2500373450992 [label=AccumulateGrad]
	2500373450560 -> 2500373450608
	2496643691008 [label="encoder.layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2496643691008 -> 2500373450560
	2500373450560 [label=AccumulateGrad]
	2500373450800 -> 2500373450608
	2496643690928 [label="encoder.layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2496643690928 -> 2500373450800
	2500373450800 [label=AccumulateGrad]
	2500373450512 -> 2500373450272
	2496643689968 [label="encoder.layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2496643689968 -> 2500373450512
	2500373450512 [label=AccumulateGrad]
	2500373450320 -> 2500373450128
	2496643687008 [label="encoder.layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	2496643687008 -> 2500373450320
	2500373450320 [label=AccumulateGrad]
	2500373450224 -> 2500373450128
	2496472644288 [label="encoder.layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	2496472644288 -> 2500373450224
	2500373450224 [label=AccumulateGrad]
	2500373449840 -> 2500373449984
	2500373449840 [label=CudnnBatchNormBackward0]
	2500373450656 -> 2500373449840
	2500373450656 [label=ConvolutionBackward0]
	2500373451424 -> 2500373450656
	2500373450896 -> 2500373450656
	2496643692528 [label="encoder.layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2496643692528 -> 2500373450896
	2500373450896 [label=AccumulateGrad]
	2500373450416 -> 2500373449840
	2496643691968 [label="encoder.layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2496643691968 -> 2500373450416
	2500373450416 [label=AccumulateGrad]
	2500373450368 -> 2500373449840
	2496643692208 [label="encoder.layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2496643692208 -> 2500373450368
	2500373450368 [label=AccumulateGrad]
	2500373449888 -> 2500373449696
	2496645535280 [label="encoder.layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2496645535280 -> 2500373449888
	2500373449888 [label=AccumulateGrad]
	2500373449504 -> 2500373449552
	2496369939696 [label="encoder.layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2496369939696 -> 2500373449504
	2500373449504 [label=AccumulateGrad]
	2500373449744 -> 2500373449552
	2496369943616 [label="encoder.layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2496369943616 -> 2500373449744
	2500373449744 [label=AccumulateGrad]
	2500373449456 -> 2500373449216
	2496369935056 [label="encoder.layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2496369935056 -> 2500373449456
	2500373449456 [label=AccumulateGrad]
	2500373449024 -> 2500373449072
	2496369937296 [label="encoder.layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2496369937296 -> 2500373449024
	2500373449024 [label=AccumulateGrad]
	2500373449264 -> 2500373449072
	2496369944336 [label="encoder.layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2496369944336 -> 2500373449264
	2500373449264 [label=AccumulateGrad]
	2500373448976 -> 2500373448736
	2496369948096 [label="encoder.layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2496369948096 -> 2500373448976
	2500373448976 [label=AccumulateGrad]
	2500373448784 -> 2500373448592
	2496369942256 [label="encoder.layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	2496369942256 -> 2500373448784
	2500373448784 [label=AccumulateGrad]
	2500373448688 -> 2500373448592
	2496369937936 [label="encoder.layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	2496369937936 -> 2500373448688
	2500373448688 [label=AccumulateGrad]
	2500373448304 -> 2500373448448
	2500373448352 -> 2500373448160
	2496369943776 [label="encoder.layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2496369943776 -> 2500373448352
	2500373448352 [label=AccumulateGrad]
	2500373447968 -> 2500373448016
	2496369943216 [label="encoder.layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	2496369943216 -> 2500373447968
	2500373447968 [label=AccumulateGrad]
	2500373448208 -> 2500373448016
	2496369939456 [label="encoder.layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	2496369939456 -> 2500373448208
	2500373448208 [label=AccumulateGrad]
	2500373447920 -> 2500373447680
	2496369941376 [label="encoder.layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2496369941376 -> 2500373447920
	2500373447920 [label=AccumulateGrad]
	2500373447488 -> 2500373447536
	2496369946256 [label="encoder.layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	2496369946256 -> 2500373447488
	2500373447488 [label=AccumulateGrad]
	2500373447728 -> 2500373447536
	2496369947936 [label="encoder.layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	2496369947936 -> 2500373447728
	2500373447728 [label=AccumulateGrad]
	2500373447440 -> 2500373447200
	2496369944176 [label="encoder.layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2496369944176 -> 2500373447440
	2500373447440 [label=AccumulateGrad]
	2500373447248 -> 2500373447056
	2496369940896 [label="encoder.layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	2496369940896 -> 2500373447248
	2500373447248 [label=AccumulateGrad]
	2500373447152 -> 2500373447056
	2496369944816 [label="encoder.layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	2496369944816 -> 2500373447152
	2500373447152 [label=AccumulateGrad]
	2500373446768 -> 2500373446912
	2500373446816 -> 2500373397408
	2496369938416 [label="encoder.layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	2496369938416 -> 2500373446816
	2500373446816 [label=AccumulateGrad]
	2500373397216 -> 2500373397264
	2496369947776 [label="encoder.layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2496369947776 -> 2500373397216
	2500373397216 [label=AccumulateGrad]
	2500373397456 -> 2500373397264
	2496369942576 [label="encoder.layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2496369942576 -> 2500373397456
	2500373397456 [label=AccumulateGrad]
	2500373397168 -> 2500373396928
	2496369948176 [label="encoder.layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2496369948176 -> 2500373397168
	2500373397168 [label=AccumulateGrad]
	2500373396736 -> 2500373396784
	2496369945216 [label="encoder.layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2496369945216 -> 2500373396736
	2500373396736 [label=AccumulateGrad]
	2500373396976 -> 2500373396784
	2496369942096 [label="encoder.layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2496369942096 -> 2500373396976
	2500373396976 [label=AccumulateGrad]
	2500373396688 -> 2500373396448
	2496369941936 [label="encoder.layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2496369941936 -> 2500373396688
	2500373396688 [label=AccumulateGrad]
	2500373396496 -> 2500373396304
	2496369948336 [label="encoder.layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	2496369948336 -> 2500373396496
	2500373396496 [label=AccumulateGrad]
	2500373396400 -> 2500373396304
	2496369945296 [label="encoder.layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	2496369945296 -> 2500373396400
	2500373396400 [label=AccumulateGrad]
	2500373396016 -> 2500373396160
	2500373396016 [label=CudnnBatchNormBackward0]
	2500373396832 -> 2500373396016
	2500373396832 [label=ConvolutionBackward0]
	2500373310112 -> 2500373396832
	2500373397072 -> 2500373396832
	2496369945056 [label="encoder.layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2496369945056 -> 2500373397072
	2500373397072 [label=AccumulateGrad]
	2500373396592 -> 2500373396016
	2496369945936 [label="encoder.layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2496369945936 -> 2500373396592
	2500373396592 [label=AccumulateGrad]
	2500373396544 -> 2500373396016
	2496369934976 [label="encoder.layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2496369934976 -> 2500373396544
	2500373396544 [label=AccumulateGrad]
	2500373396064 -> 2500373395872
	2496369937696 [label="encoder.layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2496369937696 -> 2500373396064
	2500373396064 [label=AccumulateGrad]
	2500373395680 -> 2500373395728
	2496369948816 [label="encoder.layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2496369948816 -> 2500373395680
	2500373395680 [label=AccumulateGrad]
	2500373395920 -> 2500373395728
	2496369938176 [label="encoder.layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2496369938176 -> 2500373395920
	2500373395920 [label=AccumulateGrad]
	2500373395632 -> 2500373395392
	2496369938896 [label="encoder.layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2496369938896 -> 2500373395632
	2500373395632 [label=AccumulateGrad]
	2500373395200 -> 2500373395248
	2496369946976 [label="encoder.layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2496369946976 -> 2500373395200
	2500373395200 [label=AccumulateGrad]
	2500373395440 -> 2500373395248
	2496369949856 [label="encoder.layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2496369949856 -> 2500373395440
	2500373395440 [label=AccumulateGrad]
	2500373395152 -> 2500373394912
	2496369939536 [label="encoder.layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2496369939536 -> 2500373395152
	2500373395152 [label=AccumulateGrad]
	2500373394960 -> 2500373394768
	2496369942896 [label="encoder.layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	2496369942896 -> 2500373394960
	2500373394960 [label=AccumulateGrad]
	2500373394864 -> 2500373394768
	2496369937056 [label="encoder.layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	2496369937056 -> 2500373394864
	2500373394864 [label=AccumulateGrad]
	2500373394480 -> 2500373394624
	2500373394528 -> 2500373394336
	2496369938256 [label="encoder.layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2496369938256 -> 2500373394528
	2500373394528 [label=AccumulateGrad]
	2500373394144 -> 2500373394192
	2496369942816 [label="encoder.layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	2496369942816 -> 2500373394144
	2500373394144 [label=AccumulateGrad]
	2500373394384 -> 2500373394192
	2496369949696 [label="encoder.layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	2496369949696 -> 2500373394384
	2500373394384 [label=AccumulateGrad]
	2500373394096 -> 2500373393856
	2496369936496 [label="encoder.layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2496369936496 -> 2500373394096
	2500373394096 [label=AccumulateGrad]
	2500373393664 -> 2500373393712
	2496369943696 [label="encoder.layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	2496369943696 -> 2500373393664
	2500373393664 [label=AccumulateGrad]
	2500373393904 -> 2500373393712
	2496369935696 [label="encoder.layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	2496369935696 -> 2500373393904
	2500373393904 [label=AccumulateGrad]
	2500373393616 -> 2500373393376
	2496369950016 [label="encoder.layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2496369950016 -> 2500373393616
	2500373393616 [label=AccumulateGrad]
	2500373393424 -> 2500373393232
	2496369944496 [label="encoder.layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	2496369944496 -> 2500373393424
	2500373393424 [label=AccumulateGrad]
	2500373393328 -> 2500373393232
	2496369946736 [label="encoder.layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	2496369946736 -> 2500373393328
	2500373393328 [label=AccumulateGrad]
	2500373392944 -> 2500373393088
	2500373392992 -> 2500373392800
	2496369940096 [label="encoder.layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2496369940096 -> 2500373392992
	2500373392992 [label=AccumulateGrad]
	2500373392608 -> 2500373392656
	2496369935296 [label="encoder.layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	2496369935296 -> 2500373392608
	2500373392608 [label=AccumulateGrad]
	2500373392848 -> 2500373392656
	2496369945776 [label="encoder.layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	2496369945776 -> 2500373392848
	2500373392848 [label=AccumulateGrad]
	2500373392560 -> 2500373392320
	2496369947856 [label="encoder.layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2496369947856 -> 2500373392560
	2500373392560 [label=AccumulateGrad]
	2500373392128 -> 2500373392176
	2496369941296 [label="encoder.layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	2496369941296 -> 2500373392128
	2500373392128 [label=AccumulateGrad]
	2500373392368 -> 2500373392176
	2496369939296 [label="encoder.layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	2496369939296 -> 2500373392368
	2500373392368 [label=AccumulateGrad]
	2500373392080 -> 2500373391840
	2496369941616 [label="encoder.layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2496369941616 -> 2500373392080
	2500373392080 [label=AccumulateGrad]
	2500373391888 -> 2500373391696
	2496369938096 [label="encoder.layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	2496369938096 -> 2500373391888
	2500373391888 [label=AccumulateGrad]
	2500373391792 -> 2500373391696
	2496369948416 [label="encoder.layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	2496369948416 -> 2500373391792
	2500373391792 [label=AccumulateGrad]
	2500373391408 -> 2500373391552
	2500373391456 -> 2500373391264
	2496369946016 [label="encoder.layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	2496369946016 -> 2500373391456
	2500373391456 [label=AccumulateGrad]
	2500373391072 -> 2500373391120
	2496369934576 [label="encoder.layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2496369934576 -> 2500373391072
	2500373391072 [label=AccumulateGrad]
	2500373391312 -> 2500373391120
	2496369942976 [label="encoder.layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2496369942976 -> 2500373391312
	2500373391312 [label=AccumulateGrad]
	2500373391024 -> 2500373390784
	2496369936096 [label="encoder.layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2496369936096 -> 2500373391024
	2500373391024 [label=AccumulateGrad]
	2500373390592 -> 2500373390640
	2496369949536 [label="encoder.layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2496369949536 -> 2500373390592
	2500373390592 [label=AccumulateGrad]
	2500373390832 -> 2500373390640
	2496369937216 [label="encoder.layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2496369937216 -> 2500373390832
	2500373390832 [label=AccumulateGrad]
	2500373390544 -> 2500373390304
	2496369936416 [label="encoder.layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2496369936416 -> 2500373390544
	2500373390544 [label=AccumulateGrad]
	2500373390352 -> 2500373390160
	2496369939376 [label="encoder.layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	2496369939376 -> 2500373390352
	2500373390352 [label=AccumulateGrad]
	2500373390256 -> 2500373390160
	2496369941776 [label="encoder.layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	2496369941776 -> 2500373390256
	2500373390256 [label=AccumulateGrad]
	2500373389872 -> 2500373390016
	2500373389872 [label=CudnnBatchNormBackward0]
	2500373390688 -> 2500373389872
	2500373390688 [label=ConvolutionBackward0]
	2500373311216 -> 2500373390688
	2500373390928 -> 2500373390688
	2496369948656 [label="encoder.layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	2496369948656 -> 2500373390928
	2500373390928 [label=AccumulateGrad]
	2500373390448 -> 2500373389872
	2496369935136 [label="encoder.layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	2496369935136 -> 2500373390448
	2500373390448 [label=AccumulateGrad]
	2500373390400 -> 2500373389872
	2496369934496 [label="encoder.layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	2496369934496 -> 2500373390400
	2500373390400 [label=AccumulateGrad]
	2500373389920 -> 2500373389728
	2496369940416 [label="encoder.layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2496369940416 -> 2500373389920
	2500373389920 [label=AccumulateGrad]
	2500373389536 -> 2500373389584
	2496369947216 [label="encoder.layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2496369947216 -> 2500373389536
	2500373389536 [label=AccumulateGrad]
	2500373389776 -> 2500373389584
	2496369935456 [label="encoder.layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2496369935456 -> 2500373389776
	2500373389776 [label=AccumulateGrad]
	2500373389488 -> 2500373389248
	2496369944096 [label="encoder.layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2496369944096 -> 2500373389488
	2500373389488 [label=AccumulateGrad]
	2500373389056 -> 2500373389104
	2496369948736 [label="encoder.layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2496369948736 -> 2500373389056
	2500373389056 [label=AccumulateGrad]
	2500373389296 -> 2500373389104
	2496369936896 [label="encoder.layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2496369936896 -> 2500373389296
	2500373389296 [label=AccumulateGrad]
	2500373389008 -> 2500373388768
	2496472993696 [label="encoder.layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2496472993696 -> 2500373389008
	2500373389008 [label=AccumulateGrad]
	2500373388816 -> 2500373388624
	2496472994016 [label="encoder.layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	2496472994016 -> 2500373388816
	2500373388816 [label=AccumulateGrad]
	2500373388720 -> 2500373388624
	2496369897744 [label="encoder.layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	2496369897744 -> 2500373388720
	2500373388720 [label=AccumulateGrad]
	2500373388336 -> 2500373388480
	2500373388384 -> 2500373388192
	2496369901024 [label="encoder.layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2496369901024 -> 2500373388384
	2500373388384 [label=AccumulateGrad]
	2500373388000 -> 2500373388048
	2496369901344 [label="encoder.layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	2496369901344 -> 2500373388000
	2500373388000 [label=AccumulateGrad]
	2500373388240 -> 2500373388048
	2496369895584 [label="encoder.layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	2496369895584 -> 2500373388240
	2500373388240 [label=AccumulateGrad]
	2500373387952 -> 2500373387712
	2496369897024 [label="encoder.layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2496369897024 -> 2500373387952
	2500373387952 [label=AccumulateGrad]
	2500373387520 -> 2500373387568
	2496369898624 [label="encoder.layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	2496369898624 -> 2500373387520
	2500373387520 [label=AccumulateGrad]
	2500373387760 -> 2500373387568
	2496369897824 [label="encoder.layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	2496369897824 -> 2500373387760
	2500373387760 [label=AccumulateGrad]
	2500373387472 -> 2500373387232
	2496369897584 [label="encoder.layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2496369897584 -> 2500373387472
	2500373387472 [label=AccumulateGrad]
	2500373387280 -> 2500373387088
	2496369895984 [label="encoder.layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	2496369895984 -> 2500373387280
	2500373387280 [label=AccumulateGrad]
	2500373387184 -> 2500373387088
	2496369896704 [label="encoder.layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	2496369896704 -> 2500373387184
	2500373387184 [label=AccumulateGrad]
	2500373386800 -> 2500373386944
	2500373386848 -> 2500373386656
	2496369898544 [label="encoder.layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2496369898544 -> 2500373386848
	2500373386848 [label=AccumulateGrad]
	2500373386464 -> 2500373386512
	2496369898464 [label="encoder.layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	2496369898464 -> 2500373386464
	2500373386464 [label=AccumulateGrad]
	2500373386704 -> 2500373386512
	2496369899424 [label="encoder.layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	2496369899424 -> 2500373386704
	2500373386704 [label=AccumulateGrad]
	2500373386416 -> 2500373386176
	2496369892704 [label="encoder.layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2496369892704 -> 2500373386416
	2500373386416 [label=AccumulateGrad]
	2500373385984 -> 2500373386032
	2496369900144 [label="encoder.layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	2496369900144 -> 2500373385984
	2500373385984 [label=AccumulateGrad]
	2500373386224 -> 2500373386032
	2496369893984 [label="encoder.layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	2496369893984 -> 2500373386224
	2500373386224 [label=AccumulateGrad]
	2500373385936 -> 2500373385696
	2496369893744 [label="encoder.layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2496369893744 -> 2500373385936
	2500373385936 [label=AccumulateGrad]
	2500373385744 -> 2500373385552
	2496369899344 [label="encoder.layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	2496369899344 -> 2500373385744
	2500373385744 [label=AccumulateGrad]
	2500373385648 -> 2500373385552
	2496369894624 [label="encoder.layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	2496369894624 -> 2500373385648
	2500373385648 [label=AccumulateGrad]
	2500373385264 -> 2500373385408
	2500373385312 -> 2500373385120
	2496369901104 [label="encoder.layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2496369901104 -> 2500373385312
	2500373385312 [label=AccumulateGrad]
	2500373384928 -> 2500373384976
	2496369895744 [label="encoder.layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	2496369895744 -> 2500373384928
	2500373384928 [label=AccumulateGrad]
	2500373385168 -> 2500373384976
	2496369900304 [label="encoder.layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	2496369900304 -> 2500373385168
	2500373385168 [label=AccumulateGrad]
	2500373384880 -> 2500373384640
	2496369888224 [label="encoder.layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2496369888224 -> 2500373384880
	2500373384880 [label=AccumulateGrad]
	2500373384448 -> 2500373384496
	2496369894144 [label="encoder.layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	2496369894144 -> 2500373384448
	2500373384448 [label=AccumulateGrad]
	2500373384688 -> 2500373384496
	2496369897184 [label="encoder.layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	2496369897184 -> 2500373384688
	2500373384688 [label=AccumulateGrad]
	2500373384400 -> 2500373384160
	2496369896464 [label="encoder.layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2496369896464 -> 2500373384400
	2500373384400 [label=AccumulateGrad]
	2500373384208 -> 2500373384016
	2496369900784 [label="encoder.layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	2496369900784 -> 2500373384208
	2500373384208 [label=AccumulateGrad]
	2500373384112 -> 2500373384016
	2496369899824 [label="encoder.layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	2496369899824 -> 2500373384112
	2500373384112 [label=AccumulateGrad]
	2500373383728 -> 2500373383872
	2500373383776 -> 2500373383584
	2496369894464 [label="encoder.layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2496369894464 -> 2500373383776
	2500373383776 [label=AccumulateGrad]
	2500373383392 -> 2500373383440
	2496369900544 [label="encoder.layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	2496369900544 -> 2500373383392
	2500373383392 [label=AccumulateGrad]
	2500373383632 -> 2500373383440
	2496369894544 [label="encoder.layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	2496369894544 -> 2500373383632
	2500373383632 [label=AccumulateGrad]
	2500373383344 -> 2500373383104
	2496369893584 [label="encoder.layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2496369893584 -> 2500373383344
	2500373383344 [label=AccumulateGrad]
	2500373382912 -> 2500373382960
	2496369898864 [label="encoder.layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	2496369898864 -> 2500373382912
	2500373382912 [label=AccumulateGrad]
	2500373383152 -> 2500373382960
	2496369895904 [label="encoder.layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	2496369895904 -> 2500373383152
	2500373383152 [label=AccumulateGrad]
	2500373382864 -> 2500373382624
	2496369893424 [label="encoder.layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2496369893424 -> 2500373382864
	2500373382864 [label=AccumulateGrad]
	2500373382672 -> 2500373382480
	2496369894064 [label="encoder.layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	2496369894064 -> 2500373382672
	2500373382672 [label=AccumulateGrad]
	2500373382576 -> 2500373382480
	2496369894944 [label="encoder.layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	2496369894944 -> 2500373382576
	2500373382576 [label=AccumulateGrad]
	2500373382192 -> 2500373382336
	2500373382240 -> 2500373382048
	2496369896224 [label="encoder.layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	2496369896224 -> 2500373382240
	2500373382240 [label=AccumulateGrad]
	2500373381856 -> 2500373381904
	2496369896944 [label="encoder.layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2496369896944 -> 2500373381856
	2500373381856 [label=AccumulateGrad]
	2500373382096 -> 2500373381904
	2496369900384 [label="encoder.layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2496369900384 -> 2500373382096
	2500373382096 [label=AccumulateGrad]
	2500373381808 -> 2500373381568
	2496645619040 [label="encoder.layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2496645619040 -> 2500373381808
	2500373381808 [label=AccumulateGrad]
	2500373381616 -> 2500373381472
	2496645611920 [label="encoder.layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2496645611920 -> 2500373381616
	2500373381616 [label=AccumulateGrad]
	2500373381424 -> 2500373381472
	2496645616400 [label="encoder.layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2496645616400 -> 2500373381424
	2500373381424 [label=AccumulateGrad]
	2500373381376 -> 2500373315488
	2496645617680 [label="encoder.layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2496645617680 -> 2500373381376
	2500373381376 [label=AccumulateGrad]
	2500373315536 -> 2500373315392
	2496645615120 [label="encoder.layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	2496645615120 -> 2500373315536
	2500373315536 [label=AccumulateGrad]
	2500373381184 -> 2500373315392
	2496645624080 [label="encoder.layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	2496645624080 -> 2500373381184
	2500373381184 [label=AccumulateGrad]
	2500373315440 -> 2500373315296
	2500373315440 [label=CudnnBatchNormBackward0]
	2500373381520 -> 2500373315440
	2500373381520 [label=ConvolutionBackward0]
	2500373312320 -> 2500373381520
	2500373381712 -> 2500373381520
	2496369896784 [label="encoder.layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	2496369896784 -> 2500373381712
	2500373381712 [label=AccumulateGrad]
	2500373381280 -> 2500373315440
	2496369894384 [label="encoder.layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	2496369894384 -> 2500373381280
	2500373381280 [label=AccumulateGrad]
	2500373381232 -> 2500373315440
	2496369893824 [label="encoder.layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	2496369893824 -> 2500373381232
	2500373381232 [label=AccumulateGrad]
	2500373315200 -> 2500373315008
	2496645452480 [label="encoder.layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2496645452480 -> 2500373315200
	2500373315200 [label=AccumulateGrad]
	2500373315056 -> 2500373314912
	2496645204960 [label="encoder.layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2496645204960 -> 2500373315056
	2500373315056 [label=AccumulateGrad]
	2500373314864 -> 2500373314912
	2496645203920 [label="encoder.layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2496645203920 -> 2500373314864
	2500373314864 [label=AccumulateGrad]
	2500373314816 -> 2500373314576
	2496645791120 [label="encoder.layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2496645791120 -> 2500373314816
	2500373314816 [label=AccumulateGrad]
	2500373314624 -> 2500373314480
	2496645781040 [label="encoder.layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2496645781040 -> 2500373314624
	2500373314624 [label=AccumulateGrad]
	2500373314432 -> 2500373314480
	2496645787040 [label="encoder.layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2496645787040 -> 2500373314432
	2500373314432 [label=AccumulateGrad]
	2500373314384 -> 2500373314144
	2496645789360 [label="encoder.layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2496645789360 -> 2500373314384
	2500373314384 [label=AccumulateGrad]
	2500373314192 -> 2500373314000
	2496645787680 [label="encoder.layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	2496645787680 -> 2500373314192
	2500373314192 [label=AccumulateGrad]
	2500373314096 -> 2500373314000
	2496645789600 [label="encoder.layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	2496645789600 -> 2500373314096
	2500373314096 [label=AccumulateGrad]
	2500373314048 -> 2500373313904
	2500373313808 -> 2500373313616
	2496645789040 [label="encoder.layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2496645789040 -> 2500373313808
	2500373313808 [label=AccumulateGrad]
	2500373313664 -> 2500373313520
	2496645786880 [label="encoder.layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	2496645786880 -> 2500373313664
	2500373313664 [label=AccumulateGrad]
	2500373313472 -> 2500373313520
	2496645786720 [label="encoder.layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	2496645786720 -> 2500373313472
	2500373313472 [label=AccumulateGrad]
	2500373313424 -> 2500373313184
	2496645790640 [label="encoder.layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2496645790640 -> 2500373313424
	2500373313424 [label=AccumulateGrad]
	2500373313232 -> 2500373313088
	2496645784720 [label="encoder.layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	2496645784720 -> 2500373313232
	2500373313232 [label=AccumulateGrad]
	2500373313040 -> 2500373313088
	2496645784000 [label="encoder.layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	2496645784000 -> 2500373313040
	2500373313040 [label=AccumulateGrad]
	2500373312992 -> 2500373312752
	2496645784160 [label="encoder.layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2496645784160 -> 2500373312992
	2500373312992 [label=AccumulateGrad]
	2500373312800 -> 2500373312608
	2496645786960 [label="encoder.layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	2496645786960 -> 2500373312800
	2500373312800 [label=AccumulateGrad]
	2500373312704 -> 2500373312608
	2496645785280 [label="encoder.layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	2496645785280 -> 2500373312704
	2500373312704 [label=AccumulateGrad]
	2500373312656 -> 2500373312512
	2500373312320 -> 2500373312128
	2500373312176 -> 2500373311936
	2496644340048 [label="decoder.blocks.0.conv1.0.weight
 (256, 3072, 3, 3)" fillcolor=lightblue]
	2496644340048 -> 2500373312176
	2500373312176 [label=AccumulateGrad]
	2500373311984 -> 2500373311840
	2496644521312 [label="decoder.blocks.0.conv1.1.weight
 (256)" fillcolor=lightblue]
	2496644521312 -> 2500373311984
	2500373311984 [label=AccumulateGrad]
	2500373311792 -> 2500373311840
	2496462784464 [label="decoder.blocks.0.conv1.1.bias
 (256)" fillcolor=lightblue]
	2496462784464 -> 2500373311792
	2500373311792 [label=AccumulateGrad]
	2500373311744 -> 2500373311504
	2497283580832 [label="decoder.blocks.0.conv2.0.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2497283580832 -> 2500373311744
	2500373311744 [label=AccumulateGrad]
	2500373311552 -> 2500373311408
	2497283585312 [label="decoder.blocks.0.conv2.1.weight
 (256)" fillcolor=lightblue]
	2497283585312 -> 2500373311552
	2500373311552 [label=AccumulateGrad]
	2500373311264 -> 2500373311408
	2497283586992 [label="decoder.blocks.0.conv2.1.bias
 (256)" fillcolor=lightblue]
	2497283586992 -> 2500373311264
	2500373311264 [label=AccumulateGrad]
	2500373311216 -> 2500373311024
	2500373311072 -> 2500373310832
	2497283586432 [label="decoder.blocks.1.conv1.0.weight
 (128, 768, 3, 3)" fillcolor=lightblue]
	2497283586432 -> 2500373311072
	2500373311072 [label=AccumulateGrad]
	2500373310880 -> 2500373310736
	2497283583072 [label="decoder.blocks.1.conv1.1.weight
 (128)" fillcolor=lightblue]
	2497283583072 -> 2500373310880
	2500373310880 [label=AccumulateGrad]
	2500373310688 -> 2500373310736
	2497283583392 [label="decoder.blocks.1.conv1.1.bias
 (128)" fillcolor=lightblue]
	2497283583392 -> 2500373310688
	2500373310688 [label=AccumulateGrad]
	2500373310640 -> 2500373310400
	2497283581152 [label="decoder.blocks.1.conv2.0.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2497283581152 -> 2500373310640
	2500373310640 [label=AccumulateGrad]
	2500373310448 -> 2500373310304
	2497283587552 [label="decoder.blocks.1.conv2.1.weight
 (128)" fillcolor=lightblue]
	2497283587552 -> 2500373310448
	2500373310448 [label=AccumulateGrad]
	2500373310160 -> 2500373310304
	2497283584432 [label="decoder.blocks.1.conv2.1.bias
 (128)" fillcolor=lightblue]
	2497283584432 -> 2500373310160
	2500373310160 [label=AccumulateGrad]
	2500373310112 -> 2500373309920
	2500373309968 -> 2500373309728
	2497283582432 [label="decoder.blocks.2.conv1.0.weight
 (64, 384, 3, 3)" fillcolor=lightblue]
	2497283582432 -> 2500373309968
	2500373309968 [label=AccumulateGrad]
	2500373309776 -> 2500373309632
	2497283582032 [label="decoder.blocks.2.conv1.1.weight
 (64)" fillcolor=lightblue]
	2497283582032 -> 2500373309776
	2500373309776 [label=AccumulateGrad]
	2500373309584 -> 2500373309632
	2497283587312 [label="decoder.blocks.2.conv1.1.bias
 (64)" fillcolor=lightblue]
	2497283587312 -> 2500373309584
	2500373309584 [label=AccumulateGrad]
	2500373309536 -> 2500373309296
	2497283587152 [label="decoder.blocks.2.conv2.0.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2497283587152 -> 2500373309536
	2500373309536 [label=AccumulateGrad]
	2500373309344 -> 2500373309200
	2497283584112 [label="decoder.blocks.2.conv2.1.weight
 (64)" fillcolor=lightblue]
	2497283584112 -> 2500373309344
	2500373309344 [label=AccumulateGrad]
	2500373309056 -> 2500373309200
	2497283582512 [label="decoder.blocks.2.conv2.1.bias
 (64)" fillcolor=lightblue]
	2497283582512 -> 2500373309056
	2500373309056 [label=AccumulateGrad]
	2500373309008 -> 2500373308816
	2500373308864 -> 2500373308624
	2497283587632 [label="decoder.blocks.3.conv1.0.weight
 (32, 128, 3, 3)" fillcolor=lightblue]
	2497283587632 -> 2500373308864
	2500373308864 [label=AccumulateGrad]
	2500373308672 -> 2500373308528
	2497283581232 [label="decoder.blocks.3.conv1.1.weight
 (32)" fillcolor=lightblue]
	2497283581232 -> 2500373308672
	2500373308672 [label=AccumulateGrad]
	2500373308480 -> 2500373308528
	2497283580912 [label="decoder.blocks.3.conv1.1.bias
 (32)" fillcolor=lightblue]
	2497283580912 -> 2500373308480
	2500373308480 [label=AccumulateGrad]
	2500373308432 -> 2500373308192
	2497283581472 [label="decoder.blocks.3.conv2.0.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	2497283581472 -> 2500373308432
	2500373308432 [label=AccumulateGrad]
	2500373308240 -> 2500373308096
	2497283582912 [label="decoder.blocks.3.conv2.1.weight
 (32)" fillcolor=lightblue]
	2497283582912 -> 2500373308240
	2500373308240 [label=AccumulateGrad]
	2500373307904 -> 2500373308096
	2497283587392 [label="decoder.blocks.3.conv2.1.bias
 (32)" fillcolor=lightblue]
	2497283587392 -> 2500373307904
	2500373307904 [label=AccumulateGrad]
	2500373307952 -> 2500373307712
	2497283585232 [label="decoder.blocks.4.conv1.0.weight
 (16, 32, 3, 3)" fillcolor=lightblue]
	2497283585232 -> 2500373307952
	2500373307952 [label=AccumulateGrad]
	2500373307664 -> 2500373308048
	2497283581552 [label="decoder.blocks.4.conv1.1.weight
 (16)" fillcolor=lightblue]
	2497283581552 -> 2500373307664
	2500373307664 [label=AccumulateGrad]
	2500373307520 -> 2500373308048
	2497283580992 [label="decoder.blocks.4.conv1.1.bias
 (16)" fillcolor=lightblue]
	2497283580992 -> 2500373307520
	2500373307520 [label=AccumulateGrad]
	2500373307472 -> 2500373307328
	2497283581632 [label="decoder.blocks.4.conv2.0.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
	2497283581632 -> 2500373307472
	2500373307472 [label=AccumulateGrad]
	2500373306944 -> 2500373306752
	2497283585712 [label="decoder.blocks.4.conv2.1.weight
 (16)" fillcolor=lightblue]
	2497283585712 -> 2500373306944
	2500373306944 [label=AccumulateGrad]
	2500373306656 -> 2500373306752
	2497283581952 [label="decoder.blocks.4.conv2.1.bias
 (16)" fillcolor=lightblue]
	2497283581952 -> 2500373306656
	2500373306656 [label=AccumulateGrad]
	2500373307280 -> 2500373307232
	2497283582752 [label="segmentation_head.0.weight
 (5, 16, 3, 3)" fillcolor=lightblue]
	2497283582752 -> 2500373307280
	2500373307280 [label=AccumulateGrad]
	2500373306848 -> 2500373307232
	2497283586032 [label="segmentation_head.0.bias
 (5)" fillcolor=lightblue]
	2497283586032 -> 2500373306848
	2500373306848 [label=AccumulateGrad]
	2500373307232 -> 2500373336128
}
