digraph {
	graph [size="135.75,135.75"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1359194563968 [label="
 (1, 5, 224, 224)" fillcolor=darkolivegreen1]
	1359194136368 [label=ConvolutionBackward0]
	1359177090048 -> 1359194136368
	1359177090048 [label=ReluBackward0]
	1359193816752 -> 1359177090048
	1359193816752 [label=CudnnBatchNormBackward0]
	1358930848032 -> 1359193816752
	1358930848032 [label=ConvolutionBackward0]
	1359176580448 -> 1358930848032
	1359176580448 [label=ReluBackward0]
	1358930446320 -> 1359176580448
	1358930446320 [label=CudnnBatchNormBackward0]
	1358930454912 -> 1358930446320
	1358930454912 [label=ConvolutionBackward0]
	1358930356368 -> 1358930454912
	1358930356368 [label=UpsampleNearest2DBackward0]
	1358930577248 -> 1358930356368
	1358930577248 [label=ReluBackward0]
	1358930578112 -> 1358930577248
	1358930578112 [label=CudnnBatchNormBackward0]
	1359194418976 -> 1358930578112
	1359194418976 [label=ConvolutionBackward0]
	1359194412880 -> 1359194418976
	1359194412880 [label=ReluBackward0]
	1359194422720 -> 1359194412880
	1359194422720 [label=CudnnBatchNormBackward0]
	1358930310960 -> 1359194422720
	1358930310960 [label=ConvolutionBackward0]
	1358930305584 -> 1358930310960
	1358930305584 [label=CatBackward0]
	1358930297184 -> 1358930305584
	1358930297184 [label=UpsampleNearest2DBackward0]
	1358930304480 -> 1358930297184
	1358930304480 [label=ReluBackward0]
	1358930312256 -> 1358930304480
	1358930312256 [label=CudnnBatchNormBackward0]
	1358930298432 -> 1358930312256
	1358930298432 [label=ConvolutionBackward0]
	1358929571040 -> 1358930298432
	1358929571040 [label=ReluBackward0]
	1358929569840 -> 1358929571040
	1358929569840 [label=CudnnBatchNormBackward0]
	1358929564800 -> 1358929569840
	1358929564800 [label=ConvolutionBackward0]
	1359194477744 -> 1358929564800
	1359194477744 [label=CatBackward0]
	1359194480528 -> 1359194477744
	1359194480528 [label=UpsampleNearest2DBackward0]
	1359194487104 -> 1359194480528
	1359194487104 [label=ReluBackward0]
	1359194478272 -> 1359194487104
	1359194478272 [label=CudnnBatchNormBackward0]
	1359194487632 -> 1359194478272
	1359194487632 [label=ConvolutionBackward0]
	1359194477696 -> 1359194487632
	1359194477696 [label=ReluBackward0]
	1359194487536 -> 1359194477696
	1359194487536 [label=CudnnBatchNormBackward0]
	1359194477552 -> 1359194487536
	1359194477552 [label=ConvolutionBackward0]
	1359194476256 -> 1359194477552
	1359194476256 [label=CatBackward0]
	1359194485952 -> 1359194476256
	1359194485952 [label=UpsampleNearest2DBackward0]
	1359194482880 -> 1359194485952
	1359194482880 [label=ReluBackward0]
	1359194483792 -> 1359194482880
	1359194483792 [label=CudnnBatchNormBackward0]
	1359194482784 -> 1359194483792
	1359194482784 [label=ConvolutionBackward0]
	1359194476448 -> 1359194482784
	1359194476448 [label=ReluBackward0]
	1359194476352 -> 1359194476448
	1359194476352 [label=CudnnBatchNormBackward0]
	1359194476832 -> 1359194476352
	1359194476832 [label=ConvolutionBackward0]
	1359194474336 -> 1359194476832
	1359194474336 [label=CatBackward0]
	1359194483696 -> 1359194474336
	1359194483696 [label=UpsampleNearest2DBackward0]
	1359194474576 -> 1359194483696
	1359194474576 [label=ReluBackward0]
	1359194486480 -> 1359194474576
	1359194486480 [label=AddBackward0]
	1359194482976 -> 1359194486480
	1359194482976 [label=CudnnBatchNormBackward0]
	1359194481008 -> 1359194482976
	1359194481008 [label=ConvolutionBackward0]
	1359194484416 -> 1359194481008
	1359194484416 [label=ReluBackward0]
	1359194485088 -> 1359194484416
	1359194485088 [label=CudnnBatchNormBackward0]
	1359194480768 -> 1359194485088
	1359194480768 [label=ConvolutionBackward0]
	1359194488592 -> 1359194480768
	1359194488592 [label=ReluBackward0]
	1359194483360 -> 1359194488592
	1359194483360 [label=AddBackward0]
	1359194474144 -> 1359194483360
	1359194474144 [label=CudnnBatchNormBackward0]
	1359194482688 -> 1359194474144
	1359194482688 [label=ConvolutionBackward0]
	1359194484032 -> 1359194482688
	1359194484032 [label=ReluBackward0]
	1359194477936 -> 1359194484032
	1359194477936 [label=CudnnBatchNormBackward0]
	1359194481728 -> 1359194477936
	1359194481728 [label=ConvolutionBackward0]
	1359194478848 -> 1359194481728
	1359194478848 [label=ReluBackward0]
	1359193974448 -> 1359194478848
	1359193974448 [label=AddBackward0]
	1359194263648 -> 1359193974448
	1359194263648 [label=CudnnBatchNormBackward0]
	1359194263744 -> 1359194263648
	1359194263744 [label=ConvolutionBackward0]
	1359194266720 -> 1359194263744
	1359194266720 [label=ReluBackward0]
	1359194264992 -> 1359194266720
	1359194264992 [label=CudnnBatchNormBackward0]
	1359194263120 -> 1359194264992
	1359194263120 [label=ConvolutionBackward0]
	1359194480288 -> 1359194263120
	1359194480288 [label=ReluBackward0]
	1359194526272 -> 1359194480288
	1359194526272 [label=AddBackward0]
	1359194524784 -> 1359194526272
	1359194524784 [label=CudnnBatchNormBackward0]
	1359194522768 -> 1359194524784
	1359194522768 [label=ConvolutionBackward0]
	1359194522336 -> 1359194522768
	1359194522336 [label=ReluBackward0]
	1359194523728 -> 1359194522336
	1359194523728 [label=CudnnBatchNormBackward0]
	1359194527664 -> 1359194523728
	1359194527664 [label=ConvolutionBackward0]
	1359194525600 -> 1359194527664
	1359194525600 [label=ReluBackward0]
	1359194526128 -> 1359194525600
	1359194526128 [label=AddBackward0]
	1359194524928 -> 1359194526128
	1359194524928 [label=CudnnBatchNormBackward0]
	1359194521664 -> 1359194524928
	1359194521664 [label=ConvolutionBackward0]
	1359194522144 -> 1359194521664
	1359194522144 [label=ReluBackward0]
	1359194527136 -> 1359194522144
	1359194527136 [label=CudnnBatchNormBackward0]
	1359194527376 -> 1359194527136
	1359194527376 [label=ConvolutionBackward0]
	1359194524496 -> 1359194527376
	1359194524496 [label=ReluBackward0]
	1359194527808 -> 1359194524496
	1359194527808 [label=AddBackward0]
	1359194527904 -> 1359194527808
	1359194527904 [label=CudnnBatchNormBackward0]
	1359194528048 -> 1359194527904
	1359194528048 [label=ConvolutionBackward0]
	1359194528240 -> 1359194528048
	1359194528240 [label=ReluBackward0]
	1359194528384 -> 1359194528240
	1359194528384 [label=CudnnBatchNormBackward0]
	1359194528480 -> 1359194528384
	1359194528480 [label=ConvolutionBackward0]
	1359194527952 -> 1359194528480
	1359194527952 [label=ReluBackward0]
	1359194528768 -> 1359194527952
	1359194528768 [label=AddBackward0]
	1359194528864 -> 1359194528768
	1359194528864 [label=CudnnBatchNormBackward0]
	1359194529008 -> 1359194528864
	1359194529008 [label=ConvolutionBackward0]
	1359194529200 -> 1359194529008
	1359194529200 [label=ReluBackward0]
	1359194529344 -> 1359194529200
	1359194529344 [label=CudnnBatchNormBackward0]
	1359194529440 -> 1359194529344
	1359194529440 [label=ConvolutionBackward0]
	1359194528912 -> 1359194529440
	1359194528912 [label=ReluBackward0]
	1359194529728 -> 1359194528912
	1359194529728 [label=AddBackward0]
	1359194529824 -> 1359194529728
	1359194529824 [label=CudnnBatchNormBackward0]
	1359194529968 -> 1359194529824
	1359194529968 [label=ConvolutionBackward0]
	1359194530160 -> 1359194529968
	1359194530160 [label=ReluBackward0]
	1359194530304 -> 1359194530160
	1359194530304 [label=CudnnBatchNormBackward0]
	1359194530400 -> 1359194530304
	1359194530400 [label=ConvolutionBackward0]
	1359194529872 -> 1359194530400
	1359194529872 [label=ReluBackward0]
	1359194530688 -> 1359194529872
	1359194530688 [label=AddBackward0]
	1359194530784 -> 1359194530688
	1359194530784 [label=CudnnBatchNormBackward0]
	1359194530928 -> 1359194530784
	1359194530928 [label=ConvolutionBackward0]
	1359194531120 -> 1359194530928
	1359194531120 [label=ReluBackward0]
	1359194531264 -> 1359194531120
	1359194531264 [label=CudnnBatchNormBackward0]
	1359194531360 -> 1359194531264
	1359194531360 [label=ConvolutionBackward0]
	1359194483840 -> 1359194531360
	1359194483840 [label=ReluBackward0]
	1359194531648 -> 1359194483840
	1359194531648 [label=AddBackward0]
	1359194531744 -> 1359194531648
	1359194531744 [label=CudnnBatchNormBackward0]
	1359194531888 -> 1359194531744
	1359194531888 [label=ConvolutionBackward0]
	1359194532080 -> 1359194531888
	1359194532080 [label=ReluBackward0]
	1359194532224 -> 1359194532080
	1359194532224 [label=CudnnBatchNormBackward0]
	1359194532320 -> 1359194532224
	1359194532320 [label=ConvolutionBackward0]
	1359194531792 -> 1359194532320
	1359194531792 [label=ReluBackward0]
	1359194532608 -> 1359194531792
	1359194532608 [label=AddBackward0]
	1359194532704 -> 1359194532608
	1359194532704 [label=CudnnBatchNormBackward0]
	1359194532848 -> 1359194532704
	1359194532848 [label=ConvolutionBackward0]
	1359194533040 -> 1359194532848
	1359194533040 [label=ReluBackward0]
	1359194533184 -> 1359194533040
	1359194533184 [label=CudnnBatchNormBackward0]
	1359194533280 -> 1359194533184
	1359194533280 [label=ConvolutionBackward0]
	1359194532752 -> 1359194533280
	1359194532752 [label=ReluBackward0]
	1359194533568 -> 1359194532752
	1359194533568 [label=AddBackward0]
	1359194533664 -> 1359194533568
	1359194533664 [label=CudnnBatchNormBackward0]
	1359194533808 -> 1359194533664
	1359194533808 [label=ConvolutionBackward0]
	1359194534000 -> 1359194533808
	1359194534000 [label=ReluBackward0]
	1359194534144 -> 1359194534000
	1359194534144 [label=CudnnBatchNormBackward0]
	1359194534240 -> 1359194534144
	1359194534240 [label=ConvolutionBackward0]
	1359194533712 -> 1359194534240
	1359194533712 [label=ReluBackward0]
	1359194534528 -> 1359194533712
	1359194534528 [label=AddBackward0]
	1359194534624 -> 1359194534528
	1359194534624 [label=CudnnBatchNormBackward0]
	1359194534768 -> 1359194534624
	1359194534768 [label=ConvolutionBackward0]
	1359194534960 -> 1359194534768
	1359194534960 [label=ReluBackward0]
	1359194535104 -> 1359194534960
	1359194535104 [label=CudnnBatchNormBackward0]
	1359194535200 -> 1359194535104
	1359194535200 [label=ConvolutionBackward0]
	1359194477648 -> 1359194535200
	1359194477648 [label=ReluBackward0]
	1359194419168 -> 1359194477648
	1359194419168 [label=AddBackward0]
	1359194421760 -> 1359194419168
	1359194421760 [label=CudnnBatchNormBackward0]
	1359194482064 -> 1359194421760
	1359194482064 [label=ConvolutionBackward0]
	1359194523152 -> 1359194482064
	1359194523152 [label=ReluBackward0]
	1359194535536 -> 1359194523152
	1359194535536 [label=CudnnBatchNormBackward0]
	1359194535632 -> 1359194535536
	1359194535632 [label=ConvolutionBackward0]
	1359194421808 -> 1359194535632
	1359194421808 [label=ReluBackward0]
	1359194535920 -> 1359194421808
	1359194535920 [label=AddBackward0]
	1359194536016 -> 1359194535920
	1359194536016 [label=CudnnBatchNormBackward0]
	1359194536160 -> 1359194536016
	1359194536160 [label=ConvolutionBackward0]
	1359194536352 -> 1359194536160
	1359194536352 [label=ReluBackward0]
	1359194536496 -> 1359194536352
	1359194536496 [label=CudnnBatchNormBackward0]
	1359194536592 -> 1359194536496
	1359194536592 [label=ConvolutionBackward0]
	1359194536064 -> 1359194536592
	1359194536064 [label=ReluBackward0]
	1359194536880 -> 1359194536064
	1359194536880 [label=AddBackward0]
	1359194536976 -> 1359194536880
	1359194536976 [label=CudnnBatchNormBackward0]
	1359194537120 -> 1359194536976
	1359194537120 [label=ConvolutionBackward0]
	1359194537312 -> 1359194537120
	1359194537312 [label=ReluBackward0]
	1359194537456 -> 1359194537312
	1359194537456 [label=CudnnBatchNormBackward0]
	1359194537552 -> 1359194537456
	1359194537552 [label=ConvolutionBackward0]
	1359194537024 -> 1359194537552
	1359194537024 [label=MaxPool2DWithIndicesBackward0]
	1358930310000 -> 1359194537024
	1358930310000 [label=ReluBackward0]
	1359194537888 -> 1358930310000
	1359194537888 [label=CudnnBatchNormBackward0]
	1359194537840 -> 1359194537888
	1359194537840 [label=ConvolutionBackward0]
	1359194587344 -> 1359194537840
	1356717849840 [label="encoder.conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	1356717849840 -> 1359194587344
	1359194587344 [label=AccumulateGrad]
	1359194537936 -> 1359194537888
	1356523107776 [label="encoder.bn1.weight
 (64)" fillcolor=lightblue]
	1356523107776 -> 1359194537936
	1359194537936 [label=AccumulateGrad]
	1359194587200 -> 1359194537888
	1356523095136 [label="encoder.bn1.bias
 (64)" fillcolor=lightblue]
	1356523095136 -> 1359194587200
	1359194587200 [label=AccumulateGrad]
	1359194537744 -> 1359194537552
	1356523109216 [label="encoder.layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1356523109216 -> 1359194537744
	1359194537744 [label=AccumulateGrad]
	1359194537600 -> 1359194537456
	1356523097056 [label="encoder.layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1356523097056 -> 1359194537600
	1359194537600 [label=AccumulateGrad]
	1359194537408 -> 1359194537456
	1356523102416 [label="encoder.layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1356523102416 -> 1359194537408
	1359194537408 [label=AccumulateGrad]
	1359194537360 -> 1359194537120
	1356523105696 [label="encoder.layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1356523105696 -> 1359194537360
	1359194537360 [label=AccumulateGrad]
	1359194537168 -> 1359194536976
	1356523096496 [label="encoder.layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1356523096496 -> 1359194537168
	1359194537168 [label=AccumulateGrad]
	1359194537072 -> 1359194536976
	1356523100656 [label="encoder.layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1356523100656 -> 1359194537072
	1359194537072 [label=AccumulateGrad]
	1359194537024 -> 1359194536880
	1359194536784 -> 1359194536592
	1356523101056 [label="encoder.layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1356523101056 -> 1359194536784
	1359194536784 [label=AccumulateGrad]
	1359194536640 -> 1359194536496
	1356523101696 [label="encoder.layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1356523101696 -> 1359194536640
	1359194536640 [label=AccumulateGrad]
	1359194536448 -> 1359194536496
	1356523107056 [label="encoder.layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1356523107056 -> 1359194536448
	1359194536448 [label=AccumulateGrad]
	1359194536400 -> 1359194536160
	1356523096016 [label="encoder.layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1356523096016 -> 1359194536400
	1359194536400 [label=AccumulateGrad]
	1359194536208 -> 1359194536016
	1356523109616 [label="encoder.layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1356523109616 -> 1359194536208
	1359194536208 [label=AccumulateGrad]
	1359194536112 -> 1359194536016
	1356523105936 [label="encoder.layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1356523105936 -> 1359194536112
	1359194536112 [label=AccumulateGrad]
	1359194536064 -> 1359194535920
	1359194535824 -> 1359194535632
	1356523108576 [label="encoder.layer1.2.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1356523108576 -> 1359194535824
	1359194535824 [label=AccumulateGrad]
	1359194535680 -> 1359194535536
	1356523107936 [label="encoder.layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	1356523107936 -> 1359194535680
	1359194535680 [label=AccumulateGrad]
	1359194535488 -> 1359194535536
	1356523104576 [label="encoder.layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	1356523104576 -> 1359194535488
	1359194535488 [label=AccumulateGrad]
	1359194535440 -> 1359194482064
	1356523104416 [label="encoder.layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1356523104416 -> 1359194535440
	1359194535440 [label=AccumulateGrad]
	1359194484800 -> 1359194421760
	1356523103776 [label="encoder.layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	1356523103776 -> 1359194484800
	1359194484800 [label=AccumulateGrad]
	1359194481872 -> 1359194421760
	1356523101456 [label="encoder.layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	1356523101456 -> 1359194481872
	1359194481872 [label=AccumulateGrad]
	1359194421808 -> 1359194419168
	1359194418880 -> 1359194535200
	1355522595392 [label="encoder.layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	1355522595392 -> 1359194418880
	1359194418880 [label=AccumulateGrad]
	1359194535248 -> 1359194535104
	1355522591632 [label="encoder.layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1355522591632 -> 1359194535248
	1359194535248 [label=AccumulateGrad]
	1359194535056 -> 1359194535104
	1355522604032 [label="encoder.layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1355522604032 -> 1359194535056
	1359194535056 [label=AccumulateGrad]
	1359194535008 -> 1359194534768
	1355522593952 [label="encoder.layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1355522593952 -> 1359194535008
	1359194535008 [label=AccumulateGrad]
	1359194534816 -> 1359194534624
	1355522599952 [label="encoder.layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1355522599952 -> 1359194534816
	1359194534816 [label=AccumulateGrad]
	1359194534720 -> 1359194534624
	1355522597152 [label="encoder.layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1355522597152 -> 1359194534720
	1359194534720 [label=AccumulateGrad]
	1359194534672 -> 1359194534528
	1359194534672 [label=CudnnBatchNormBackward0]
	1359194419936 -> 1359194534672
	1359194419936 [label=ConvolutionBackward0]
	1359194477648 -> 1359194419936
	1359194534912 -> 1359194419936
	1356110262144 [label="encoder.layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1356110262144 -> 1359194534912
	1359194534912 [label=AccumulateGrad]
	1359194422336 -> 1359194534672
	1356110261744 [label="encoder.layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	1356110261744 -> 1359194422336
	1359194422336 [label=AccumulateGrad]
	1359194419840 -> 1359194534672
	1356110256704 [label="encoder.layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	1356110256704 -> 1359194419840
	1359194419840 [label=AccumulateGrad]
	1359194534432 -> 1359194534240
	1355522603232 [label="encoder.layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1355522603232 -> 1359194534432
	1359194534432 [label=AccumulateGrad]
	1359194534288 -> 1359194534144
	1355522603712 [label="encoder.layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1355522603712 -> 1359194534288
	1359194534288 [label=AccumulateGrad]
	1359194534096 -> 1359194534144
	1355522594512 [label="encoder.layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1355522594512 -> 1359194534096
	1359194534096 [label=AccumulateGrad]
	1359194534048 -> 1359194533808
	1355522601952 [label="encoder.layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1355522601952 -> 1359194534048
	1359194534048 [label=AccumulateGrad]
	1359194533856 -> 1359194533664
	1355522590752 [label="encoder.layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1355522590752 -> 1359194533856
	1359194533856 [label=AccumulateGrad]
	1359194533760 -> 1359194533664
	1355522595552 [label="encoder.layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1355522595552 -> 1359194533760
	1359194533760 [label=AccumulateGrad]
	1359194533712 -> 1359194533568
	1359194533472 -> 1359194533280
	1355522604112 [label="encoder.layer2.2.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1355522604112 -> 1359194533472
	1359194533472 [label=AccumulateGrad]
	1359194533328 -> 1359194533184
	1355522595072 [label="encoder.layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	1355522595072 -> 1359194533328
	1359194533328 [label=AccumulateGrad]
	1359194533136 -> 1359194533184
	1355522599472 [label="encoder.layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	1355522599472 -> 1359194533136
	1359194533136 [label=AccumulateGrad]
	1359194533088 -> 1359194532848
	1355522597392 [label="encoder.layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1355522597392 -> 1359194533088
	1359194533088 [label=AccumulateGrad]
	1359194532896 -> 1359194532704
	1355522598032 [label="encoder.layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	1355522598032 -> 1359194532896
	1359194532896 [label=AccumulateGrad]
	1359194532800 -> 1359194532704
	1355522590272 [label="encoder.layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	1355522590272 -> 1359194532800
	1359194532800 [label=AccumulateGrad]
	1359194532752 -> 1359194532608
	1359194532512 -> 1359194532320
	1355522601472 [label="encoder.layer2.3.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1355522601472 -> 1359194532512
	1359194532512 [label=AccumulateGrad]
	1359194532368 -> 1359194532224
	1355522592272 [label="encoder.layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	1355522592272 -> 1359194532368
	1359194532368 [label=AccumulateGrad]
	1359194532176 -> 1359194532224
	1355522593552 [label="encoder.layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	1355522593552 -> 1359194532176
	1359194532176 [label=AccumulateGrad]
	1359194532128 -> 1359194531888
	1355522599552 [label="encoder.layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1355522599552 -> 1359194532128
	1359194532128 [label=AccumulateGrad]
	1359194531936 -> 1359194531744
	1355522588992 [label="encoder.layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	1355522588992 -> 1359194531936
	1359194531936 [label=AccumulateGrad]
	1359194531840 -> 1359194531744
	1355522601872 [label="encoder.layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	1355522601872 -> 1359194531840
	1359194531840 [label=AccumulateGrad]
	1359194531792 -> 1359194531648
	1359194531552 -> 1359194531360
	1355522603312 [label="encoder.layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	1355522603312 -> 1359194531552
	1359194531552 [label=AccumulateGrad]
	1359194531408 -> 1359194531264
	1355522602752 [label="encoder.layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1355522602752 -> 1359194531408
	1359194531408 [label=AccumulateGrad]
	1359194531216 -> 1359194531264
	1355522596272 [label="encoder.layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1355522596272 -> 1359194531216
	1359194531216 [label=AccumulateGrad]
	1359194531168 -> 1359194530928
	1355522588912 [label="encoder.layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1355522588912 -> 1359194531168
	1359194531168 [label=AccumulateGrad]
	1359194530976 -> 1359194530784
	1355522600112 [label="encoder.layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1355522600112 -> 1359194530976
	1359194530976 [label=AccumulateGrad]
	1359194530880 -> 1359194530784
	1355522599232 [label="encoder.layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1355522599232 -> 1359194530880
	1359194530880 [label=AccumulateGrad]
	1359194530832 -> 1359194530688
	1359194530832 [label=CudnnBatchNormBackward0]
	1359194420128 -> 1359194530832
	1359194420128 [label=ConvolutionBackward0]
	1359194483840 -> 1359194420128
	1359194531600 -> 1359194420128
	1355522603872 [label="encoder.layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1355522603872 -> 1359194531600
	1359194531600 [label=AccumulateGrad]
	1359194531072 -> 1359194530832
	1355522603392 [label="encoder.layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1355522603392 -> 1359194531072
	1359194531072 [label=AccumulateGrad]
	1359194531024 -> 1359194530832
	1355522592512 [label="encoder.layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1355522592512 -> 1359194531024
	1359194531024 [label=AccumulateGrad]
	1359194530592 -> 1359194530400
	1355522594912 [label="encoder.layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1355522594912 -> 1359194530592
	1359194530592 [label=AccumulateGrad]
	1359194530448 -> 1359194530304
	1355522602992 [label="encoder.layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1355522602992 -> 1359194530448
	1359194530448 [label=AccumulateGrad]
	1359194530256 -> 1359194530304
	1355522601152 [label="encoder.layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1355522601152 -> 1359194530256
	1359194530256 [label=AccumulateGrad]
	1359194530208 -> 1359194529968
	1355522597232 [label="encoder.layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1355522597232 -> 1359194530208
	1359194530208 [label=AccumulateGrad]
	1359194530016 -> 1359194529824
	1355522596192 [label="encoder.layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1355522596192 -> 1359194530016
	1359194530016 [label=AccumulateGrad]
	1359194529920 -> 1359194529824
	1355522591712 [label="encoder.layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1355522591712 -> 1359194529920
	1359194529920 [label=AccumulateGrad]
	1359194529872 -> 1359194529728
	1359194529632 -> 1359194529440
	1355522596512 [label="encoder.layer3.2.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1355522596512 -> 1359194529632
	1359194529632 [label=AccumulateGrad]
	1359194529488 -> 1359194529344
	1355522590112 [label="encoder.layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	1355522590112 -> 1359194529488
	1359194529488 [label=AccumulateGrad]
	1359194529296 -> 1359194529344
	1355522592832 [label="encoder.layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	1355522592832 -> 1359194529296
	1359194529296 [label=AccumulateGrad]
	1359194529248 -> 1359194529008
	1355522589792 [label="encoder.layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1355522589792 -> 1359194529248
	1359194529248 [label=AccumulateGrad]
	1359194529056 -> 1359194528864
	1355522602032 [label="encoder.layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	1355522602032 -> 1359194529056
	1359194529056 [label=AccumulateGrad]
	1359194528960 -> 1359194528864
	1355522589952 [label="encoder.layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	1355522589952 -> 1359194528960
	1359194528960 [label=AccumulateGrad]
	1359194528912 -> 1359194528768
	1359194528672 -> 1359194528480
	1355522595712 [label="encoder.layer3.3.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1355522595712 -> 1359194528672
	1359194528672 [label=AccumulateGrad]
	1359194528528 -> 1359194528384
	1355522589712 [label="encoder.layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	1355522589712 -> 1359194528528
	1359194528528 [label=AccumulateGrad]
	1359194528336 -> 1359194528384
	1355522593232 [label="encoder.layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	1355522593232 -> 1359194528336
	1359194528336 [label=AccumulateGrad]
	1359194528288 -> 1359194528048
	1355522598752 [label="encoder.layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1355522598752 -> 1359194528288
	1359194528288 [label=AccumulateGrad]
	1359194528096 -> 1359194527904
	1355522588752 [label="encoder.layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	1355522588752 -> 1359194528096
	1359194528096 [label=AccumulateGrad]
	1359194528000 -> 1359194527904
	1355522595792 [label="encoder.layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	1355522595792 -> 1359194528000
	1359194528000 [label=AccumulateGrad]
	1359194527952 -> 1359194527808
	1359194527616 -> 1359194527376
	1355522591072 [label="encoder.layer3.4.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1355522591072 -> 1359194527616
	1359194527616 [label=AccumulateGrad]
	1359194527328 -> 1359194527136
	1355522600592 [label="encoder.layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	1355522600592 -> 1359194527328
	1359194527328 [label=AccumulateGrad]
	1359194527520 -> 1359194527136
	1355522590432 [label="encoder.layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	1355522590432 -> 1359194527520
	1359194527520 [label=AccumulateGrad]
	1359194527232 -> 1359194521664
	1355522598192 [label="encoder.layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1355522598192 -> 1359194527232
	1359194527232 [label=AccumulateGrad]
	1359194524640 -> 1359194524928
	1355522597312 [label="encoder.layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	1355522597312 -> 1359194524640
	1359194524640 [label=AccumulateGrad]
	1359194522000 -> 1359194524928
	1355522596352 [label="encoder.layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	1355522596352 -> 1359194522000
	1359194522000 [label=AccumulateGrad]
	1359194524496 -> 1359194526128
	1359194525696 -> 1359194527664
	1355522594992 [label="encoder.layer3.5.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1355522594992 -> 1359194525696
	1359194525696 [label=AccumulateGrad]
	1359194522192 -> 1359194523728
	1355522594592 [label="encoder.layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	1355522594592 -> 1359194522192
	1359194522192 [label=AccumulateGrad]
	1359194524880 -> 1359194523728
	1355522594352 [label="encoder.layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	1355522594352 -> 1359194524880
	1359194524880 [label=AccumulateGrad]
	1359194526896 -> 1359194522768
	1355522592112 [label="encoder.layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1355522592112 -> 1359194526896
	1359194526896 [label=AccumulateGrad]
	1359194526512 -> 1359194524784
	1355522594752 [label="encoder.layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	1355522594752 -> 1359194526512
	1359194526512 [label=AccumulateGrad]
	1359194522672 -> 1359194524784
	1355522591232 [label="encoder.layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	1355522591232 -> 1359194522672
	1359194522672 [label=AccumulateGrad]
	1359194525600 -> 1359194526272
	1359194265472 -> 1359194263120
	1355522602432 [label="encoder.layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	1355522602432 -> 1359194265472
	1359194265472 [label=AccumulateGrad]
	1359194263792 -> 1359194264992
	1355522604352 [label="encoder.layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1355522604352 -> 1359194263792
	1359194263792 [label=AccumulateGrad]
	1359194263168 -> 1359194264992
	1355522593872 [label="encoder.layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1355522593872 -> 1359194263168
	1359194263168 [label=AccumulateGrad]
	1359194266096 -> 1359194263744
	1355522594032 [label="encoder.layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1355522594032 -> 1359194266096
	1359194266096 [label=AccumulateGrad]
	1359194264416 -> 1359194263648
	1355522590832 [label="encoder.layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1355522590832 -> 1359194264416
	1359194264416 [label=AccumulateGrad]
	1359194269216 -> 1359194263648
	1355522596112 [label="encoder.layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1355522596112 -> 1359194269216
	1359194269216 [label=AccumulateGrad]
	1359194264752 -> 1359193974448
	1359194264752 [label=CudnnBatchNormBackward0]
	1359194269360 -> 1359194264752
	1359194269360 [label=ConvolutionBackward0]
	1359194480288 -> 1359194269360
	1359194525552 -> 1359194269360
	1355522590352 [label="encoder.layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1355522590352 -> 1359194525552
	1359194525552 [label=AccumulateGrad]
	1359194266336 -> 1359194264752
	1355522589552 [label="encoder.layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1355522589552 -> 1359194266336
	1359194266336 [label=AccumulateGrad]
	1359194262928 -> 1359194264752
	1355522591152 [label="encoder.layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1355522591152 -> 1359194262928
	1359194262928 [label=AccumulateGrad]
	1359193974880 -> 1359194481728
	1355522601552 [label="encoder.layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1355522601552 -> 1359193974880
	1359193974880 [label=AccumulateGrad]
	1359193976224 -> 1359194477936
	1355522589632 [label="encoder.layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1355522589632 -> 1359193976224
	1359193976224 [label=AccumulateGrad]
	1359193974400 -> 1359194477936
	1355522603072 [label="encoder.layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1355522603072 -> 1359193974400
	1359193974400 [label=AccumulateGrad]
	1359194482640 -> 1359194482688
	1355522548960 [label="encoder.layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1355522548960 -> 1359194482640
	1359194482640 [label=AccumulateGrad]
	1359194483888 -> 1359194474144
	1355522550560 [label="encoder.layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1355522550560 -> 1359194483888
	1359194483888 [label=AccumulateGrad]
	1359194478320 -> 1359194474144
	1355522550160 [label="encoder.layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1355522550160 -> 1359194478320
	1359194478320 [label=AccumulateGrad]
	1359194478848 -> 1359194483360
	1359194481536 -> 1359194480768
	1355522551280 [label="encoder.layer4.2.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1355522551280 -> 1359194481536
	1359194481536 [label=AccumulateGrad]
	1359194472992 -> 1359194485088
	1355522546960 [label="encoder.layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	1355522546960 -> 1359194472992
	1359194472992 [label=AccumulateGrad]
	1359194483072 -> 1359194485088
	1355522554240 [label="encoder.layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	1355522554240 -> 1359194483072
	1359194483072 [label=AccumulateGrad]
	1359194483552 -> 1359194481008
	1355522548480 [label="encoder.layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1355522548480 -> 1359194483552
	1359194483552 [label=AccumulateGrad]
	1359194480912 -> 1359194482976
	1355522551440 [label="encoder.layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	1355522551440 -> 1359194480912
	1359194480912 [label=AccumulateGrad]
	1359194486960 -> 1359194482976
	1355522555520 [label="encoder.layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	1355522555520 -> 1359194486960
	1359194486960 [label=AccumulateGrad]
	1359194488592 -> 1359194486480
	1359194480288 -> 1359194474336
	1359194480192 -> 1359194476832
	1356523100496 [label="decoder.blocks.0.conv1.0.weight
 (256, 768, 3, 3)" fillcolor=lightblue]
	1356523100496 -> 1359194480192
	1359194480192 [label=AccumulateGrad]
	1359194477504 -> 1359194476352
	1356790822016 [label="decoder.blocks.0.conv1.1.weight
 (256)" fillcolor=lightblue]
	1356790822016 -> 1359194477504
	1359194477504 [label=AccumulateGrad]
	1359194480720 -> 1359194476352
	1356790814416 [label="decoder.blocks.0.conv1.1.bias
 (256)" fillcolor=lightblue]
	1356790814416 -> 1359194480720
	1359194480720 [label=AccumulateGrad]
	1359194479376 -> 1359194482784
	1355522497168 [label="decoder.blocks.0.conv2.0.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1355522497168 -> 1359194479376
	1359194479376 [label=AccumulateGrad]
	1359194488784 -> 1359194483792
	1357177969776 [label="decoder.blocks.0.conv2.1.weight
 (256)" fillcolor=lightblue]
	1357177969776 -> 1359194488784
	1359194488784 [label=AccumulateGrad]
	1359194478752 -> 1359194483792
	1355522554000 [label="decoder.blocks.0.conv2.1.bias
 (256)" fillcolor=lightblue]
	1355522554000 -> 1359194478752
	1359194478752 [label=AccumulateGrad]
	1359194483840 -> 1359194476256
	1359194473568 -> 1359194477552
	1355522554480 [label="decoder.blocks.1.conv1.0.weight
 (128, 384, 3, 3)" fillcolor=lightblue]
	1355522554480 -> 1359194473568
	1359194473568 [label=AccumulateGrad]
	1359194480240 -> 1359194487536
	1355522549040 [label="decoder.blocks.1.conv1.1.weight
 (128)" fillcolor=lightblue]
	1355522549040 -> 1359194480240
	1359194480240 [label=AccumulateGrad]
	1359194479808 -> 1359194487536
	1355522547600 [label="decoder.blocks.1.conv1.1.bias
 (128)" fillcolor=lightblue]
	1355522547600 -> 1359194479808
	1359194479808 [label=AccumulateGrad]
	1359194480000 -> 1359194487632
	1355522550240 [label="decoder.blocks.1.conv2.0.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1355522550240 -> 1359194480000
	1359194480000 [label=AccumulateGrad]
	1359194481296 -> 1359194478272
	1355522550960 [label="decoder.blocks.1.conv2.1.weight
 (128)" fillcolor=lightblue]
	1355522550960 -> 1359194481296
	1359194481296 [label=AccumulateGrad]
	1359194479952 -> 1359194478272
	1355522549840 [label="decoder.blocks.1.conv2.1.bias
 (128)" fillcolor=lightblue]
	1355522549840 -> 1359194479952
	1359194479952 [label=AccumulateGrad]
	1359194477648 -> 1359194477744
	1359194480048 -> 1358929564800
	1355522547760 [label="decoder.blocks.2.conv1.0.weight
 (64, 192, 3, 3)" fillcolor=lightblue]
	1355522547760 -> 1359194480048
	1359194480048 [label=AccumulateGrad]
	1359194474384 -> 1358929569840
	1355522552800 [label="decoder.blocks.2.conv1.1.weight
 (64)" fillcolor=lightblue]
	1355522552800 -> 1359194474384
	1359194474384 [label=AccumulateGrad]
	1359194473040 -> 1358929569840
	1355522550640 [label="decoder.blocks.2.conv1.1.bias
 (64)" fillcolor=lightblue]
	1355522550640 -> 1359194473040
	1359194473040 [label=AccumulateGrad]
	1358929571760 -> 1358930298432
	1355522547200 [label="decoder.blocks.2.conv2.0.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1355522547200 -> 1358929571760
	1358929571760 [label=AccumulateGrad]
	1358930311968 -> 1358930312256
	1355522552960 [label="decoder.blocks.2.conv2.1.weight
 (64)" fillcolor=lightblue]
	1355522552960 -> 1358930311968
	1358930311968 [label=AccumulateGrad]
	1359176203616 -> 1358930312256
	1355522548880 [label="decoder.blocks.2.conv2.1.bias
 (64)" fillcolor=lightblue]
	1355522548880 -> 1359176203616
	1359176203616 [label=AccumulateGrad]
	1358930310000 -> 1358930305584
	1358930300448 -> 1358930310960
	1355522547040 [label="decoder.blocks.3.conv1.0.weight
 (32, 128, 3, 3)" fillcolor=lightblue]
	1355522547040 -> 1358930300448
	1358930300448 [label=AccumulateGrad]
	1358930307744 -> 1359194422720
	1355522542560 [label="decoder.blocks.3.conv1.1.weight
 (32)" fillcolor=lightblue]
	1355522542560 -> 1358930307744
	1358930307744 [label=AccumulateGrad]
	1358930303472 -> 1359194422720
	1355522552080 [label="decoder.blocks.3.conv1.1.bias
 (32)" fillcolor=lightblue]
	1355522552080 -> 1358930303472
	1358930303472 [label=AccumulateGrad]
	1359194411008 -> 1359194418976
	1355522555280 [label="decoder.blocks.3.conv2.0.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	1355522555280 -> 1359194411008
	1359194411008 [label=AccumulateGrad]
	1359194422768 -> 1358930578112
	1355522549360 [label="decoder.blocks.3.conv2.1.weight
 (32)" fillcolor=lightblue]
	1355522549360 -> 1359194422768
	1359194422768 [label=AccumulateGrad]
	1359194417248 -> 1358930578112
	1355522551840 [label="decoder.blocks.3.conv2.1.bias
 (32)" fillcolor=lightblue]
	1355522551840 -> 1359194417248
	1359194417248 [label=AccumulateGrad]
	1358930359728 -> 1358930454912
	1355522553120 [label="decoder.blocks.4.conv1.0.weight
 (16, 32, 3, 3)" fillcolor=lightblue]
	1355522553120 -> 1358930359728
	1358930359728 [label=AccumulateGrad]
	1358930449872 -> 1358930446320
	1355522551760 [label="decoder.blocks.4.conv1.1.weight
 (16)" fillcolor=lightblue]
	1355522551760 -> 1358930449872
	1358930449872 [label=AccumulateGrad]
	1358930454864 -> 1358930446320
	1355522554560 [label="decoder.blocks.4.conv1.1.bias
 (16)" fillcolor=lightblue]
	1355522554560 -> 1358930454864
	1358930454864 [label=AccumulateGrad]
	1359176567104 -> 1358930848032
	1355522548320 [label="decoder.blocks.4.conv2.0.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
	1355522548320 -> 1359176567104
	1359176567104 [label=AccumulateGrad]
	1358930852064 -> 1359193816752
	1355522553920 [label="decoder.blocks.4.conv2.1.weight
 (16)" fillcolor=lightblue]
	1355522553920 -> 1358930852064
	1358930852064 [label=AccumulateGrad]
	1358930846208 -> 1359193816752
	1355522550320 [label="decoder.blocks.4.conv2.1.bias
 (16)" fillcolor=lightblue]
	1355522550320 -> 1358930846208
	1358930846208 [label=AccumulateGrad]
	1359177097200 -> 1359194136368
	1355522552640 [label="segmentation_head.0.weight
 (5, 16, 3, 3)" fillcolor=lightblue]
	1355522552640 -> 1359177097200
	1359177097200 [label=AccumulateGrad]
	1359175471776 -> 1359194136368
	1355522554080 [label="segmentation_head.0.bias
 (5)" fillcolor=lightblue]
	1355522554080 -> 1359175471776
	1359175471776 [label=AccumulateGrad]
	1359194136368 -> 1359194563968
}
